{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df95143d",
   "metadata": {},
   "source": [
    "# Nahuatl Notebook for the WHP_EarlyNahuatl_Dataset\n",
    "\n",
    "This notebook processes Nahuatl dictionary data, analyzing HTML tags, repairing malformed tags, and extracting citations and cross-references. This is a merged version of Todd's version and I where there is a SQLite-based data management approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8105d6",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f542ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import glob\n",
    "import csv\n",
    "import sqlite3\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "from inscriptis import get_text\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60814645",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHP_EarlyNahuatl_Data</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    name\n",
       "0  WHP_EarlyNahuatl_Data"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create working directory\n",
    "os.makedirs('working_files', exist_ok=True)\n",
    "\n",
    "# load in the SQLite database holding the WHP Dataset\n",
    "conn = sqlite3.connect('../../data/sqLiteDb/Whp_Raw_Dataset.db')\n",
    "table_name = \"WHP_EarlyNahuatl_Data\"\n",
    "\n",
    "tables_query = \"SELECT name FROM sqlite_master WHERE type='table';\"\n",
    "tables = pd.read_sql(tables_query, conn)\n",
    "tables\n",
    "\n",
    "\n",
    "# If there's issues check the following\n",
    "# Possible solutions:\n",
    "# 1. Ensure the db file is in the correct directory\n",
    "# 2. Check the exact filename\n",
    "# 3. Verify the file extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cff32578",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_excel(data_dict: Dict[str, pd.DataFrame], filename: str, directory: str = 'working_files'):\n",
    "    \"\"\"Save multiple DataFrames as sheets in an Excel file\"\"\"\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n",
    "        for sheet_name, df in data_dict.items():\n",
    "            # Truncate sheet name if too long (Excel limit is 31 characters)\n",
    "            clean_sheet_name = sheet_name[:31] if len(sheet_name) > 31 else sheet_name\n",
    "            df.to_excel(writer, sheet_name=clean_sheet_name, index=False)\n",
    "    print(f\"Saved to: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2a92de2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe(df: pd.DataFrame, filename: str, directory: str = 'working_files'):\n",
    "    \"\"\"Save a single DataFrame to CSV\"\"\"\n",
    "    filepath = os.path.join(directory, filename)\n",
    "    df.to_csv(filepath, index=False)\n",
    "    print(f\"Saved to: {filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2643a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_sqlite(df: pd.DataFrame, table_name: str, conn: sqlite3.Connection, if_exists: str = 'replace'):\n",
    "    \"\"\"Save DataFrame to SQLite table\"\"\"\n",
    "    df.to_sql(table_name, conn, if_exists=if_exists, index=False)\n",
    "    print(f\"Saved to SQLite table: {table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86643dc",
   "metadata": {},
   "source": [
    "## Step 1: Import Data and Create Working Copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5cdecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_csv(filename: str) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load data and create a working copy\"\"\"\n",
    "    print(f\"Loading data from: {filename}\")\n",
    "\n",
    "    # Read the original data\n",
    "    original_df = pd.read_csv(filename)\n",
    "\n",
    "    # Create working copy\n",
    "    working_df = original_df.copy()\n",
    "\n",
    "    print(f\"Data loaded successfully:\")\n",
    "    print(f\"- Shape: {original_df.shape}\")\n",
    "    print(f\"- Columns: {list(original_df.columns)}\")\n",
    "\n",
    "    return original_df, working_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6b20cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_sqlite(db_path: str, table_name: str = \"WHP_EarlyNahuatl_Data\") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Load data from SQLite and create a working copy\"\"\"\n",
    "    print(f\"Loading data from: {db_path}\")\n",
    "    \n",
    "    conn = sqlite3.connect(db_path)\n",
    "    original_df = pd.read_sql(f\"SELECT * FROM {table_name}\", conn)\n",
    "    working_df = original_df.copy()\n",
    "    \n",
    "    print(f\"Data loaded successfully:\")\n",
    "    print(f\"- Shape: {original_df.shape}\")\n",
    "    print(f\"- Columns: {list(original_df.columns)}\")\n",
    "    \n",
    "    # Don't close connection yet - return it for later use\n",
    "    return original_df, working_df, conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8466f58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ref</th>\n",
       "      <th>Headword</th>\n",
       "      <th>Orthographic Variants</th>\n",
       "      <th>Principal English Translation</th>\n",
       "      <th>Attestations from sources in English</th>\n",
       "      <th>Attestations from sources in Spanish</th>\n",
       "      <th>Alonso de Molina</th>\n",
       "      <th>Frances Karttunen</th>\n",
       "      <th>Horacio Carochi / English</th>\n",
       "      <th>Andrés de Olmos</th>\n",
       "      <th>Lockhart’s Nahuatl as Written</th>\n",
       "      <th>themes</th>\n",
       "      <th>Spanish Loanword</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WHP-171879</td>\n",
       "      <td>acazomo.</td>\n",
       "      <td>accaçomo, acaçomo</td>\n",
       "      <td>&lt;p&gt;perhaps not (adverb) (see Molina)&lt;/p&gt;</td>\n",
       "      <td>&lt;p&gt;acaçomo iuhqui yez yn anoço yuhquiez = whet...</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;p&gt;Acaçomo. quiça no. Aduerbio.&lt;br /&gt; &lt;bibl&gt; A...</td>\n",
       "      <td>&lt;p&gt;AHCAZOMŌ perhaps not / quizá no (M).  In on...</td>\n",
       "      <td>&lt;p&gt;àcaçomō = perhaps not&lt;br /&gt; &lt;bibl&gt;Horacio C...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WHP-171881</td>\n",
       "      <td>ayac.</td>\n",
       "      <td>aiaac</td>\n",
       "      <td>&lt;p&gt;no one; nobody; or, for someone to be absen...</td>\n",
       "      <td>&lt;p&gt;aiaac mic in mexica = None of the Mexica di...</td>\n",
       "      <td>&lt;p&gt;ayac guincuiliz = no se la quite nadie (Tla...</td>\n",
       "      <td>&lt;p&gt;Ayac. ninguno, o nadie o estar alguno ausen...</td>\n",
       "      <td>&lt;p&gt;AYĀC no one / ninguno, o nadie (M) See AH-,...</td>\n",
       "      <td>&lt;p&gt;ayāc = no one&lt;br /&gt; &lt;bibl&gt;Horacio Carochi, ...</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;p&gt;no one; nobody; or, for someone to be absen...</td>\n",
       "      <td>None</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WHP-171882</td>\n",
       "      <td>acan.</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;p&gt;nowhere, no place (see Molina, Karttunen, L...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>&lt;p&gt;acan. en ninguna parte o lugar. aduerbio.&lt;b...</td>\n",
       "      <td>&lt;p&gt;AHCĀN nowhere / en ninguna parte o lugar (M...</td>\n",
       "      <td>&lt;p&gt;àcān = nowhere&lt;br /&gt; &lt;bibl&gt;Horacio Carochi,...</td>\n",
       "      <td>&lt;p&gt;en ningun lugar, por, de, etc.&lt;br /&gt; &lt;bibl&gt;...</td>\n",
       "      <td>&lt;p&gt;ahcān = (particle) nowhere&lt;br /&gt; &lt;bibl&gt;Jame...</td>\n",
       "      <td>Cardinal Directions, Cosmos</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Ref  Headword Orthographic Variants  \\\n",
       "0  WHP-171879  acazomo.     accaçomo, acaçomo   \n",
       "1  WHP-171881     ayac.                 aiaac   \n",
       "2  WHP-171882     acan.                  None   \n",
       "\n",
       "                       Principal English Translation  \\\n",
       "0          <p>perhaps not (adverb) (see Molina)</p>    \n",
       "1  <p>no one; nobody; or, for someone to be absen...   \n",
       "2  <p>nowhere, no place (see Molina, Karttunen, L...   \n",
       "\n",
       "                Attestations from sources in English  \\\n",
       "0  <p>acaçomo iuhqui yez yn anoço yuhquiez = whet...   \n",
       "1  <p>aiaac mic in mexica = None of the Mexica di...   \n",
       "2                                               None   \n",
       "\n",
       "                Attestations from sources in Spanish  \\\n",
       "0                                               None   \n",
       "1  <p>ayac guincuiliz = no se la quite nadie (Tla...   \n",
       "2                                               None   \n",
       "\n",
       "                                    Alonso de Molina  \\\n",
       "0  <p>Acaçomo. quiça no. Aduerbio.<br /> <bibl> A...   \n",
       "1  <p>Ayac. ninguno, o nadie o estar alguno ausen...   \n",
       "2  <p>acan. en ninguna parte o lugar. aduerbio.<b...   \n",
       "\n",
       "                                   Frances Karttunen  \\\n",
       "0  <p>AHCAZOMŌ perhaps not / quizá no (M).  In on...   \n",
       "1  <p>AYĀC no one / ninguno, o nadie (M) See AH-,...   \n",
       "2  <p>AHCĀN nowhere / en ninguna parte o lugar (M...   \n",
       "\n",
       "                           Horacio Carochi / English  \\\n",
       "0  <p>àcaçomō = perhaps not<br /> <bibl>Horacio C...   \n",
       "1  <p>ayāc = no one<br /> <bibl>Horacio Carochi, ...   \n",
       "2  <p>àcān = nowhere<br /> <bibl>Horacio Carochi,...   \n",
       "\n",
       "                                     Andrés de Olmos  \\\n",
       "0                                               None   \n",
       "1                                               None   \n",
       "2  <p>en ningun lugar, por, de, etc.<br /> <bibl>...   \n",
       "\n",
       "                       Lockhart’s Nahuatl as Written  \\\n",
       "0                                               None   \n",
       "1  <p>no one; nobody; or, for someone to be absen...   \n",
       "2  <p>ahcān = (particle) nowhere<br /> <bibl>Jame...   \n",
       "\n",
       "                        themes Spanish Loanword  \n",
       "0                         None               No  \n",
       "1                         None               No  \n",
       "2  Cardinal Directions, Cosmos               No  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ref', 'Headword', 'Orthographic Variants', 'Principal English Translation', 'Attestations from sources in English', 'Attestations from sources in Spanish', 'Alonso de Molina', 'Frances Karttunen', 'Horacio Carochi / English', 'Andrés de Olmos', 'Lockhart’s Nahuatl as Written', 'themes', 'Spanish Loanword']\n"
     ]
    }
   ],
   "source": [
    "# Load your data\n",
    "\n",
    "original_df = pd.read_sql(\"SELECT * FROM WHP_EarlyNahuatl_Data\", conn)\n",
    "df = original_df.copy(deep=True)\n",
    "\n",
    "query = \"SELECT * FROM WHP_EarlyNahuatl_Data LIMIT 3;\"\n",
    "whp_dataset = pd.read_sql(query, conn)\n",
    "display(whp_dataset)\n",
    "\n",
    "cursor = conn.execute(f\"PRAGMA table_info({table_name})\")\n",
    "columns_info = cursor.fetchall()\n",
    "column_names = [col[1] for col in columns_info]\n",
    "\n",
    "print(column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1d64e",
   "metadata": {},
   "source": [
    "## Step 2: Save Intermediate Stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f613b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_intermediate_stage(df: pd.DataFrame, stage_name: str):\n",
    "    \"\"\"Save intermediate processing stage\"\"\"\n",
    "    filename = f\"{stage_name}_stage.csv\"\n",
    "    save_dataframe(df, filename)\n",
    "    return df\n",
    "\n",
    "def save_intermediate_stage_sqlite(df: pd.DataFrame, stage_name: str, conn: sqlite3.Connection):\n",
    "    \"\"\"Save intermediate processing stage to SQLite\"\"\"\n",
    "    table_name = f\"{stage_name}_stage\"\n",
    "    save_to_sqlite(df, table_name, conn)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45105fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save initial stage\n",
    "# save_intermediate_stage_sqlite(df, \"01_initial\", conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ca7114",
   "metadata": {},
   "source": [
    "## Step 3: HTML Tag Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c717f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLTagAnalyzer:\n",
    "    def __init__(self):\n",
    "        # HTML tags\n",
    "        self.html_tags = {\n",
    "            'p', 'br', 'div', 'span', 'a', 'b', 'i', 'u', 'strong', 'em',\n",
    "            'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'li', 'table',\n",
    "            'tr', 'td', 'th', 'img', 'link', 'meta', 'head', 'body', 'html',\n",
    "            'bibl', 'title', 'sup', 'sub', 'del'\n",
    "        }\n",
    "        \n",
    "        # Define columns that should contain HTML content\n",
    "        self.content_columns = [\n",
    "            'Principal English Translation',\n",
    "            'Attestations from sources in English',\n",
    "            'Attestations from sources in Spanish',\n",
    "            'Alonso de Molina',\n",
    "            'Frances Karttunen', \n",
    "            'Horacio Carochi / English',\n",
    "            'Andrés de Olmos',\n",
    "            \"Lockhart's Nahuatl as Written\",\n",
    "            'Full Original Entry'\n",
    "        ]\n",
    "        \n",
    "        # Known malformed patterns to fix\n",
    "        self.malformed_patterns = {\n",
    "            r'</p</bibl>': '</p></bibl>',\n",
    "            r'<bibl<': '<bibl>',\n",
    "            r'</bibbl>': '</bibl>',\n",
    "            r'<bibbl>': '<bibl>',\n",
    "            r'<bobl>': '<bibl>',\n",
    "            r'</bobl>': '</bibl>',\n",
    "            r'<b9bl>': '<bibl>',\n",
    "            r'<bibi>': '<bibl>',\n",
    "            r'<bibl></p>': '</bibl></p>',\n",
    "        }\n",
    "    \n",
    "    def detect_malformed_tags(self, text: str) -> List[tuple]:\n",
    "        \"\"\"Detect specific malformed tag patterns\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return []\n",
    "        \n",
    "        malformed_found = []\n",
    "        text_str = str(text)\n",
    "        \n",
    "        # Check for known malformed patterns\n",
    "        for pattern, replacement in self.malformed_patterns.items():\n",
    "            if re.search(pattern, text_str):\n",
    "                malformed_found.append((pattern, replacement))\n",
    "        \n",
    "        # Define self-closing tags that shouldn't be counted in pair matching\n",
    "        self_closing_tags = {'br', 'hr', 'img', 'input', 'meta', 'link'}\n",
    "        \n",
    "        # Better tag counting using regex\n",
    "        for tag_name in self.html_tags:\n",
    "            if tag_name in self_closing_tags:\n",
    "                continue  # Skip self-closing tags\n",
    "            \n",
    "            # Use regex to properly count opening tags (with or without attributes)\n",
    "            # Matches <tag> or <tag attr=\"...\">~\n",
    "            open_pattern = f\"<{tag_name}(?:\\\\s+[^>]*)?>\"\n",
    "            close_pattern = f'</{tag_name}>'\n",
    "            \n",
    "            open_count = len(re.findall(open_pattern, text_str, re.IGNORECASE))\n",
    "            close_count = len(re.findall(close_pattern, text_str, re.IGNORECASE))\n",
    "            \n",
    "            if open_count != close_count:\n",
    "                malformed_found.append((f'<{tag_name}>', f'Mismatch: {open_count} open, {close_count} closed'))\n",
    "        return malformed_found\n",
    "    \n",
    "    def find_html_tags(self, text: str) -> List[str]:\n",
    "        \"\"\"Find all HTML-like tags in text with better handling of malformed tags\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return []\n",
    "        \n",
    "        # First fix known malformed patterns\n",
    "        text_str = str(text)\n",
    "        for pattern, replacement in self.malformed_patterns.items():\n",
    "            text_str = re.sub(pattern, replacement, text_str)\n",
    "        \n",
    "        # Then find tags\n",
    "        pattern = r'</?[^<>]+/?>'\n",
    "        matches = re.findall(pattern, text_str)\n",
    "        return matches\n",
    "    \n",
    "    def analyze_html_tags_in_dataframe(self, df: pd.DataFrame, \n",
    "                                      columns_to_check: List[str] = None) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Analyze HTML tags only in relevant columns\"\"\"\n",
    "        results = {\n",
    "            'tag_by_row': [],\n",
    "            'tag_summary': [],\n",
    "            'malformed_tags': []\n",
    "        }\n",
    "        \n",
    "        # Use specified columns or default to content columns\n",
    "        if columns_to_check is None:\n",
    "            columns_to_check = [col for col in self.content_columns if col in df.columns]\n",
    "        \n",
    "        # Track tags by row - only in relevant columns\n",
    "        for idx, row in df.iterrows():\n",
    "            for col in columns_to_check:\n",
    "                if col not in df.columns:\n",
    "                    continue\n",
    "                    \n",
    "                cell_value = row[col]\n",
    "                if pd.notna(cell_value) and cell_value != '':\n",
    "                    # Check for malformed tags first\n",
    "                    malformed = self.detect_malformed_tags(cell_value)\n",
    "                    if malformed:\n",
    "                        for pattern, fix in malformed:\n",
    "                            results['malformed_tags'].append({\n",
    "                                'Row': idx,\n",
    "                                'Column': col,\n",
    "                                'Pattern': pattern,\n",
    "                                'Suggested_Fix': fix,\n",
    "                                'Context': str(cell_value)[:100] + '...' if len(str(cell_value)) > 100 else str(cell_value)\n",
    "                            })\n",
    "                    \n",
    "                    # Find tags\n",
    "                    tags = self.find_html_tags(cell_value)\n",
    "                    for tag in tags:\n",
    "                        is_valid = self.is_valid_html_tag(tag)\n",
    "                        context = self.get_tag_context(cell_value, tag)\n",
    "                        results['tag_by_row'].append({\n",
    "                            'Row': idx,\n",
    "                            'Column': col,\n",
    "                            'Tag': tag,\n",
    "                            'Is_Valid_HTML': is_valid,\n",
    "                            'Context': context\n",
    "                        })\n",
    "        \n",
    "        # Create summaries\n",
    "        if results['tag_by_row']:\n",
    "            tag_by_row_df = pd.DataFrame(results['tag_by_row'])\n",
    "            \n",
    "            # Tag summary\n",
    "            tag_counts = Counter([item['Tag'] for item in results['tag_by_row']])\n",
    "            tag_locations = defaultdict(list)\n",
    "            \n",
    "            for item in results['tag_by_row']:\n",
    "                tag_locations[item['Tag']].append(f\"Row {item['Row']}, Col {item['Column']}\")\n",
    "            \n",
    "            for tag, count in tag_counts.items():\n",
    "                first_occurrence = next(item for item in results['tag_by_row'] if item['Tag'] == tag)\n",
    "                results['tag_summary'].append({\n",
    "                    'Tag': tag,\n",
    "                    'Count': count,\n",
    "                    'Is_Valid_HTML': first_occurrence['Is_Valid_HTML'],\n",
    "                    'Locations': '; '.join(tag_locations[tag][:5]) + ('...' if len(tag_locations[tag]) > 5 else ''),\n",
    "                    'Sample_Context': first_occurrence['Context']\n",
    "                })\n",
    "            \n",
    "            tag_summary_df = pd.DataFrame(results['tag_summary']).sort_values('Count', ascending=False)\n",
    "        else:\n",
    "            tag_by_row_df = pd.DataFrame()\n",
    "            tag_summary_df = pd.DataFrame()\n",
    "        \n",
    "        malformed_df = pd.DataFrame(results['malformed_tags']) if results['malformed_tags'] else pd.DataFrame()\n",
    "        \n",
    "        return {\n",
    "            'HTML_Tags_by_Row': tag_by_row_df,\n",
    "            'HTML_Tags_Summary': tag_summary_df,\n",
    "            'Malformed_Tags': malformed_df\n",
    "        }\n",
    "    \n",
    "    def is_valid_html_tag(self, tag: str) -> bool:\n",
    "        \"\"\"Check if a tag is a valid HTML tag with better error handling\"\"\"\n",
    "        try:\n",
    "            # Handle malformed tags better\n",
    "            if '<//' in tag or '><' in tag:  # Clearly malformed\n",
    "                return False\n",
    "            \n",
    "            # Remove < > and any attributes, get just the tag name\n",
    "            clean_tag = re.sub(r'^</?([^>\\s/]+).*>$', r'\\1', tag).lower()\n",
    "            \n",
    "            # Additional check for malformed tags\n",
    "            if '/' in clean_tag or '<' in clean_tag or '>' in clean_tag:\n",
    "                return False\n",
    "                \n",
    "            return clean_tag in self.html_tags\n",
    "        except:\n",
    "            return False\n",
    "    \n",
    "    def get_tag_context(self, text: str, tag: str, context_chars: int = 50) -> str:\n",
    "        \"\"\"Get context around a tag occurrence\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return ''\n",
    "        \n",
    "        text_str = str(text)\n",
    "        tag_pos = text_str.find(tag)\n",
    "        if tag_pos == -1:\n",
    "            return ''\n",
    "        \n",
    "        start = max(0, tag_pos - context_chars)\n",
    "        end = min(len(text_str), tag_pos + len(tag) + context_chars)\n",
    "        context = text_str[start:end]\n",
    "        \n",
    "        # Mark the tag in the context\n",
    "        tag_in_context = context.replace(tag, f\"[[[{tag}]]]\")\n",
    "        return tag_in_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3026a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: working_files\\02_html_tag_analysis.xlsx\n"
     ]
    }
   ],
   "source": [
    "html_analyzer = HTMLTagAnalyzer()\n",
    "html_results = html_analyzer.analyze_html_tags_in_dataframe(df)\n",
    "\n",
    "html_results['HTML_Tags_Summary'].to_sql('html_tag_analysis', conn, if_exists='replace', index=False)\n",
    "html_results['Malformed_Tags'].to_sql('malformed_tags', conn, if_exists='replace', index=False)\n",
    "\n",
    "# save_intermediate_stage_sqlite(df, \"02_htmltag_analysis\", conn)\n",
    "save_to_excel(html_results, \"02_html_tag_analysis.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09fe05",
   "metadata": {},
   "source": [
    "## Step 4: Malformed Tag Detection and Repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "83b1a2d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MalformedTagRepairer:\n",
    "    def __init__(self):\n",
    "        self.html_analyzer = HTMLTagAnalyzer()\n",
    "        \n",
    "        # Specific patterns for actual HTML malformations\n",
    "        self.html_malformation_patterns = {\n",
    "            # Bibliography tag typos\n",
    "            r'</bibbl>': '</bibl>',\n",
    "            r'<bibbl>': '<bibl>',\n",
    "            r'<\\?bibl>': '<bibl>',\n",
    "            r'<bibl<': '<bibl>',\n",
    "            r'</bobl>': '</bibl>',\n",
    "            r'<bobl>': '<bibl>',\n",
    "            r'<b9bl>': '<bibl>',\n",
    "            r'<bibi>': '<bibl>',\n",
    "            \n",
    "            # Paragraph tag issues\n",
    "            r'</p</bibl>': '</p></bibl>',\n",
    "            r'<p<': '<p>',\n",
    "            r'</p>p>': '</p>',\n",
    "            \n",
    "            # Structural issues\n",
    "            r'<<(\\w+)>': r'<\\1>',        # Double opening: <<bibl> → <bibl>\n",
    "            r'<(\\w+)>>': r'<\\1>',        # Double closing: <bibl>> → <bibl>\n",
    "            r'<(\\w+)\\s+<': r'<\\1>',      # Unclosed with new tag: <bibl <p> → <bibl>\n",
    "            \n",
    "            # Common typos\n",
    "            r'<stron>': '<strong>',\n",
    "            r'</stron>': '</strong>',\n",
    "            r'<em >': '<em>',\n",
    "            r'< (\\w+)>': r'<\\1>',        # Space after bracket: < bibl> → <bibl>\n",
    "        }\n",
    "        \n",
    "        # HTML tag patterns that indicate this IS supposed to be HTML\n",
    "        self.html_indicators = [\n",
    "            r'^</?(?:bibl|p|strong|em|b|i|u|sup|sub|div|span|a|br|h[1-6])[\\s>]',\n",
    "            r'</\\w+>$',                  # Closing tags\n",
    "            r'<\\w+\\s+\\w+=\"[^\"]*\"',      # Tags with attributes\n",
    "        ]\n",
    "\n",
    "    def is_definitely_non_html(self, tag: str) -> bool:\n",
    "        \"\"\"Conservative check - is this definitely NOT HTML?\"\"\"\n",
    "        for pattern in self.non_html_patterns:\n",
    "            if re.search(pattern, tag):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def is_close_to_html(self, tag: str) -> bool:\n",
    "        \"\"\"MUCH more conservative - only obvious HTML-like malformations\"\"\"\n",
    "        # If it's definitely non-HTML content, don't even consider it\n",
    "        if self.is_definitely_non_html(tag):\n",
    "            return False\n",
    "        \n",
    "        # Only consider malformed if it's in our exact repair list\n",
    "        if tag in self.exact_repairs:\n",
    "            return True\n",
    "            \n",
    "        # Or if it has obvious HTML tag structure problems\n",
    "        malformed_patterns = [\n",
    "            r'<[^>]*</[^>]*>',  # Mixed opening/closing in one tag\n",
    "            r'</[^>]*<[^>]*>',  # Reversed brackets  \n",
    "            r'<[^>]*<[^>]*>',   # Double opening brackets\n",
    "        ]\n",
    "        \n",
    "        for pattern in malformed_patterns:\n",
    "            if re.search(pattern, tag):\n",
    "                return True\n",
    "                \n",
    "        return False\n",
    "    \n",
    "    def is_likely_malformed_html(self, tag: str) -> bool:\n",
    "        \"\"\"Check if this tag looks like malformed HTML (not just non-HTML content)\"\"\"\n",
    "        tag_lower = tag.lower()\n",
    "        \n",
    "        # Check if it matches any HTML indicators\n",
    "        for pattern in self.html_indicators:\n",
    "            if re.search(pattern, tag_lower):\n",
    "                return True\n",
    "        \n",
    "        # Check against known malformation patterns\n",
    "        for pattern in self.html_malformation_patterns.keys():\n",
    "            if re.search(pattern, tag):\n",
    "                return True\n",
    "                \n",
    "        return False\n",
    "    \n",
    "    \n",
    "\n",
    "    def find_malformed_tags(self, text: str) -> List[str]:\n",
    "        \"\"\"Find only actual malformed HTML tags\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return []\n",
    "\n",
    "        malformed_tags = []\n",
    "        \n",
    "        # Direct pattern matching for known malformations\n",
    "        for pattern, replacement in self.html_malformation_patterns.items():\n",
    "            matches = re.findall(pattern, str(text))\n",
    "            malformed_tags.extend(matches)\n",
    "        \n",
    "        # Additional check for malformed structure\n",
    "        all_brackets = re.findall(r'<[^<>]*>', str(text))\n",
    "        for tag in all_brackets:\n",
    "            if (not self.html_analyzer.is_valid_html_tag(tag) and \n",
    "                self.is_likely_malformed_html(tag) and\n",
    "                tag not in malformed_tags):  # Avoid duplicates\n",
    "                malformed_tags.append(tag)\n",
    "        \n",
    "        return malformed_tags\n",
    "\n",
    "    def is_close_to_html(self, tag: str) -> bool:\n",
    "        \"\"\"Check if a malformed tag is close to valid HTML\"\"\"\n",
    "        malformed_patterns = [\n",
    "            r'<[^>]*</[^>]*>',  # Mixed opening/closing\n",
    "            r'</[^>]*<[^>]*>',  # Reversed brackets\n",
    "            r'<[^>]*<[^>]*>',   # Double opening\n",
    "            r'<[^/>][^>]*[^/]>$', # Missing closing slash or improper format\n",
    "        ]\n",
    "\n",
    "        for pattern in malformed_patterns:\n",
    "            if re.search(pattern, tag):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def suggest_repair(self, tag: str) -> str:\n",
    "        \"\"\"Suggest repair for malformed HTML tags\"\"\"\n",
    "        tag = tag.strip()\n",
    "        \n",
    "        # Check direct pattern matches first\n",
    "        for pattern, replacement in self.html_malformation_patterns.items():\n",
    "            if re.search(pattern, tag):\n",
    "                return re.sub(pattern, replacement, tag)\n",
    "        \n",
    "        # Additional repair logic\n",
    "        tag_lower = tag.lower()\n",
    "        \n",
    "        # Fix common spacing issues\n",
    "        if re.match(r'^<\\s+\\w+', tag):\n",
    "            return re.sub(r'^<\\s+', '<', tag)\n",
    "        if re.match(r'^<\\w+\\s+>', tag):\n",
    "            return re.sub(r'\\s+>$', '>', tag)\n",
    "            \n",
    "        # Return unchanged if no clear repair\n",
    "        return tag\n",
    "    def analyze_malformed_tags(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Analyze only actual malformed HTML tags across DataFrame\"\"\"\n",
    "        results = {\n",
    "            'malformed_by_row': [],\n",
    "            'malformed_summary': []\n",
    "        }\n",
    "\n",
    "        print(\"Scanning for actual HTML malformations...\")\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            for col in df.columns:\n",
    "                cell_value = row[col]\n",
    "                if pd.notna(cell_value) and cell_value != '':\n",
    "                    malformed_tags = self.find_malformed_tags(cell_value)\n",
    "                    for tag in malformed_tags:\n",
    "                        context = self.html_analyzer.get_tag_context(cell_value, tag)\n",
    "                        suggested_repair = self.suggest_repair(tag)\n",
    "                        results['malformed_by_row'].append({\n",
    "                            'Row': idx,\n",
    "                            'Column': col,\n",
    "                            'Malformed_Tag': tag,\n",
    "                            'Suggested_Repair': suggested_repair,\n",
    "                            'Context': context\n",
    "                        })\n",
    "\n",
    "        # Create summary\n",
    "        tag_counts = Counter([item['Malformed_Tag'] for item in results['malformed_by_row']])\n",
    "        tag_locations = defaultdict(list)\n",
    "\n",
    "        for item in results['malformed_by_row']:\n",
    "            tag_locations[item['Malformed_Tag']].append(f\"Row {item['Row']}, Col {item['Column']}\")\n",
    "\n",
    "        for tag, count in tag_counts.items():\n",
    "            first_occurrence = next(item for item in results['malformed_by_row'] if item['Malformed_Tag'] == tag)\n",
    "            results['malformed_summary'].append({\n",
    "                'Malformed_Tag': tag,\n",
    "                'Count': count,\n",
    "                'Suggested_Repair': first_occurrence['Suggested_Repair'],\n",
    "                'Locations': '; '.join(tag_locations[tag][:5]) + ('...' if len(tag_locations[tag]) > 5 else ''),\n",
    "                'Sample_Context': first_occurrence['Context']\n",
    "            })\n",
    "\n",
    "        malformed_by_row_df = pd.DataFrame(results['malformed_by_row'])\n",
    "        malformed_summary_df = pd.DataFrame(results['malformed_summary']).sort_values('Count', ascending=False)\n",
    "\n",
    "        print(f\"Found {len(malformed_by_row_df)} actual HTML malformations\")\n",
    "        \n",
    "        return {\n",
    "            'Malformed_Tags_by_Row': malformed_by_row_df,\n",
    "            'Malformed_Tags_Summary': malformed_summary_df\n",
    "        }\n",
    "\n",
    "    def repair_tags(self, df: pd.DataFrame, tag_to_repair: str, replacement: str,\n",
    "               scope: str = 'global', specific_column: str = None,\n",
    "               specific_row: int = None) -> pd.DataFrame:\n",
    "        \"\"\"Repair malformed tags in DataFrame\"\"\"\n",
    "        df_repaired = df.copy()\n",
    "        \n",
    "        print(\".\" * 40)\n",
    "        print(f\"Repairing: '{tag_to_repair}' → '{replacement}'\")\n",
    "        print(\">\" * 40)\n",
    "        \n",
    "        if scope == 'global':\n",
    "            replacements_made = 0\n",
    "            for col in df_repaired.columns:\n",
    "                # Count before\n",
    "                before_count = df_repaired[col].astype(str).str.contains(\n",
    "                    re.escape(tag_to_repair), regex=True\n",
    "                ).sum()\n",
    "                \n",
    "                # Make replacement - DON'T escape the replacement\n",
    "                df_repaired[col] = df_repaired[col].astype(str).str.replace(\n",
    "                    tag_to_repair, replacement, regex=False  # Use literal replacement\n",
    "                )\n",
    "                \n",
    "                # Count after\n",
    "                after_count = df_repaired[col].astype(str).str.contains(\n",
    "                    re.escape(tag_to_repair), regex=True\n",
    "                ).sum()\n",
    "                \n",
    "                column_replacements = before_count - after_count\n",
    "                if column_replacements > 0:\n",
    "                    print(f\"\\tColumn '{col}': {column_replacements} replacements\")\n",
    "                    # Show sample context with actual replacement\n",
    "                    sample_rows = df_repaired[col].astype(str).str.contains(\n",
    "                        re.escape(replacement), regex=True\n",
    "                    )\n",
    "                    if sample_rows.any():\n",
    "                        sample_idx = sample_rows.idxmax()\n",
    "                        context = self.html_analyzer.get_tag_context(\n",
    "                            df_repaired.loc[sample_idx, col], replacement\n",
    "                        )\n",
    "                        print(f\"\\t\\tSample context: {context[:100]}...\")\n",
    "                \n",
    "                replacements_made += column_replacements\n",
    "                \n",
    "            print(f\"Total replacements made: {replacements_made}\")\n",
    "            \n",
    "        elif scope == 'column' and specific_column:\n",
    "            if specific_column in df_repaired.columns:\n",
    "                df_repaired[specific_column] = df_repaired[specific_column].astype(str).str.replace(\n",
    "                    tag_to_repair, replacement, regex=False\n",
    "                )\n",
    "        elif scope == 'row' and specific_row is not None:\n",
    "            for col in df_repaired.columns:\n",
    "                if pd.notna(df_repaired.loc[specific_row, col]):\n",
    "                    cell_value = str(df_repaired.loc[specific_row, col])\n",
    "                    df_repaired.loc[specific_row, col] = cell_value.replace(tag_to_repair, replacement)\n",
    "        elif scope == 'cell' and specific_column and specific_row is not None:\n",
    "            if specific_column in df_repaired.columns and pd.notna(df_repaired.loc[specific_row, specific_column]):\n",
    "                cell_value = str(df_repaired.loc[specific_row, specific_column])\n",
    "                df_repaired.loc[specific_row, specific_column] = cell_value.replace(tag_to_repair, replacement)\n",
    "        \n",
    "        return df_repaired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be8a3c2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Analyzing malformed tags...\n",
      "Scanning for actual HTML malformations...\n",
      "Found 16 actual HTML malformations\n",
      "Malformed Tags Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Malformed_Tag</th>\n",
       "      <th>Count</th>\n",
       "      <th>Suggested_Repair</th>\n",
       "      <th>Locations</th>\n",
       "      <th>Sample_Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;bibbl&gt;</td>\n",
       "      <td>7</td>\n",
       "      <td>&lt;bibl&gt;</td>\n",
       "      <td>Row 18189, Col Attestations from sources in En...</td>\n",
       "      <td>e soil. (central Mexico, sixteenth century)&lt;br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;bibl&lt;</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;bibl&gt;</td>\n",
       "      <td>Row 2927, Col Attestations from sources in Spa...</td>\n",
       "      <td>fibras vegetales para transportar granos.\"&lt;br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>em</td>\n",
       "      <td>1</td>\n",
       "      <td>em</td>\n",
       "      <td>Row 2927, Col Attestations from sources in Spa...</td>\n",
       "      <td>s vegetales para transportar granos.\"&lt;br /&gt; &lt;b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;/wup&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;/wup&gt;</td>\n",
       "      <td>Row 3616, Col Attestations from sources in Eng...</td>\n",
       "      <td>l&gt;&lt;/p&gt; &lt;p&gt;ynic omochiuh missas 10 &lt;strong&gt;p&lt;wu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;bibi&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;bibl&gt;</td>\n",
       "      <td>Row 16873, Col Attestations from sources in En...</td>\n",
       "      <td>te: a \"tonsured\" priest had a shaved head.]&lt;br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>bibl</td>\n",
       "      <td>1</td>\n",
       "      <td>bibl</td>\n",
       "      <td>Row 19677, Col Attestations from sources in En...</td>\n",
       "      <td>ished. (central Mexico, sixteenth century)&lt;br ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;b9bl&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;bibl&gt;</td>\n",
       "      <td>Row 30486, Col Attestations from sources in En...</td>\n",
       "      <td>son of a Xonacatl who had died around 1530.&lt;br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;bobl&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;bibl&gt;</td>\n",
       "      <td>Row 30849, Col Alonso de Molina</td>\n",
       "      <td>uixti.) enxaguar la ropa despues de lauada.&lt;br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;/bobl&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>&lt;/bibl&gt;</td>\n",
       "      <td>Row 31700, Col Attestations from sources in Sp...</td>\n",
       "      <td>blicaciones/publicadigital/libros/cuentos_i......</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Malformed_Tag  Count Suggested_Repair  \\\n",
       "4       <bibbl>      7           <bibl>   \n",
       "0        <bibl<      2           <bibl>   \n",
       "1            em      1               em   \n",
       "2        </wup>      1           </wup>   \n",
       "3        <bibi>      1           <bibl>   \n",
       "5          bibl      1             bibl   \n",
       "6        <b9bl>      1           <bibl>   \n",
       "7        <bobl>      1           <bibl>   \n",
       "8       </bobl>      1          </bibl>   \n",
       "\n",
       "                                           Locations  \\\n",
       "4  Row 18189, Col Attestations from sources in En...   \n",
       "0  Row 2927, Col Attestations from sources in Spa...   \n",
       "1  Row 2927, Col Attestations from sources in Spa...   \n",
       "2  Row 3616, Col Attestations from sources in Eng...   \n",
       "3  Row 16873, Col Attestations from sources in En...   \n",
       "5  Row 19677, Col Attestations from sources in En...   \n",
       "6  Row 30486, Col Attestations from sources in En...   \n",
       "7                    Row 30849, Col Alonso de Molina   \n",
       "8  Row 31700, Col Attestations from sources in Sp...   \n",
       "\n",
       "                                      Sample_Context  \n",
       "4  e soil. (central Mexico, sixteenth century)<br...  \n",
       "0   fibras vegetales para transportar granos.\"<br...  \n",
       "1  s vegetales para transportar granos.\"<br /> <b...  \n",
       "2  l></p> <p>ynic omochiuh missas 10 <strong>p<wu...  \n",
       "3  te: a \"tonsured\" priest had a shaved head.]<br...  \n",
       "5  ished. (central Mexico, sixteenth century)<br ...  \n",
       "6  son of a Xonacatl who had died around 1530.<br...  \n",
       "7  uixti.) enxaguar la ropa despues de lauada.<br...  \n",
       "8  blicaciones/publicadigital/libros/cuentos_i......  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying repairs...\n",
      "Repairing '<bibbl>' -> '<bibl>'\n",
      "........................................\n",
      "Repairing: '<bibbl>' → '<bibl>'\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\tColumn 'Attestations from sources in English': 7 replacements\n",
      "\t\tSample context: ne or not (Coyoacan, mid-sixteenth century)<br /> [[[<bibl>]]]Beyond the Codices, eds. Arthur J.O. A...\n",
      "Total replacements made: 7\n",
      "Repairing '<bibl<' -> '<bibl>'\n",
      "........................................\n",
      "Repairing: '<bibl<' → '<bibl>'\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\tColumn 'Principal English Translation': 1 replacements\n",
      "\t\tSample context: <p>to do or make all; completely</p> <p>[[[<bibl>]]]Robert Haskett and Stephanie Wood's notes from N...\n",
      "\tColumn 'Attestations from sources in Spanish': 1 replacements\n",
      "\t\tSample context: liz = no se la quite nadie (Tlaxcala, 1609)<br /> [[[<bibl>]]]Vidas y bienes olvidados: Testamentos ...\n",
      "Total replacements made: 2\n",
      "Tag 'em' needs no repair\n",
      "Tag '</wup>' needs no repair\n",
      "Repairing '<bibi>' -> '<bibl>'\n",
      "........................................\n",
      "Repairing: '<bibi>' → '<bibl>'\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\tColumn 'Attestations from sources in English': 1 replacements\n",
      "\t\tSample context: ne or not (Coyoacan, mid-sixteenth century)<br /> [[[<bibl>]]]Beyond the Codices, eds. Arthur J.O. A...\n",
      "Total replacements made: 1\n",
      "Tag 'bibl' needs no repair\n",
      "Repairing '<b9bl>' -> '<bibl>'\n",
      "........................................\n",
      "Repairing: '<b9bl>' → '<bibl>'\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\tColumn 'Attestations from sources in English': 1 replacements\n",
      "\t\tSample context: ne or not (Coyoacan, mid-sixteenth century)<br /> [[[<bibl>]]]Beyond the Codices, eds. Arthur J.O. A...\n",
      "Total replacements made: 1\n",
      "Repairing '<bobl>' -> '<bibl>'\n",
      "........................................\n",
      "Repairing: '<bobl>' → '<bibl>'\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\tColumn 'Alonso de Molina': 1 replacements\n",
      "\t\tSample context: <p>Acaçomo. quiça no. Aduerbio.<br /> [[[<bibl>]]] Alonso de Molina, Vocabulario en lengua mexicana ...\n",
      "Total replacements made: 1\n",
      "Repairing '</bobl>' -> '</bibl>'\n",
      "........................................\n",
      "Repairing: '</bobl>' → '</bibl>'\n",
      ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "\tColumn 'Attestations from sources in Spanish': 1 replacements\n",
      "\t\tSample context: abiela, et al, eds. (México: CIESAS, 2002), 62–63.[[[</bibl>]]]</p> <p>ayac yconetzin = no tiene hij...\n",
      "Total replacements made: 1\n",
      "✓ Applied 9 HTML repairs\n",
      "Saved to: working_files\\04_malformed_tag_analysis.xlsx\n",
      "Saved to SQLite table: 04_malformed_repair_stage\n",
      "Step 4 complete: HTML malformation detection and repair\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Malformed Tag Detection and Repair\n",
    "print(\"Step 4: Analyzing malformed tags...\")\n",
    "\n",
    "# Use the enhanced MalformedTagRepairer class\n",
    "malformed_repairer = MalformedTagRepairer()\n",
    "malformed_results = malformed_repairer.analyze_malformed_tags(df)\n",
    "\n",
    "print(\"Malformed Tags Summary:\")\n",
    "if not malformed_results['Malformed_Tags_Summary'].empty:\n",
    "    display(malformed_results['Malformed_Tags_Summary'])\n",
    "    \n",
    "    # Save to SQLite\n",
    "    malformed_results['Malformed_Tags_Summary'].to_sql(\n",
    "        'malformed_tags_summary', conn, if_exists='replace', index=False\n",
    "    )\n",
    "    malformed_results['Malformed_Tags_by_Row'].to_sql(\n",
    "        'malformed_tags_by_row', conn, if_exists='replace', index=False\n",
    "    )\n",
    "    \n",
    "    # Apply repairs automatically\n",
    "    print(\"\\nApplying repairs...\")\n",
    "    for _, row in malformed_results['Malformed_Tags_Summary'].iterrows():  # Fixed syntax\n",
    "        malformed_tag = row['Malformed_Tag']\n",
    "        suggested_repair = row['Suggested_Repair']\n",
    "        if malformed_tag != suggested_repair:\n",
    "            print(f\"Repairing '{malformed_tag}' -> '{suggested_repair}'\")\n",
    "            df = malformed_repairer.repair_tags(\n",
    "                df, malformed_tag, suggested_repair, scope='global'\n",
    "            )\n",
    "        else:\n",
    "            print(f\"Tag '{malformed_tag}' needs no repair\")\n",
    "    \n",
    "    print(f\"✓ Applied {len(malformed_results['Malformed_Tags_Summary'])} HTML repairs\")\n",
    "    \n",
    "else:\n",
    "    print(\"No malformed HTML tags found!\")\n",
    "\n",
    "# Save to Excel and SQLite\n",
    "save_to_excel(malformed_results, \"04_malformed_tag_analysis.xlsx\")\n",
    "save_intermediate_stage_sqlite(df, \"04_malformed_repair\", conn)\n",
    "\n",
    "print(\"Step 4 complete: HTML malformation detection and repair\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6966f3",
   "metadata": {},
   "source": [
    "## Step 5: Non-HTML Tag Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2cffa486",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonHTMLTagProcessor:\n",
    "    def __init__(self):\n",
    "        self.html_analyzer = HTMLTagAnalyzer()\n",
    "        \n",
    "        # Known HTML tags that should never be encoded\n",
    "        self.valid_html_tags = {\n",
    "            'p', 'br', 'div', 'span', 'a', 'b', 'i', 'u', 'strong', 'em',\n",
    "            'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'li', 'table',\n",
    "            'tr', 'td', 'th', 'img', 'link', 'meta', 'head', 'body', 'html',\n",
    "            'bibl', 'title', 'sup', 'sub', 'del'\n",
    "        }\n",
    "\n",
    "    def get_tag_name(self, tag: str) -> str:\n",
    "        \"\"\"Extract just the tag name from a tag\"\"\"\n",
    "        match = re.match(r'^</?([a-zA-Z][a-zA-Z0-9]*)', tag)\n",
    "        if match:\n",
    "            return match.group(1).lower()\n",
    "        return \"\"\n",
    "\n",
    "    def find_non_html_tags(self, text: str) -> List[str]:\n",
    "        \"\"\"Find tags that are definitely not HTML\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return []\n",
    "\n",
    "        # Find all < ... > patterns\n",
    "        pattern = r'<[^<>]*>'\n",
    "        all_brackets = re.findall(pattern, str(text))\n",
    "\n",
    "        non_html_tags = []\n",
    "        for tag in all_brackets:\n",
    "            # Skip if it's valid HTML\n",
    "            if self.html_analyzer.is_valid_html_tag(tag):\n",
    "                continue\n",
    "                \n",
    "            # Skip if it looks like malformed HTML (should have been caught in Step 4)\n",
    "            if self._looks_like_malformed_html(tag):\n",
    "                continue\n",
    "                \n",
    "            # This must be non-HTML content (linguistic, descriptive, etc.)\n",
    "            non_html_tags.append(tag)\n",
    "\n",
    "        return non_html_tags\n",
    "\n",
    "    def _looks_like_malformed_html(self, tag: str) -> bool:\n",
    "        \"\"\"Check if tag looks like malformed HTML (should have been caught in Step 4)\"\"\"\n",
    "        tag_name = self.get_tag_name(tag)\n",
    "        \n",
    "        # If it has a recognizable HTML tag name, it's likely malformed HTML\n",
    "        if tag_name in self.valid_html_tags:\n",
    "            return True\n",
    "            \n",
    "        # Check for HTML-like patterns\n",
    "        html_like_patterns = [\n",
    "            r'^</?[a-zA-Z][a-zA-Z0-9]*\\s',    # Tag with attributes\n",
    "            r'bibl', r'href', r'www', r'http', # HTML-related content\n",
    "        ]\n",
    "        \n",
    "        for pattern in html_like_patterns:\n",
    "            if re.search(pattern, tag, re.IGNORECASE):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def analyze_non_html_tags(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Analyze non-HTML tags across DataFrame\"\"\"\n",
    "        results = {\n",
    "            'non_html_by_row': [],\n",
    "            'non_html_summary': []\n",
    "        }\n",
    "\n",
    "        print(\"Scanning for non-HTML content in angle brackets...\")\n",
    "\n",
    "        for idx, row in df.iterrows():\n",
    "            for col in df.columns:\n",
    "                cell_value = row[col]\n",
    "                if pd.notna(cell_value) and cell_value != '':\n",
    "                    non_html_tags = self.find_non_html_tags(cell_value)\n",
    "                    for tag in non_html_tags:\n",
    "                        context = self.html_analyzer.get_tag_context(cell_value, tag)\n",
    "                        results['non_html_by_row'].append({\n",
    "                            'Row': idx,\n",
    "                            'Column': col,\n",
    "                            'Non_HTML_Tag': tag,\n",
    "                            'Context': context\n",
    "                        })\n",
    "\n",
    "        # Create summary\n",
    "        tag_counts = Counter([item['Non_HTML_Tag'] for item in results['non_html_by_row']])\n",
    "        tag_locations = defaultdict(list)\n",
    "\n",
    "        for item in results['non_html_by_row']:\n",
    "            tag_locations[item['Non_HTML_Tag']].append(f\"Row {item['Row']}, Col {item['Column']}\")\n",
    "\n",
    "        for tag, count in tag_counts.items():\n",
    "            first_occurrence = next(item for item in results['non_html_by_row'] if item['Non_HTML_Tag'] == tag)\n",
    "            results['non_html_summary'].append({\n",
    "                'Non_HTML_Tag': tag,\n",
    "                'Count': count,\n",
    "                'Locations': '; '.join(tag_locations[tag][:5]) + ('...' if len(tag_locations[tag]) > 5 else ''),\n",
    "                'Sample_Context': first_occurrence['Context']\n",
    "            })\n",
    "\n",
    "        # Convert to DataFrames\n",
    "        non_html_by_row_df = pd.DataFrame(results['non_html_by_row'])\n",
    "        non_html_summary_df = pd.DataFrame(results['non_html_summary']).sort_values('Count', ascending=False)\n",
    "\n",
    "        print(f\"Found {len(non_html_by_row_df)} non-HTML tags for encoding\")\n",
    "\n",
    "        return {\n",
    "            'Non_HTML_Tags_by_Row': non_html_by_row_df,\n",
    "            'Non_HTML_Tags_Summary': non_html_summary_df\n",
    "        }\n",
    "\n",
    "    def encode_brackets(self, df: pd.DataFrame, tag_to_encode: str,\n",
    "                       left_replacement: str = '&lt;', right_replacement: str = '&gt;',\n",
    "                       scope: str = 'global', specific_column: str = None,\n",
    "                       specific_row: int = None) -> pd.DataFrame:\n",
    "        \"\"\"Encode < > brackets in non-HTML tags\"\"\"\n",
    "        df_encoded = df.copy()\n",
    "\n",
    "        # Create the encoded replacement\n",
    "        encoded_tag = tag_to_encode.replace('<', left_replacement).replace('>', right_replacement)\n",
    "        \n",
    "        if scope == 'global':\n",
    "            encodings_made = 0\n",
    "            for col in df_encoded.columns:\n",
    "                # Count before\n",
    "                before_count = df_encoded[col].astype(str).str.contains(\n",
    "                    re.escape(tag_to_encode), regex=True\n",
    "                ).sum()\n",
    "                \n",
    "                # Make replacement\n",
    "                df_encoded[col] = df_encoded[col].astype(str).str.replace(\n",
    "                    tag_to_encode, encoded_tag, regex=False\n",
    "                )\n",
    "                \n",
    "                # Count after\n",
    "                after_count = df_encoded[col].astype(str).str.contains(\n",
    "                    re.escape(tag_to_encode), regex=True\n",
    "                ).sum()\n",
    "                \n",
    "                column_encodings = before_count - after_count\n",
    "                if column_encodings > 0:\n",
    "                    print(f\"  Column '{col}': {column_encodings} encodings\")\n",
    "                encodings_made += column_encodings\n",
    "            \n",
    "            print(f\"Total encodings made: {encodings_made}\")\n",
    "            \n",
    "        elif scope == 'column' and specific_column:\n",
    "            if specific_column in df_encoded.columns:\n",
    "                df_encoded[specific_column] = df_encoded[specific_column].astype(str).str.replace(\n",
    "                    tag_to_encode, encoded_tag, regex=False\n",
    "                )\n",
    "        elif scope == 'row' and specific_row is not None:\n",
    "            for col in df_encoded.columns:\n",
    "                if pd.notna(df_encoded.loc[specific_row, col]):\n",
    "                    cell_value = str(df_encoded.loc[specific_row, col])\n",
    "                    df_encoded.loc[specific_row, col] = cell_value.replace(tag_to_encode, encoded_tag)\n",
    "        elif scope == 'cell' and specific_column and specific_row is not None:\n",
    "            if specific_column in df_encoded.columns and pd.notna(df_encoded.loc[specific_row, specific_column]):\n",
    "                cell_value = str(df_encoded.loc[specific_row, specific_column])\n",
    "                df_encoded.loc[specific_row, specific_column] = cell_value.replace(tag_to_encode, encoded_tag)\n",
    "\n",
    "        # Handle NaN and empty string conversion\n",
    "        for col in df_encoded.columns:\n",
    "            mask_nan = df_encoded[col] == 'nan'\n",
    "            mask_empty = df_encoded[col] == ''\n",
    "            df_encoded.loc[mask_nan, col] = np.nan\n",
    "            df_encoded.loc[mask_empty, col] = ''\n",
    "\n",
    "        return df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "426d9c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 5: Analyzing non-HTML tags...\n",
      "Scanning for non-HTML content in angle brackets...\n",
      "Found 49 non-HTML tags for encoding\n",
      "Non-HTML Tags Summary:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Non_HTML_Tag</th>\n",
       "      <th>Count</th>\n",
       "      <th>Locations</th>\n",
       "      <th>Sample_Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;Concanauhtli&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>Row 2158, Col Attestations from sources in Eng...</td>\n",
       "      <td>nauhtli (Zoquicanauhtli) is the same as the go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>&lt;TLACOOCELUTL: [...] yoan qujtocaiotia, tlacom...</td>\n",
       "      <td>3</td>\n",
       "      <td>Row 5139, Col Attestations from sources in Eng...</td>\n",
       "      <td>tl*, as the following passage (FC 11: 3) impli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>&lt;ue&gt;</td>\n",
       "      <td>3</td>\n",
       "      <td>Row 10787, Col Alonso de Molina; Row 16924, Co...</td>\n",
       "      <td>ret. onitemiquiztlapopolhui.) perdonar la muer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>&lt;Canauhtli&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>Row 26094, Col Attestations from sources in En...</td>\n",
       "      <td>dum in Paragraph Three [FC: 57] elaborates: “D...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>&lt;Ā-COYO-TL&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>Row 31289, Col Attestations from sources in En...</td>\n",
       "      <td>omeone beat the two-toned drum.” However, acoy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;n&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>Row 1408, Col Alonso de Molina; Row 12954, Col...</td>\n",
       "      <td>&lt;p&gt;aami. n. (pret. onaan.) mo[[[&lt;n&gt;]]]tear o c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>&lt;Ā-TŌTO-LIN&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>Row 31289, Col Attestations from sources in En...</td>\n",
       "      <td>Ā-COYO-TL&gt; is the Neotropic Cormorant and atot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>&lt;ZŌL-IN&gt;</td>\n",
       "      <td>2</td>\n",
       "      <td>Row 31335, Col Attestations from sources in En...</td>\n",
       "      <td>49 Ooaton] “It is the same as the Montezuma Qu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;sip&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 1137, Col Attestations from sources in Eng...</td>\n",
       "      <td>07), 157.&lt;/bibl&gt; Escrivano de cabil tlsn— fabi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;susp&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 1137, Col Attestations from sources in Eng...</td>\n",
       "      <td>erican Center, 1976), Doc. 8.&lt;/bibl&gt; diego jua...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>&lt;/wup&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 3616, Col Attestations from sources in Eng...</td>\n",
       "      <td>l&gt;&lt;/p&gt; &lt;p&gt;ynic omochiuh missas 10 &lt;strong&gt;p&lt;wu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>&lt;wup&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 3616, Col Attestations from sources in Eng...</td>\n",
       "      <td>6.&lt;/bibl&gt;&lt;/p&gt; &lt;p&gt;ynic omochiuh missas 10 &lt;stro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;tzanatl&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 5257, Col Attestations from sources in Eng...</td>\n",
       "      <td>ahuac. It is like all the slender-billed grack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>&lt;tlacoocelutl: yoan=\"\" qujtocaiotia=\"\" tlacomj...</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 3731, Col Attestations from sources in Eng...</td>\n",
       "      <td>tl*, as the following passage (FC 11: 3) impli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>&lt;Zoquicanauhtli, literally, “mud duck&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 6766, Col Attestations from sources in Eng...</td>\n",
       "      <td>. Note also this elaboration: “The Çoquicanauh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>&lt;CANAUH-TLI&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 5621, Col Attestations from sources in Eng...</td>\n",
       "      <td>ticorax nycticorax) [FC: 39 Oactli] “It is a d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>&lt;duck&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 6766, Col Attestations from sources in Eng...</td>\n",
       "      <td>-TL &lt;Black-bellied Whistling-Duck&gt; and CANAUH-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>&lt;Black-bellied Whistling-Duck&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 6766, Col Attestations from sources in Eng...</td>\n",
       "      <td>nae). See ZOQUI-CANAUH-TLI; see also TLALALACA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>&lt;tlavitequjnj&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 22572, Col Attestations from sources in En...</td>\n",
       "      <td>otli. It is ashen. It is a hunter, a bird of p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>&lt;COZCA-CUAUH-TLI, likely the Crested Caracara&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 5621, Col Attestations from sources in Eng...</td>\n",
       "      <td>res cachinnans) [FC: 42 Oactli] “It resembles ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>&lt;/tlacoocelutl:&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 3731, Col Attestations from sources in Eng...</td>\n",
       "      <td>arch and the University of Utah, 1951), 49.&lt;/b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>&lt;atole&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 5054, Col Frances Karttunen</td>\n",
       "      <td>e, a drink made from cornstarch / papilla de m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>&lt;alo- poss- ichcatl -yoa:-l2 tla7&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 29831, Col Principal English Translation</td>\n",
       "      <td>l group, May 11, 2015.  His analysis of the te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>&lt;toznene&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 29151, Col Attestations from sources in En...</td>\n",
       "      <td>23 Toztli]: “When the young yellow-headed par...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>&lt;tzoniaiauhquj, which see&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 26094, Col Attestations from sources in En...</td>\n",
       "      <td>is the collective name for the white-breast, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>&lt;zoquiazolin, literally, “mud quail”&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 31296, Col Attestations from sources in En...</td>\n",
       "      <td>a) [FC: 28 Açolin] “Also it is called çoquiaço...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>&lt;Amazon&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 31304, Col Attestations from sources in En...</td>\n",
       "      <td>: 23 Cocho] “It resembles the young yellow-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>&lt;Slender-billed Grackle&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 31306, Col Attestations from sources in En...</td>\n",
       "      <td>ototl] “It is the same as the &lt;em&gt;acatzanatl&lt;/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>&lt;“paloma”&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 31313, Col Attestations from sources in En...</td>\n",
       "      <td>r] birds come. It is of average size, like a d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>&lt;totolin&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 31320, Col Attestations from sources in En...</td>\n",
       "      <td>Quauhtotoli] “It is like the domesticated tur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>&lt;CHĪCUA-TLI&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 31339, Col Attestations from sources in En...</td>\n",
       "      <td>nae)” [FC: 46 Poxaquatl] “It is like the barn ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>&lt;Tenitztli&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 31348, Col Attestations from sources in En...</td>\n",
       "      <td>e] bills are one over the other…. The food of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>&lt;tzoniaiauhqui, Lesser Scaup&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 31354, Col Attestations from sources in En...</td>\n",
       "      <td>is the collective name for the white-breast, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>&lt;shorebirds&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 31354, Col Attestations from sources in En...</td>\n",
       "      <td>the &lt;tzoniaiauhqui, Lesser Scaup&gt;, the teçoloc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>&lt;XIUH-TŌTŌ-TL&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 31373, Col Attestations from sources in En...</td>\n",
       "      <td>is entirely, completely light blue like a coti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>&lt;ĀCAL-LI, \"boat\"&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 31375, Col Attestations from sources in En...</td>\n",
       "      <td>ion of \"tenalcaltic\"]; rather, \"boat-shaped bi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>&lt;boat-shaped&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 31375, Col Attestations from sources in En...</td>\n",
       "      <td>k mingled with white. The Bill widens; it beco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>&lt;tlhotli&gt;</td>\n",
       "      <td>1</td>\n",
       "      <td>Row 31382, Col Attestations from sources in En...</td>\n",
       "      <td>C: 45  Iooaltlhotli]: “It is the same as a fal...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Non_HTML_Tag  Count  \\\n",
       "3                                      <Concanauhtli>      3   \n",
       "9   <TLACOOCELUTL: [...] yoan qujtocaiotia, tlacom...      3   \n",
       "16                                               <ue>      3   \n",
       "18                                        <Canauhtli>      2   \n",
       "22                                        <Ā-COYO-TL>      2   \n",
       "2                                                 <n>      2   \n",
       "23                                       <Ā-TŌTO-LIN>      2   \n",
       "29                                           <ZŌL-IN>      2   \n",
       "0                                               <sip>      1   \n",
       "1                                              <susp>      1   \n",
       "5                                              </wup>      1   \n",
       "4                                               <wup>      1   \n",
       "10                                          <tzanatl>      1   \n",
       "6   <tlacoocelutl: yoan=\"\" qujtocaiotia=\"\" tlacomj...      1   \n",
       "13             <Zoquicanauhtli, literally, “mud duck>      1   \n",
       "12                                       <CANAUH-TLI>      1   \n",
       "15                                             <duck>      1   \n",
       "14                     <Black-bellied Whistling-Duck>      1   \n",
       "17                                     <tlavitequjnj>      1   \n",
       "11     <COZCA-CUAUH-TLI, likely the Crested Caracara>      1   \n",
       "7                                    </tlacoocelutl:>      1   \n",
       "8                                             <atole>      1   \n",
       "21                 <alo- poss- ichcatl -yoa:-l2 tla7>      1   \n",
       "20                                          <toznene>      1   \n",
       "19                         <tzoniaiauhquj, which see>      1   \n",
       "24              <zoquiazolin, literally, “mud quail”>      1   \n",
       "25                                           <Amazon>      1   \n",
       "26                           <Slender-billed Grackle>      1   \n",
       "27                                         <“paloma”>      1   \n",
       "28                                          <totolin>      1   \n",
       "30                                       <CHĪCUA-TLI>      1   \n",
       "31                                        <Tenitztli>      1   \n",
       "32                      <tzoniaiauhqui, Lesser Scaup>      1   \n",
       "33                                       <shorebirds>      1   \n",
       "34                                     <XIUH-TŌTŌ-TL>      1   \n",
       "35                                  <ĀCAL-LI, \"boat\">      1   \n",
       "36                                      <boat-shaped>      1   \n",
       "37                                          <tlhotli>      1   \n",
       "\n",
       "                                            Locations  \\\n",
       "3   Row 2158, Col Attestations from sources in Eng...   \n",
       "9   Row 5139, Col Attestations from sources in Eng...   \n",
       "16  Row 10787, Col Alonso de Molina; Row 16924, Co...   \n",
       "18  Row 26094, Col Attestations from sources in En...   \n",
       "22  Row 31289, Col Attestations from sources in En...   \n",
       "2   Row 1408, Col Alonso de Molina; Row 12954, Col...   \n",
       "23  Row 31289, Col Attestations from sources in En...   \n",
       "29  Row 31335, Col Attestations from sources in En...   \n",
       "0   Row 1137, Col Attestations from sources in Eng...   \n",
       "1   Row 1137, Col Attestations from sources in Eng...   \n",
       "5   Row 3616, Col Attestations from sources in Eng...   \n",
       "4   Row 3616, Col Attestations from sources in Eng...   \n",
       "10  Row 5257, Col Attestations from sources in Eng...   \n",
       "6   Row 3731, Col Attestations from sources in Eng...   \n",
       "13  Row 6766, Col Attestations from sources in Eng...   \n",
       "12  Row 5621, Col Attestations from sources in Eng...   \n",
       "15  Row 6766, Col Attestations from sources in Eng...   \n",
       "14  Row 6766, Col Attestations from sources in Eng...   \n",
       "17  Row 22572, Col Attestations from sources in En...   \n",
       "11  Row 5621, Col Attestations from sources in Eng...   \n",
       "7   Row 3731, Col Attestations from sources in Eng...   \n",
       "8                     Row 5054, Col Frances Karttunen   \n",
       "21       Row 29831, Col Principal English Translation   \n",
       "20  Row 29151, Col Attestations from sources in En...   \n",
       "19  Row 26094, Col Attestations from sources in En...   \n",
       "24  Row 31296, Col Attestations from sources in En...   \n",
       "25  Row 31304, Col Attestations from sources in En...   \n",
       "26  Row 31306, Col Attestations from sources in En...   \n",
       "27  Row 31313, Col Attestations from sources in En...   \n",
       "28  Row 31320, Col Attestations from sources in En...   \n",
       "30  Row 31339, Col Attestations from sources in En...   \n",
       "31  Row 31348, Col Attestations from sources in En...   \n",
       "32  Row 31354, Col Attestations from sources in En...   \n",
       "33  Row 31354, Col Attestations from sources in En...   \n",
       "34  Row 31373, Col Attestations from sources in En...   \n",
       "35  Row 31375, Col Attestations from sources in En...   \n",
       "36  Row 31375, Col Attestations from sources in En...   \n",
       "37  Row 31382, Col Attestations from sources in En...   \n",
       "\n",
       "                                       Sample_Context  \n",
       "3   nauhtli (Zoquicanauhtli) is the same as the go...  \n",
       "9   tl*, as the following passage (FC 11: 3) impli...  \n",
       "16  ret. onitemiquiztlapopolhui.) perdonar la muer...  \n",
       "18  dum in Paragraph Three [FC: 57] elaborates: “D...  \n",
       "22  omeone beat the two-toned drum.” However, acoy...  \n",
       "2   <p>aami. n. (pret. onaan.) mo[[[<n>]]]tear o c...  \n",
       "23  Ā-COYO-TL> is the Neotropic Cormorant and atot...  \n",
       "29  49 Ooaton] “It is the same as the Montezuma Qu...  \n",
       "0   07), 157.</bibl> Escrivano de cabil tlsn— fabi...  \n",
       "1   erican Center, 1976), Doc. 8.</bibl> diego jua...  \n",
       "5   l></p> <p>ynic omochiuh missas 10 <strong>p<wu...  \n",
       "4   6.</bibl></p> <p>ynic omochiuh missas 10 <stro...  \n",
       "10  ahuac. It is like all the slender-billed grack...  \n",
       "6   tl*, as the following passage (FC 11: 3) impli...  \n",
       "13  . Note also this elaboration: “The Çoquicanauh...  \n",
       "12  ticorax nycticorax) [FC: 39 Oactli] “It is a d...  \n",
       "15  -TL <Black-bellied Whistling-Duck> and CANAUH-...  \n",
       "14  nae). See ZOQUI-CANAUH-TLI; see also TLALALACA...  \n",
       "17  otli. It is ashen. It is a hunter, a bird of p...  \n",
       "11  res cachinnans) [FC: 42 Oactli] “It resembles ...  \n",
       "7   arch and the University of Utah, 1951), 49.</b...  \n",
       "8   e, a drink made from cornstarch / papilla de m...  \n",
       "21  l group, May 11, 2015.  His analysis of the te...  \n",
       "20   23 Toztli]: “When the young yellow-headed par...  \n",
       "19   is the collective name for the white-breast, ...  \n",
       "24  a) [FC: 28 Açolin] “Also it is called çoquiaço...  \n",
       "25  : 23 Cocho] “It resembles the young yellow-hea...  \n",
       "26  ototl] “It is the same as the <em>acatzanatl</...  \n",
       "27  r] birds come. It is of average size, like a d...  \n",
       "28   Quauhtotoli] “It is like the domesticated tur...  \n",
       "30  nae)” [FC: 46 Poxaquatl] “It is like the barn ...  \n",
       "31  e] bills are one over the other…. The food of ...  \n",
       "32   is the collective name for the white-breast, ...  \n",
       "33  the <tzoniaiauhqui, Lesser Scaup>, the teçoloc...  \n",
       "34  is entirely, completely light blue like a coti...  \n",
       "35  ion of \"tenalcaltic\"]; rather, \"boat-shaped bi...  \n",
       "36  k mingled with white. The Bill widens; it beco...  \n",
       "37  C: 45  Iooaltlhotli]: “It is the same as a fal...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying bracket encoding...\n",
      "Encoding brackets in '<Concanauhtli>' → '&lt;Concanauhtli&gt;'\n",
      "  Column 'Attestations from sources in English': 3 encodings\n",
      "Total encodings made: 3\n",
      "Encoding brackets in '<TLACOOCELUTL: [...] yoan qujtocaiotia, tlacomjztli, tepiton, pachtontli, melacpil: vel ixqujch, in castillan mjzton, nexeoac, ticeoac, cujcujltic, ocelocujcujltic, molchachapatz>' → '&lt;TLACOOCELUTL: [...] yoan qujtocaiotia, tlacomjztli, tepiton, pachtontli, melacpil: vel ixqujch, in castillan mjzton, nexeoac, ticeoac, cujcujltic, ocelocujcujltic, molchachapatz&gt;'\n",
      "  Column 'Attestations from sources in English': 3 encodings\n",
      "Total encodings made: 3\n",
      "Encoding brackets in '<ue>' → '&lt;ue&gt;'\n",
      "  Column 'Alonso de Molina': 3 encodings\n",
      "Total encodings made: 3\n",
      "Encoding brackets in '<Canauhtli>' → '&lt;Canauhtli&gt;'\n",
      "  Column 'Attestations from sources in English': 2 encodings\n",
      "Total encodings made: 2\n",
      "Encoding brackets in '<Ā-COYO-TL>' → '&lt;Ā-COYO-TL&gt;'\n",
      "  Column 'Attestations from sources in English': 2 encodings\n",
      "Total encodings made: 2\n",
      "Encoding brackets in '<n>' → '&lt;n&gt;'\n",
      "  Column 'Alonso de Molina': 2 encodings\n",
      "Total encodings made: 2\n",
      "Encoding brackets in '<Ā-TŌTO-LIN>' → '&lt;Ā-TŌTO-LIN&gt;'\n",
      "  Column 'Attestations from sources in English': 2 encodings\n",
      "Total encodings made: 2\n",
      "Encoding brackets in '<ZŌL-IN>' → '&lt;ZŌL-IN&gt;'\n",
      "  Column 'Attestations from sources in English': 2 encodings\n",
      "Total encodings made: 2\n",
      "Encoding brackets in '<sip>' → '&lt;sip&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<susp>' → '&lt;susp&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '</wup>' → '&lt;/wup&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<wup>' → '&lt;wup&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<tzanatl>' → '&lt;tzanatl&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<tlacoocelutl: yoan=\"\" qujtocaiotia=\"\" tlacomjztli=\"\" tepiton=\"\" pachtontli=\"\" melacpil:=\"\" vel=\"\" ixqujch=\"\" in=\"\" castillan=\"\" mjzton=\"\" nexeoac=\"\" ticeoac=\"\" cujcujltic=\"\" ocelocujcujltic=\"\" molchachapatz=\"\">' → '&lt;tlacoocelutl: yoan=\"\" qujtocaiotia=\"\" tlacomjztli=\"\" tepiton=\"\" pachtontli=\"\" melacpil:=\"\" vel=\"\" ixqujch=\"\" in=\"\" castillan=\"\" mjzton=\"\" nexeoac=\"\" ticeoac=\"\" cujcujltic=\"\" ocelocujcujltic=\"\" molchachapatz=\"\"&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<Zoquicanauhtli, literally, “mud duck>' → '&lt;Zoquicanauhtli, literally, “mud duck&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<CANAUH-TLI>' → '&lt;CANAUH-TLI&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<duck>' → '&lt;duck&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<Black-bellied Whistling-Duck>' → '&lt;Black-bellied Whistling-Duck&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<tlavitequjnj>' → '&lt;tlavitequjnj&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<COZCA-CUAUH-TLI, likely the Crested Caracara>' → '&lt;COZCA-CUAUH-TLI, likely the Crested Caracara&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '</tlacoocelutl:>' → '&lt;/tlacoocelutl:&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<atole>' → '&lt;atole&gt;'\n",
      "  Column 'Frances Karttunen': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<alo- poss- ichcatl -yoa:-l2 tla7>' → '&lt;alo- poss- ichcatl -yoa:-l2 tla7&gt;'\n",
      "  Column 'Principal English Translation': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<toznene>' → '&lt;toznene&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<tzoniaiauhquj, which see>' → '&lt;tzoniaiauhquj, which see&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<zoquiazolin, literally, “mud quail”>' → '&lt;zoquiazolin, literally, “mud quail”&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<Amazon>' → '&lt;Amazon&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<Slender-billed Grackle>' → '&lt;Slender-billed Grackle&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<“paloma”>' → '&lt;“paloma”&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<totolin>' → '&lt;totolin&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<CHĪCUA-TLI>' → '&lt;CHĪCUA-TLI&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<Tenitztli>' → '&lt;Tenitztli&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<tzoniaiauhqui, Lesser Scaup>' → '&lt;tzoniaiauhqui, Lesser Scaup&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<shorebirds>' → '&lt;shorebirds&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<XIUH-TŌTŌ-TL>' → '&lt;XIUH-TŌTŌ-TL&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<ĀCAL-LI, \"boat\">' → '&lt;ĀCAL-LI, \"boat\"&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<boat-shaped>' → '&lt;boat-shaped&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "Encoding brackets in '<tlhotli>' → '&lt;tlhotli&gt;'\n",
      "  Column 'Attestations from sources in English': 1 encodings\n",
      "Total encodings made: 1\n",
      "✓ Encoded 38 non-HTML tags\n",
      "Saved to: working_files\\05_non_html_tag_analysis.xlsx\n",
      "Saved to SQLite table: 05_non_html_encoding_stage\n",
      "Step 5 complete: Non-HTML tag detection and encoding\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Non-HTML Tag Detection and Encoding\n",
    "print(\"Step 5: Analyzing non-HTML tags...\")\n",
    "\n",
    "non_html_processor = NonHTMLTagProcessor()\n",
    "non_html_results = non_html_processor.analyze_non_html_tags(df)\n",
    "\n",
    "print(\"Non-HTML Tags Summary:\")\n",
    "if not non_html_results['Non_HTML_Tags_Summary'].empty:\n",
    "    display(non_html_results['Non_HTML_Tags_Summary'])\n",
    "    \n",
    "    # Save to SQLite\n",
    "    non_html_results['Non_HTML_Tags_Summary'].to_sql(\n",
    "        'non_html_tags_summary', conn, if_exists='replace', index=False\n",
    "    )\n",
    "    non_html_results['Non_HTML_Tags_by_Row'].to_sql(\n",
    "        'non_html_tags_by_row', conn, if_exists='replace', index=False\n",
    "    )\n",
    "    \n",
    "    # Apply encoding automatically\n",
    "    print(\"\\nApplying bracket encoding...\")\n",
    "    for _, row in non_html_results['Non_HTML_Tags_Summary'].iterrows():\n",
    "        non_html_tag = row['Non_HTML_Tag']\n",
    "        print(f\"Encoding brackets in '{non_html_tag}' → '&lt;{non_html_tag[1:-1]}&gt;'\")\n",
    "        df = non_html_processor.encode_brackets(\n",
    "            df, non_html_tag, '&lt;', '&gt;', scope='global'\n",
    "        )\n",
    "    \n",
    "    print(f\"✓ Encoded {len(non_html_results['Non_HTML_Tags_Summary'])} non-HTML tags\")\n",
    "    \n",
    "else:\n",
    "    print(\"No non-HTML tags found!\")\n",
    "\n",
    "# Save to Excel and SQLite\n",
    "save_to_excel(non_html_results, \"05_non_html_tag_analysis.xlsx\")\n",
    "save_intermediate_stage_sqlite(df, \"05_non_html_encoding\", conn)\n",
    "\n",
    "print(\"Step 5 complete: Non-HTML tag detection and encoding\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a69fe1",
   "metadata": {},
   "source": [
    "## Step 6: Citation Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68c325e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CitationExtractor:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def extract_bibl_content(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract from clean bibl tags (post Steps 1-5)\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return []\n",
    "\n",
    "        text_str = str(text)\n",
    "        \n",
    "        # Simple pattern for well-formed bibl tags\n",
    "        pattern = r'<bibl[^>]*>(.*?)</bibl>'\n",
    "        matches = re.findall(pattern, text_str, re.DOTALL | re.IGNORECASE)\n",
    "        \n",
    "        cleaned_citations = []\n",
    "        for match in matches:\n",
    "            # Clean up the content\n",
    "            cleaned = re.sub(r'<[^>]+>', '', match)  # Remove nested HTML\n",
    "            cleaned = ' '.join(cleaned.strip().split())  # Normalize whitespace\n",
    "            \n",
    "            if cleaned and len(cleaned.strip()) > 5:  # Must be substantial\n",
    "                cleaned_citations.append(cleaned.strip())\n",
    "        \n",
    "        return cleaned_citations\n",
    "\n",
    "    def add_citation_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add citation columns to cleaned dataframe\"\"\"\n",
    "        print(\"Adding citation columns...\")\n",
    "        df_with_citations = df.copy()\n",
    "\n",
    "        # Initialize new columns\n",
    "        df_with_citations['Citations'] = ''\n",
    "        df_with_citations['Number_of_Citations'] = 0\n",
    "\n",
    "        for idx, row in df_with_citations.iterrows():\n",
    "            if idx % 5000 == 0:\n",
    "                print(f\"  Processing row {idx}...\")\n",
    "                \n",
    "            all_citations = []\n",
    "\n",
    "            # Extract from all original columns\n",
    "            for col in df.columns:  # Original columns only\n",
    "                cell_value = row[col]\n",
    "                if pd.notna(cell_value):\n",
    "                    citations = self.extract_bibl_content(cell_value)\n",
    "                    all_citations.extend(citations)\n",
    "\n",
    "            # Set new column values\n",
    "            if all_citations:\n",
    "                df_with_citations.loc[idx, 'Citations'] = ' | '.join(all_citations)\n",
    "                df_with_citations.loc[idx, 'Number_of_Citations'] = len(all_citations)\n",
    "\n",
    "        return df_with_citations\n",
    "\n",
    "def process_citations_step6(conn: sqlite3.Connection) -> pd.DataFrame:\n",
    "    \"\"\"Step 6: Citation extraction using cleaned data from Step 5\"\"\"\n",
    "    print(\"Step 6: Citation Extraction\")\n",
    "    \n",
    "    # Load the CLEANED data from Step 5\n",
    "    print(\"  - Loading cleaned data from Step 5...\")\n",
    "    df_clean = pd.read_sql(\"SELECT * FROM [05_non_html_encoding_stage]\", conn)\n",
    "    print(f\"  - Loaded {len(df_clean)} rows\")\n",
    "    \n",
    "    # Extract citations from clean data\n",
    "    citation_extractor = CitationExtractor()\n",
    "    df_with_citations = citation_extractor.add_citation_columns(df_clean)\n",
    "    \n",
    "    # Save processed stage\n",
    "    save_intermediate_stage_sqlite(df_with_citations, \"06_citation_extraction_stage\", conn)\n",
    "    \n",
    "    # Quick stats\n",
    "    rows_with_citations = len(df_with_citations[df_with_citations['Number_of_Citations'] > 0])\n",
    "    total_citations = df_with_citations['Number_of_Citations'].sum()\n",
    "    \n",
    "    print(f\"  - Rows with citations: {rows_with_citations}\")\n",
    "    print(f\"  - Total citations extracted: {total_citations}\")\n",
    "    \n",
    "    return df_with_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3b946b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 6: Citation Extraction\n",
      "  - Loading cleaned data from Step 5...\n",
      "  - Loaded 31806 rows\n",
      "Adding citation columns...\n",
      "  Processing row 0...\n",
      "  Processing row 5000...\n",
      "  Processing row 10000...\n",
      "  Processing row 15000...\n",
      "  Processing row 20000...\n",
      "  Processing row 25000...\n",
      "  Processing row 30000...\n",
      "Saved to SQLite table: 06_citation_extraction_stage_stage\n",
      "  - Rows with citations: 31281\n",
      "  - Total citations extracted: 53497\n"
     ]
    }
   ],
   "source": [
    "df_with_citations = process_citations_step6(conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be34751",
   "metadata": {},
   "source": [
    "## Step 7: Cross-Reference Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8db928f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossReferenceExtractor:\n",
    "    def __init__(self):\n",
    "        # Common cross-reference patterns in academic/dictionary entries\n",
    "        self.crossref_patterns = [\n",
    "            r'\\(see\\s+([^)]+)\\)',           # (see something)\n",
    "            r'\\(cf\\.\\s+([^)]+)\\)',          # (cf. something)  \n",
    "            r'\\(compare\\s+([^)]+)\\)',       # (compare something)\n",
    "            r'\\(also\\s+([^)]+)\\)',          # (also something)\n",
    "            r'\\(variant\\s+of\\s+([^)]+)\\)',  # (variant of something)\n",
    "            r'\\(see\\s+also\\s+([^)]+)\\)'     # (see also something)\n",
    "        ]\n",
    "\n",
    "    def extract_cross_references(self, text: str) -> List[str]:\n",
    "        \"\"\"Extract cross-references from text\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return []\n",
    "\n",
    "        text_str = str(text)\n",
    "        crossrefs = []\n",
    "\n",
    "        # Extract using all patterns\n",
    "        for pattern in self.crossref_patterns:\n",
    "            matches = re.findall(pattern, text_str, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                cleaned = self._clean_crossref_content(match)\n",
    "                if cleaned and cleaned not in crossrefs:  # Avoid duplicates\n",
    "                    crossrefs.append(cleaned)\n",
    "\n",
    "        return crossrefs\n",
    "\n",
    "    def _clean_crossref_content(self, content: str) -> str:\n",
    "        \"\"\"Clean up extracted cross-reference content\"\"\"\n",
    "        if not content:\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove any HTML tags\n",
    "        cleaned = re.sub(r'<[^>]+>', '', content)\n",
    "        \n",
    "        # Clean up whitespace\n",
    "        cleaned = ' '.join(cleaned.strip().split())\n",
    "        \n",
    "        # Remove trailing punctuation\n",
    "        cleaned = cleaned.strip('.,;:')\n",
    "        \n",
    "        # Must be substantial and start with a letter/number\n",
    "        if len(cleaned) > 1 and cleaned[0].isalnum():\n",
    "            return cleaned.strip()\n",
    "        \n",
    "        return \"\"\n",
    "\n",
    "    def _classify_crossref(self, crossref: str) -> str:\n",
    "        \"\"\"Classify the type of cross-reference\"\"\"\n",
    "        crossref_lower = crossref.lower()\n",
    "        \n",
    "        if any(word in crossref_lower for word in ['molina', 'karttunen', 'carochi']):\n",
    "            return 'Scholar_Reference'\n",
    "        elif len(crossref.split()) == 1:\n",
    "            return 'Single_Word'\n",
    "        elif len(crossref.split()) <= 3:\n",
    "            return 'Short_Phrase'\n",
    "        else:\n",
    "            return 'Long_Phrase'\n",
    "\n",
    "    def analyze_crossrefs_in_dataframe(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Analyze all cross-references in the dataframe\"\"\"\n",
    "        crossref_details = []\n",
    "        row_summaries = []\n",
    "        \n",
    "        print(\"Analyzing cross-references across dataframe...\")\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            if idx % 5000 == 0:\n",
    "                print(f\"  Processing row {idx}...\")\n",
    "            \n",
    "            row_crossrefs = []\n",
    "            \n",
    "            # Check each column for cross-references\n",
    "            for col in df.columns:\n",
    "                # Skip the columns we just added in Step 6\n",
    "                if col in ['Citations', 'Number_of_Citations']:\n",
    "                    continue\n",
    "                    \n",
    "                cell_value = row[col]\n",
    "                if pd.notna(cell_value) and str(cell_value).strip():\n",
    "                    crossrefs = self.extract_cross_references(cell_value)\n",
    "                    \n",
    "                    for crossref in crossrefs:\n",
    "                        crossref_details.append({\n",
    "                            'Row_Index': idx,\n",
    "                            'Row_ID': row.get('Ref', f'Row_{idx}'),\n",
    "                            'Column': col,\n",
    "                            'Cross_Reference': crossref,\n",
    "                            'Crossref_Length': len(crossref),\n",
    "                            'Crossref_Type': self._classify_crossref(crossref)\n",
    "                        })\n",
    "                        row_crossrefs.append(crossref)\n",
    "            \n",
    "            # Summary for this row\n",
    "            if row_crossrefs:\n",
    "                row_summaries.append({\n",
    "                    'Row_Index': idx,\n",
    "                    'Row_ID': row.get('Ref', f'Row_{idx}'),\n",
    "                    'Headword': row.get('Headword', ''),\n",
    "                    'Total_Cross_References': len(row_crossrefs),\n",
    "                    'Cross_References_Preview': ' | '.join(row_crossrefs[:3]) + ('...' if len(row_crossrefs) > 3 else '')\n",
    "                })\n",
    "        \n",
    "        # Overall statistics\n",
    "        total_crossrefs = len(crossref_details)\n",
    "        rows_with_crossrefs = len(row_summaries)\n",
    "        \n",
    "        # Count by type\n",
    "        type_counts = {}\n",
    "        for detail in crossref_details:\n",
    "            crossref_type = detail['Crossref_Type']\n",
    "            type_counts[crossref_type] = type_counts.get(crossref_type, 0) + 1\n",
    "        \n",
    "        overall_stats = pd.DataFrame([{\n",
    "            'Total_Rows_Processed': len(df),\n",
    "            'Rows_With_Cross_References': rows_with_crossrefs,\n",
    "            'Total_Cross_References_Found': total_crossrefs,\n",
    "            'Single_Word_Refs': type_counts.get('Single_Word', 0),\n",
    "            'Short_Phrase_Refs': type_counts.get('Short_Phrase', 0),\n",
    "            'Scholar_Refs': type_counts.get('Scholar_Reference', 0),\n",
    "            'Avg_Crossrefs_Per_Row': total_crossrefs / rows_with_crossrefs if rows_with_crossrefs > 0 else 0,\n",
    "            'Max_Crossrefs_Per_Row': max([r['Total_Cross_References'] for r in row_summaries]) if row_summaries else 0\n",
    "        }])\n",
    "        \n",
    "        return {\n",
    "            'CrossRef_Details': pd.DataFrame(crossref_details),\n",
    "            'Row_Summaries': pd.DataFrame(row_summaries),\n",
    "            'Overall_Stats': overall_stats\n",
    "        }\n",
    "\n",
    "    def add_crossref_columns(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Add cross-reference columns WITHOUT removing original content\"\"\"\n",
    "        print(\"Adding cross-reference columns...\")\n",
    "        df_with_crossrefs = df.copy()\n",
    "\n",
    "        # Initialize new columns\n",
    "        df_with_crossrefs['Cross_References'] = ''\n",
    "        df_with_crossrefs['Number_of_Cross_References'] = 0\n",
    "        df_with_crossrefs['CrossRef_Types'] = ''\n",
    "\n",
    "        for idx, row in df_with_crossrefs.iterrows():\n",
    "            if idx % 5000 == 0:\n",
    "                print(f\"  Processing row {idx}...\")\n",
    "                \n",
    "            all_crossrefs = []\n",
    "            crossref_types = []\n",
    "\n",
    "            # Extract from original columns (skip our new citation columns)\n",
    "            for col in df.columns:\n",
    "                if col in ['Citations', 'Number_of_Citations']:\n",
    "                    continue\n",
    "                    \n",
    "                cell_value = row[col]\n",
    "                if pd.notna(cell_value) and str(cell_value).strip():\n",
    "                    crossrefs = self.extract_cross_references(cell_value)\n",
    "                    all_crossrefs.extend(crossrefs)\n",
    "                    \n",
    "                    # Track cross-reference types\n",
    "                    for crossref in crossrefs:\n",
    "                        crossref_types.append(self._classify_crossref(crossref))\n",
    "\n",
    "            # Set new column values\n",
    "            if all_crossrefs:\n",
    "                df_with_crossrefs.loc[idx, 'Cross_References'] = ' | '.join(all_crossrefs)\n",
    "                df_with_crossrefs.loc[idx, 'Number_of_Cross_References'] = len(all_crossrefs)\n",
    "                df_with_crossrefs.loc[idx, 'CrossRef_Types'] = ', '.join(set(crossref_types))\n",
    "\n",
    "        return df_with_crossrefs\n",
    "\n",
    "def process_crossrefs_step7(conn: sqlite3.Connection) -> pd.DataFrame:\n",
    "    \"\"\"Step 7: Cross-reference extraction using data from Step 6\"\"\"\n",
    "    print(\"Step 7: Cross-Reference Extraction\")\n",
    "    \n",
    "    # Load data from Step 6 (includes citations)\n",
    "    print(\"  - Loading data from Step 6...\")\n",
    "    df_with_citations = pd.read_sql(\"SELECT * FROM [06_citation_extraction_stage_stage]\", conn)\n",
    "    print(f\"  - Loaded {len(df_with_citations)} rows\")\n",
    "    \n",
    "    # Extract cross-references\n",
    "    crossref_extractor = CrossReferenceExtractor()\n",
    "    \n",
    "    # Optional: Analyze first for insights\n",
    "    print(\"  - Analyzing cross-references...\")\n",
    "    crossref_analysis = crossref_extractor.analyze_crossrefs_in_dataframe(df_with_citations)\n",
    "    \n",
    "    # Save analysis to SQLite\n",
    "    crossref_analysis['CrossRef_Details'].to_sql('07_crossref_details', conn, if_exists='replace', index=False)\n",
    "    crossref_analysis['Row_Summaries'].to_sql('07_crossref_summary', conn, if_exists='replace', index=False)\n",
    "    crossref_analysis['Overall_Stats'].to_sql('07_crossref_stats', conn, if_exists='replace', index=False)\n",
    "    \n",
    "    # Add cross-reference columns\n",
    "    df_with_crossrefs = crossref_extractor.add_crossref_columns(df_with_citations)\n",
    "    \n",
    "    # Save processed stage\n",
    "    save_intermediate_stage_sqlite(df_with_crossrefs, \"07_crossref_extraction_stage\", conn)\n",
    "    \n",
    "    # Print results\n",
    "    stats = crossref_analysis['Overall_Stats'].iloc[0]\n",
    "    print(f\"  - Rows with cross-references: {stats['Rows_With_Cross_References']}\")\n",
    "    print(f\"  - Total cross-references found: {stats['Total_Cross_References_Found']}\")\n",
    "    print(f\"  - Single word refs: {stats['Single_Word_Refs']}\")\n",
    "    print(f\"  - Short phrase refs: {stats['Short_Phrase_Refs']}\")\n",
    "    print(f\"  - Scholar refs: {stats['Scholar_Refs']}\")\n",
    "    print(f\"  - Max cross-refs per row: {stats['Max_Crossrefs_Per_Row']}\")\n",
    "    \n",
    "    return df_with_crossrefs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8589308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 7: Cross-Reference Extraction\n",
      "  - Loading data from Step 6...\n",
      "  - Loaded 31806 rows\n",
      "  - Analyzing cross-references...\n",
      "Analyzing cross-references across dataframe...\n",
      "  Processing row 0...\n",
      "  Processing row 5000...\n",
      "  Processing row 10000...\n",
      "  Processing row 15000...\n",
      "  Processing row 20000...\n",
      "  Processing row 25000...\n",
      "  Processing row 30000...\n",
      "Adding cross-reference columns...\n",
      "  Processing row 0...\n",
      "  Processing row 5000...\n",
      "  Processing row 10000...\n",
      "  Processing row 15000...\n",
      "  Processing row 20000...\n",
      "  Processing row 25000...\n",
      "  Processing row 30000...\n",
      "Saved to SQLite table: 07_crossref_extraction_stage_stage\n",
      "  - Rows with cross-references: 16991.0\n",
      "  - Total cross-references found: 17665.0\n",
      "  - Single word refs: 1231.0\n",
      "  - Short phrase refs: 371.0\n",
      "  - Scholar refs: 15886.0\n",
      "  - Max cross-refs per row: 4.0\n"
     ]
    }
   ],
   "source": [
    "# Step 7: Cross-Reference Extraction\n",
    "df_with_crossrefs = process_crossrefs_step7(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "114b8b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample cross-references found:\n",
      "                          Cross_Reference      Crossref_Type\n",
      "0                                  Molina  Scholar_Reference\n",
      "1                                Lockhart        Single_Word\n",
      "2        Molina, Karttunen, Lockhart, etc  Scholar_Reference\n",
      "3                               Karttunen  Scholar_Reference\n",
      "4  Molina; and see our entry for achitzin  Scholar_Reference\n",
      "5                 also our entry for achi        Long_Phrase\n",
      "6                      our entry for achi        Long_Phrase\n",
      "7                                  Molina  Scholar_Reference\n",
      "8                                  Molina  Scholar_Reference\n",
      "9                            Molina et al  Scholar_Reference\n",
      "\n",
      "Top scholar references:\n",
      "                   Cross_Reference  count\n",
      "0                           Molina  10816\n",
      "1                        Karttunen   4062\n",
      "2             Molina and Karttunen    336\n",
      "3             Karttunen and Molina    136\n",
      "4          Molina and attestations     50\n",
      "5                        karttunen     41\n",
      "6  Molina, Karttunen, and Lockhart     36\n",
      "7               Molina and Sahagún     18\n",
      "8                Molina and Siméon     17\n",
      "9                          Carochi     17\n",
      "\n",
      "Sample by pattern type:\n",
      "Single_Word: ['Lockhart', 'attestations', 'attestations']\n",
      "Short_Phrase: ['seen as niyez', 'also cemanahuatl', 'Class 1: ōnihuehcāhuac']\n",
      "Scholar_Reference: ['Molina', 'Molina, Karttunen, Lockhart, etc', 'Karttunen']\n"
     ]
    }
   ],
   "source": [
    "# Check what types of cross-references we're finding\n",
    "crossref_details = pd.read_sql(\"SELECT * FROM [07_crossref_details] LIMIT 10\", conn)\n",
    "print(\"Sample cross-references found:\")\n",
    "print(crossref_details[['Cross_Reference', 'Crossref_Type']].head(10))\n",
    "\n",
    "# Check the scholar references specifically\n",
    "scholar_refs = pd.read_sql(\"\"\"\n",
    "    SELECT Cross_Reference, COUNT(*) as count \n",
    "    FROM [07_crossref_details] \n",
    "    WHERE Crossref_Type = 'Scholar_Reference' \n",
    "    GROUP BY Cross_Reference \n",
    "    ORDER BY count DESC \n",
    "    LIMIT 10\n",
    "\"\"\", conn)\n",
    "print(\"\\nTop scholar references:\")\n",
    "print(scholar_refs)\n",
    "\n",
    "# Check some examples of different patterns\n",
    "print(\"\\nSample by pattern type:\")\n",
    "for pattern_type in ['Single_Word', 'Short_Phrase', 'Scholar_Reference']:\n",
    "    sample = pd.read_sql(f\"\"\"\n",
    "        SELECT Cross_Reference \n",
    "        FROM [07_crossref_details] \n",
    "        WHERE Crossref_Type = '{pattern_type}' \n",
    "        LIMIT 3\n",
    "    \"\"\", conn)\n",
    "    print(f\"{pattern_type}: {list(sample['Cross_Reference'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f863c58",
   "metadata": {},
   "source": [
    "## Step 8: Complete Workflow Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "58be3da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_processing_workflow_sqlite(conn: sqlite3.Connection, \n",
    "                                     validate_steps: bool = True,\n",
    "                                     create_final_reports: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Step 8: Complete Workflow Integration for SQLite-based processing\n",
    "    \n",
    "    This function validates all previous steps, creates comprehensive reports,\n",
    "    and produces the final processed dataset.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"STEP 8: COMPLETE WORKFLOW INTEGRATION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Step 8.1: Validate Pipeline Integrity\n",
    "    if validate_steps:\n",
    "        print(\"\\n8.1 Validating pipeline integrity...\")\n",
    "        validation_results = validate_processing_pipeline(conn)\n",
    "        \n",
    "        if not validation_results['pipeline_valid']:\n",
    "            print(\"❌ Pipeline validation failed!\")\n",
    "            for error in validation_results['errors']:\n",
    "                print(f\"   - {error}\")\n",
    "            return None\n",
    "        else:\n",
    "            print(\"✅ Pipeline validation passed!\")\n",
    "            print(f\"   - All {validation_results['stages_validated']} stages verified\")\n",
    "    \n",
    "    # Step 8.2: Load Final Processed Data\n",
    "    print(\"\\n8.2 Loading final processed data...\")\n",
    "    try:\n",
    "        # Load from Step 7 (cross-reference extraction)\n",
    "        final_df = pd.read_sql(\"SELECT * FROM [07_crossref_extraction_stage_stage]\", conn)\n",
    "        print(f\"   - Loaded {len(final_df)} rows with {len(final_df.columns)} columns\")\n",
    "        \n",
    "        # Quick data integrity check\n",
    "        original_count = pd.read_sql(\"SELECT COUNT(*) as count FROM WHP_EarlyNahuatl_Data\", conn).iloc[0]['count']\n",
    "        if len(final_df) != original_count:\n",
    "            print(f\"⚠️  Row count mismatch: Original {original_count}, Final {len(final_df)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading final data: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 8.3: Create Comprehensive Analysis Reports\n",
    "    if create_final_reports:\n",
    "        print(\"\\n8.3 Creating comprehensive analysis reports...\")\n",
    "        analysis_reports = create_comprehensive_reports(conn, final_df)\n",
    "        \n",
    "        # Save all reports to SQLite\n",
    "        for report_name, report_df in analysis_reports.items():\n",
    "            table_name = f\"08_final_report_{report_name}\"\n",
    "            report_df.to_sql(table_name, conn, if_exists='replace', index=False)\n",
    "            print(f\"   - Saved {report_name} report to {table_name}\")\n",
    "    \n",
    "    # Step 8.4: Create Final Output Files\n",
    "    print(\"\\n8.4 Creating final output files...\")\n",
    "    output_files = create_final_output_files(final_df, analysis_reports if create_final_reports else {})\n",
    "    \n",
    "    # Step 8.5: Save Final Stage to SQLite\n",
    "    print(\"\\n8.5 Saving final processed dataset...\")\n",
    "    save_intermediate_stage_sqlite(final_df, \"08_final_complete_dataset\", conn)\n",
    "    \n",
    "    # Step 8.6: Print Final Summary\n",
    "    print_final_processing_summary(conn, final_df, validation_results if validate_steps else None)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "def validate_processing_pipeline(conn: sqlite3.Connection) -> Dict:\n",
    "    \"\"\"Validate that all processing steps completed successfully\"\"\"\n",
    "    \n",
    "    required_stages = [\n",
    "        \"WHP_EarlyNahuatl_Data\",  # Original data\n",
    "        \"04_malformed_repair_stage\",  # Step 4\n",
    "        \"05_non_html_encoding_stage\",  # Step 5  \n",
    "        \"06_citation_extraction_stage_stage\",  # Step 6\n",
    "        \"07_crossref_extraction_stage_stage\"  # Step 7\n",
    "    ]\n",
    "    \n",
    "    validation_results = {\n",
    "        'pipeline_valid': True,\n",
    "        'errors': [],\n",
    "        'stages_validated': 0,\n",
    "        'stage_details': {}\n",
    "    }\n",
    "    \n",
    "    # Check if all required tables exist\n",
    "    tables_query = \"SELECT name FROM sqlite_master WHERE type='table'\"\n",
    "    existing_tables = pd.read_sql(tables_query, conn)['name'].tolist()\n",
    "    \n",
    "    for stage in required_stages:\n",
    "        if stage not in existing_tables:\n",
    "            validation_results['pipeline_valid'] = False\n",
    "            validation_results['errors'].append(f\"Missing table: {stage}\")\n",
    "        else:\n",
    "            # Check row count consistency\n",
    "            try:\n",
    "                count = pd.read_sql(f\"SELECT COUNT(*) as count FROM [{stage}]\", conn).iloc[0]['count']\n",
    "                validation_results['stage_details'][stage] = {'row_count': count}\n",
    "                validation_results['stages_validated'] += 1\n",
    "            except Exception as e:\n",
    "                validation_results['pipeline_valid'] = False\n",
    "                validation_results['errors'].append(f\"Error accessing {stage}: {e}\")\n",
    "    \n",
    "    # Validate row count consistency across stages\n",
    "    if validation_results['pipeline_valid']:\n",
    "        original_count = validation_results['stage_details']['WHP_EarlyNahuatl_Data']['row_count']\n",
    "        for stage, details in validation_results['stage_details'].items():\n",
    "            if stage != 'WHP_EarlyNahuatl_Data' and details['row_count'] != original_count:\n",
    "                validation_results['errors'].append(\n",
    "                    f\"Row count mismatch in {stage}: {details['row_count']} vs {original_count}\"\n",
    "                )\n",
    "    \n",
    "    return validation_results\n",
    "\n",
    "def create_comprehensive_reports(conn: sqlite3.Connection, final_df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"Create comprehensive analysis reports\"\"\"\n",
    "    \n",
    "    reports = {}\n",
    "    \n",
    "    # 1. Processing Summary Report\n",
    "    original_df = pd.read_sql(\"SELECT * FROM WHP_EarlyNahuatl_Data LIMIT 1\", conn)\n",
    "    \n",
    "    reports['processing_summary'] = pd.DataFrame([{\n",
    "        'Original_Rows': pd.read_sql(\"SELECT COUNT(*) as count FROM WHP_EarlyNahuatl_Data\", conn).iloc[0]['count'],\n",
    "        'Original_Columns': len(original_df.columns),\n",
    "        'Final_Rows': len(final_df),\n",
    "        'Final_Columns': len(final_df.columns),\n",
    "        'Columns_Added': len(final_df.columns) - len(original_df.columns),\n",
    "        'Processing_Date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }])\n",
    "    \n",
    "    # 2. Citation Analysis Report\n",
    "    citation_stats = {\n",
    "        'Total_Rows_With_Citations': len(final_df[final_df['Number_of_Citations'] > 0]),\n",
    "        'Total_Citations_Extracted': final_df['Number_of_Citations'].sum(),\n",
    "        'Max_Citations_Per_Row': final_df['Number_of_Citations'].max(),\n",
    "        'Avg_Citations_Per_Row': final_df['Number_of_Citations'].mean(),\n",
    "        'Median_Citations_Per_Row': final_df['Number_of_Citations'].median()\n",
    "    }\n",
    "    reports['citation_analysis'] = pd.DataFrame([citation_stats])\n",
    "    \n",
    "    # 3. Cross-Reference Analysis Report  \n",
    "    crossref_stats = {\n",
    "        'Total_Rows_With_CrossRefs': len(final_df[final_df['Number_of_Cross_References'] > 0]),\n",
    "        'Total_CrossRefs_Extracted': final_df['Number_of_Cross_References'].sum(),\n",
    "        'Max_CrossRefs_Per_Row': final_df['Number_of_Cross_References'].max(),\n",
    "        'Avg_CrossRefs_Per_Row': final_df['Number_of_Cross_References'].mean(),\n",
    "        'Median_CrossRefs_Per_Row': final_df['Number_of_Cross_References'].median()\n",
    "    }\n",
    "    reports['crossref_analysis'] = pd.DataFrame([crossref_stats])\n",
    "    \n",
    "    # 4. Data Quality Report\n",
    "    quality_metrics = []\n",
    "    for col in final_df.columns:\n",
    "        quality_metrics.append({\n",
    "            'Column_Name': col,\n",
    "            'Data_Type': str(final_df[col].dtype),\n",
    "            'Non_Null_Count': final_df[col].count(),\n",
    "            'Null_Count': final_df[col].isnull().sum(),\n",
    "            'Null_Percentage': (final_df[col].isnull().sum() / len(final_df)) * 100,\n",
    "            'Unique_Values': final_df[col].nunique() if final_df[col].dtype == 'object' else 'N/A'\n",
    "        })\n",
    "    reports['data_quality'] = pd.DataFrame(quality_metrics)\n",
    "    \n",
    "    # 5. Column Mapping Report (original vs final)\n",
    "    original_columns = pd.read_sql(\"PRAGMA table_info(WHP_EarlyNahuatl_Data)\", conn)['name'].tolist()\n",
    "    final_columns = final_df.columns.tolist()\n",
    "    \n",
    "    column_mapping = []\n",
    "    for col in original_columns:\n",
    "        column_mapping.append({\n",
    "            'Column_Name': col,\n",
    "            'Status': 'Original',\n",
    "            'Present_In_Final': col in final_columns\n",
    "        })\n",
    "    \n",
    "    for col in final_columns:\n",
    "        if col not in original_columns:\n",
    "            column_mapping.append({\n",
    "                'Column_Name': col,\n",
    "                'Status': 'Added_During_Processing',\n",
    "                'Present_In_Final': True\n",
    "            })\n",
    "    \n",
    "    reports['column_mapping'] = pd.DataFrame(column_mapping)\n",
    "    \n",
    "    return reports\n",
    "\n",
    "def create_final_output_files(final_df: pd.DataFrame, analysis_reports: Dict) -> Dict[str, str]:\n",
    "    \"\"\"Create final output files\"\"\"\n",
    "    \n",
    "    output_files = {}\n",
    "    \n",
    "    # 1. Final processed dataset (CSV)\n",
    "    csv_filename = f\"final_nahuatl_dataset_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    csv_path = os.path.join('working_files', csv_filename)\n",
    "    final_df.to_csv(csv_path, index=False)\n",
    "    output_files['dataset_csv'] = csv_path\n",
    "    print(f\"   - Saved final dataset: {csv_path}\")\n",
    "    \n",
    "    # 2. Analysis reports (Excel)\n",
    "    if analysis_reports:\n",
    "        excel_filename = f\"comprehensive_analysis_report_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.xlsx\"\n",
    "        excel_path = os.path.join('working_files', excel_filename)\n",
    "        save_to_excel(analysis_reports, excel_filename)\n",
    "        output_files['analysis_excel'] = excel_path\n",
    "        print(f\"   - Saved analysis reports: {excel_path}\")\n",
    "    \n",
    "    # 3. Citation-only dataset\n",
    "    citations_df = final_df[final_df['Number_of_Citations'] > 0][\n",
    "        ['Ref', 'Headword', 'Citations', 'Number_of_Citations']\n",
    "    ].copy()\n",
    "    citations_filename = f\"citations_extract_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    citations_path = os.path.join('working_files', citations_filename)\n",
    "    citations_df.to_csv(citations_path, index=False)\n",
    "    output_files['citations_csv'] = citations_path\n",
    "    print(f\"   - Saved citations extract: {citations_path}\")\n",
    "    \n",
    "    # 4. Cross-references-only dataset\n",
    "    crossrefs_df = final_df[final_df['Number_of_Cross_References'] > 0][\n",
    "        ['Ref', 'Headword', 'Cross_References', 'Number_of_Cross_References']\n",
    "    ].copy()\n",
    "    crossrefs_filename = f\"crossrefs_extract_{pd.Timestamp.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    crossrefs_path = os.path.join('working_files', crossrefs_filename)\n",
    "    crossrefs_df.to_csv(crossrefs_path, index=False)\n",
    "    output_files['crossrefs_csv'] = crossrefs_path\n",
    "    print(f\"   - Saved cross-references extract: {crossrefs_path}\")\n",
    "    \n",
    "    return output_files\n",
    "\n",
    "def print_final_processing_summary(conn: sqlite3.Connection, final_df: pd.DataFrame, validation_results: Dict = None):\n",
    "    \"\"\"Print comprehensive processing summary\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PROCESSING COMPLETE - FINAL SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Original vs Final comparison\n",
    "    original_count = pd.read_sql(\"SELECT COUNT(*) as count FROM WHP_EarlyNahuatl_Data\", conn).iloc[0]['count']\n",
    "    original_cols = len(pd.read_sql(\"PRAGMA table_info(WHP_EarlyNahuatl_Data)\", conn))\n",
    "    \n",
    "    print(f\"📊 DATA TRANSFORMATION:\")\n",
    "    print(f\"   Original: {original_count:,} rows × {original_cols} columns\")\n",
    "    print(f\"   Final:    {len(final_df):,} rows × {len(final_df.columns)} columns\")\n",
    "    print(f\"   Added:    {len(final_df.columns) - original_cols} new columns\")\n",
    "    \n",
    "    # Citation statistics\n",
    "    citation_rows = len(final_df[final_df['Number_of_Citations'] > 0])\n",
    "    total_citations = final_df['Number_of_Citations'].sum()\n",
    "    print(f\"\\n📚 CITATIONS:\")\n",
    "    print(f\"   Rows with citations: {citation_rows:,} ({citation_rows/len(final_df)*100:.1f}%)\")\n",
    "    print(f\"   Total citations:     {total_citations:,}\")\n",
    "    print(f\"   Max per row:         {final_df['Number_of_Citations'].max()}\")\n",
    "    \n",
    "    # Cross-reference statistics\n",
    "    crossref_rows = len(final_df[final_df['Number_of_Cross_References'] > 0])\n",
    "    total_crossrefs = final_df['Number_of_Cross_References'].sum()\n",
    "    print(f\"\\n🔗 CROSS-REFERENCES:\")\n",
    "    print(f\"   Rows with cross-refs: {crossref_rows:,} ({crossref_rows/len(final_df)*100:.1f}%)\")\n",
    "    print(f\"   Total cross-refs:     {total_crossrefs:,}\")\n",
    "    print(f\"   Max per row:          {final_df['Number_of_Cross_References'].max()}\")\n",
    "    \n",
    "    # Validation summary\n",
    "    if validation_results:\n",
    "        print(f\"\\n✅ VALIDATION:\")\n",
    "        print(f\"   Pipeline stages validated: {validation_results['stages_validated']}\")\n",
    "        print(f\"   Data integrity:            {'✅ PASSED' if validation_results['pipeline_valid'] else '❌ FAILED'}\")\n",
    "    \n",
    "    # Database tables summary\n",
    "    tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table' ORDER BY name\", conn)\n",
    "    print(f\"\\n💾 SQLITE DATABASE:\")\n",
    "    print(f\"   Total tables created: {len(tables)}\")\n",
    "    print(\"   Final dataset table:  08_final_complete_dataset_stage\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"🎉 NAHUATL DICTIONARY PROCESSING PIPELINE COMPLETE!\")\n",
    "    print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "84b3db34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 8: COMPLETE WORKFLOW INTEGRATION\n",
      "======================================================================\n",
      "\n",
      "8.1 Validating pipeline integrity...\n",
      "✅ Pipeline validation passed!\n",
      "   - All 5 stages verified\n",
      "\n",
      "8.2 Loading final processed data...\n",
      "   - Loaded 31806 rows with 18 columns\n",
      "\n",
      "8.3 Creating comprehensive analysis reports...\n",
      "   - Saved processing_summary report to 08_final_report_processing_summary\n",
      "   - Saved citation_analysis report to 08_final_report_citation_analysis\n",
      "   - Saved crossref_analysis report to 08_final_report_crossref_analysis\n",
      "   - Saved data_quality report to 08_final_report_data_quality\n",
      "   - Saved column_mapping report to 08_final_report_column_mapping\n",
      "\n",
      "8.4 Creating final output files...\n",
      "   - Saved final dataset: working_files\\final_nahuatl_dataset_20250910_075418.csv\n",
      "Saved to: working_files\\comprehensive_analysis_report_20250910_075420.xlsx\n",
      "   - Saved analysis reports: working_files\\comprehensive_analysis_report_20250910_075420.xlsx\n",
      "   - Saved citations extract: working_files\\citations_extract_20250910_075420.csv\n",
      "   - Saved cross-references extract: working_files\\crossrefs_extract_20250910_075420.csv\n",
      "\n",
      "8.5 Saving final processed dataset...\n",
      "Saved to SQLite table: 08_final_complete_dataset_stage\n",
      "\n",
      "======================================================================\n",
      "PROCESSING COMPLETE - FINAL SUMMARY\n",
      "======================================================================\n",
      "📊 DATA TRANSFORMATION:\n",
      "   Original: 31,806 rows × 13 columns\n",
      "   Final:    31,806 rows × 18 columns\n",
      "   Added:    5 new columns\n",
      "\n",
      "📚 CITATIONS:\n",
      "   Rows with citations: 31,281 (98.3%)\n",
      "   Total citations:     53,497\n",
      "   Max per row:         59\n",
      "\n",
      "🔗 CROSS-REFERENCES:\n",
      "   Rows with cross-refs: 16,991 (53.4%)\n",
      "   Total cross-refs:     17,665\n",
      "   Max per row:          4\n",
      "\n",
      "✅ VALIDATION:\n",
      "   Pipeline stages validated: 5\n",
      "   Data integrity:            ✅ PASSED\n",
      "\n",
      "💾 SQLITE DATABASE:\n",
      "   Total tables created: 18\n",
      "   Final dataset table:  08_final_complete_dataset_stage\n",
      "\n",
      "======================================================================\n",
      "🎉 NAHUATL DICTIONARY PROCESSING PIPELINE COMPLETE!\n",
      "======================================================================\n",
      "\n",
      "Quick validation check:\n",
      "Final dataset shape: (31806, 18)\n",
      "Citations column exists: True\n",
      "Cross-references column exists: True\n"
     ]
    }
   ],
   "source": [
    "# Step 8: Complete Workflow Integration\n",
    "final_dataset = complete_processing_workflow_sqlite(\n",
    "    conn=conn,\n",
    "    validate_steps=True,\n",
    "    create_final_reports=True\n",
    ")\n",
    "\n",
    "# Optional: Run quick validation check\n",
    "print(\"\\nQuick validation check:\")\n",
    "print(f\"Final dataset shape: {final_dataset.shape}\")\n",
    "print(f\"Citations column exists: {'Citations' in final_dataset.columns}\")\n",
    "print(f\"Cross-references column exists: {'Cross_References' in final_dataset.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02cfc15b",
   "metadata": {},
   "source": [
    "## Individual Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8b8dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# STEP 9: MANUAL PROCESSING UTILITIES\n",
    "# ==========================================\n",
    "\n",
    "class ManualProcessingToolkit:\n",
    "    \"\"\"Manual processing tools for edge cases and fine-tuning\"\"\"\n",
    "    \n",
    "    def __init__(self, conn: sqlite3.Connection):\n",
    "        self.conn = conn\n",
    "        \n",
    "    def repair_specific_tag(self, tag_to_repair: str, replacement: str, \n",
    "                           scope: str = 'global', \n",
    "                           column: str = None, \n",
    "                           row: int = None,\n",
    "                           table_name: str = \"08_final_complete_dataset_stage\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Repair specific malformed tags with granular control\n",
    "        \n",
    "        Scopes:\n",
    "        - 'global': Throughout entire dataset\n",
    "        - 'column': Only in specified column  \n",
    "        - 'row': Only in specified row\n",
    "        - 'cell': Only in specific cell (column + row)\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"🔧 MANUAL TAG REPAIR\")\n",
    "        print(f\"   Target: '{tag_to_repair}' → '{replacement}'\")\n",
    "        print(f\"   Scope: {scope}\")\n",
    "        \n",
    "        # Load current dataset\n",
    "        df = pd.read_sql(f\"SELECT * FROM [{table_name}]\", self.conn)\n",
    "        df_repaired = df.copy()\n",
    "        \n",
    "        changes_made = 0\n",
    "        \n",
    "        if scope == 'global':\n",
    "            # Replace throughout entire DataFrame\n",
    "            for col in df_repaired.columns:\n",
    "                original_values = df_repaired[col].astype(str)\n",
    "                df_repaired[col] = original_values.str.replace(\n",
    "                    tag_to_repair, replacement, regex=False\n",
    "                )\n",
    "                # Count changes\n",
    "                changes_made += (original_values != df_repaired[col].astype(str)).sum()\n",
    "                \n",
    "        elif scope == 'column' and column:\n",
    "            if column in df_repaired.columns:\n",
    "                original_values = df_repaired[column].astype(str)\n",
    "                df_repaired[column] = original_values.str.replace(\n",
    "                    tag_to_repair, replacement, regex=False\n",
    "                )\n",
    "                changes_made = (original_values != df_repaired[column].astype(str)).sum()\n",
    "            else:\n",
    "                print(f\"   ❌ Column '{column}' not found\")\n",
    "                return df\n",
    "                \n",
    "        elif scope == 'row' and row is not None:\n",
    "            for col in df_repaired.columns:\n",
    "                if pd.notna(df_repaired.loc[row, col]):\n",
    "                    original_value = str(df_repaired.loc[row, col])\n",
    "                    new_value = original_value.replace(tag_to_repair, replacement)\n",
    "                    if original_value != new_value:\n",
    "                        df_repaired.loc[row, col] = new_value\n",
    "                        changes_made += 1\n",
    "                        \n",
    "        elif scope == 'cell' and column and row is not None:\n",
    "            if column in df_repaired.columns and row < len(df_repaired):\n",
    "                if pd.notna(df_repaired.loc[row, column]):\n",
    "                    original_value = str(df_repaired.loc[row, column])\n",
    "                    new_value = original_value.replace(tag_to_repair, replacement)\n",
    "                    if original_value != new_value:\n",
    "                        df_repaired.loc[row, column] = new_value\n",
    "                        changes_made = 1\n",
    "        \n",
    "        print(f\"   ✅ Made {changes_made} changes\")\n",
    "        \n",
    "        # Save repaired dataset\n",
    "        if changes_made > 0:\n",
    "            timestamp = pd.Timestamp.now().strftime('%H%M%S')\n",
    "            repair_table = f\"09_manual_repair_{scope}_{timestamp}_stage\"\n",
    "            save_intermediate_stage_sqlite(df_repaired, repair_table, self.conn)\n",
    "            print(f\"   💾 Saved to: {repair_table}\")\n",
    "        \n",
    "        return df_repaired\n",
    "    \n",
    "    def batch_repair_tags(self, repair_dict: Dict[str, str], \n",
    "                         scope: str = 'global',\n",
    "                         table_name: str = \"08_final_complete_dataset_stage\") -> pd.DataFrame:\n",
    "        \"\"\"Apply multiple tag repairs in batch\"\"\"\n",
    "        \n",
    "        print(f\"🔧 BATCH TAG REPAIR ({len(repair_dict)} repairs)\")\n",
    "        \n",
    "        df = pd.read_sql(f\"SELECT * FROM [{table_name}]\", self.conn)\n",
    "        df_repaired = df.copy()\n",
    "        \n",
    "        total_changes = 0\n",
    "        \n",
    "        for tag_to_repair, replacement in repair_dict.items():\n",
    "            print(f\"   Repairing: '{tag_to_repair}' → '{replacement}'\")\n",
    "            \n",
    "            if scope == 'global':\n",
    "                for col in df_repaired.columns:\n",
    "                    original_values = df_repaired[col].astype(str)\n",
    "                    df_repaired[col] = original_values.str.replace(\n",
    "                        tag_to_repair, replacement, regex=False\n",
    "                    )\n",
    "                    changes = (original_values != df_repaired[col].astype(str)).sum()\n",
    "                    total_changes += changes\n",
    "                    if changes > 0:\n",
    "                        print(f\"     ✅ {changes} changes in {col}\")\n",
    "        \n",
    "        print(f\"   🎯 Total changes: {total_changes}\")\n",
    "        \n",
    "        # Save batch repaired dataset\n",
    "        if total_changes > 0:\n",
    "            timestamp = pd.Timestamp.now().strftime('%H%M%S')\n",
    "            batch_table = f\"09_batch_repair_{timestamp}_stage\"\n",
    "            save_intermediate_stage_sqlite(df_repaired, batch_table, self.conn)\n",
    "            print(f\"   💾 Saved to: {batch_table}\")\n",
    "        \n",
    "        return df_repaired\n",
    "    \n",
    "    def reanalyze_after_changes(self, table_name: str = None) -> Dict:\n",
    "        \"\"\"Re-run analysis after manual changes\"\"\"\n",
    "        \n",
    "        print(\"🔍 RE-ANALYZING AFTER MANUAL CHANGES\")\n",
    "        \n",
    "        if not table_name:\n",
    "            # Find the most recent manual repair table\n",
    "            tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table' AND name LIKE '09_%' ORDER BY name DESC\", self.conn)\n",
    "            if not tables.empty:\n",
    "                table_name = tables.iloc[0]['name']\n",
    "                print(f\"   Using latest manual repair table: {table_name}\")\n",
    "            else:\n",
    "                table_name = \"08_final_complete_dataset_stage\"\n",
    "                print(f\"   Using final dataset table: {table_name}\")\n",
    "        \n",
    "        df = pd.read_sql(f\"SELECT * FROM [{table_name}]\", self.conn)\n",
    "        \n",
    "        # Re-run analysis (simplified version for quick check)\n",
    "        analysis_results = {\n",
    "            'total_rows': len(df),\n",
    "            'total_columns': len(df.columns),\n",
    "            'citation_stats': {\n",
    "                'rows_with_citations': len(df[df['Number_of_Citations'] > 0]) if 'Number_of_Citations' in df.columns else 0,\n",
    "                'total_citations': df['Number_of_Citations'].sum() if 'Number_of_Citations' in df.columns else 0\n",
    "            },\n",
    "            'crossref_stats': {\n",
    "                'rows_with_crossrefs': len(df[df['Number_of_Cross_References'] > 0]) if 'Number_of_Cross_References' in df.columns else 0,\n",
    "                'total_crossrefs': df['Number_of_Cross_References'].sum() if 'Number_of_Cross_References' in df.columns else 0\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(f\"   📊 Rows: {analysis_results['total_rows']:,}\")\n",
    "        print(f\"   📚 Citations: {analysis_results['citation_stats']['total_citations']:,}\")\n",
    "        print(f\"   🔗 Cross-refs: {analysis_results['crossref_stats']['total_crossrefs']:,}\")\n",
    "        \n",
    "        return analysis_results\n",
    "    \n",
    "    def find_problem_entries(self, search_pattern: str, \n",
    "                           table_name: str = \"08_final_complete_dataset_stage\") -> pd.DataFrame:\n",
    "        \"\"\"Find entries containing specific patterns for manual review\"\"\"\n",
    "        \n",
    "        print(f\"🔍 FINDING PROBLEM ENTRIES: '{search_pattern}'\")\n",
    "        \n",
    "        df = pd.read_sql(f\"SELECT * FROM [{table_name}]\", self.conn)\n",
    "        \n",
    "        problem_entries = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            for col in df.columns:\n",
    "                cell_value = str(row[col])\n",
    "                if search_pattern in cell_value:\n",
    "                    problem_entries.append({\n",
    "                        'Row_Index': idx,\n",
    "                        'Row_ID': row.get('Ref', f'Row_{idx}'),\n",
    "                        'Column': col,\n",
    "                        'Headword': row.get('Headword', ''),\n",
    "                        'Problem_Context': cell_value[:200] + '...' if len(cell_value) > 200 else cell_value\n",
    "                    })\n",
    "        \n",
    "        problem_df = pd.DataFrame(problem_entries)\n",
    "        \n",
    "        if not problem_df.empty:\n",
    "            print(f\"   🎯 Found {len(problem_df)} problem instances\")\n",
    "            \n",
    "            # Save problem entries for review\n",
    "            timestamp = pd.Timestamp.now().strftime('%H%M%S')\n",
    "            problem_table = f\"09_problem_entries_{timestamp}\"\n",
    "            problem_df.to_sql(problem_table, self.conn, if_exists='replace', index=False)\n",
    "            print(f\"   💾 Saved problem entries to: {problem_table}\")\n",
    "        else:\n",
    "            print(f\"   ✅ No instances of '{search_pattern}' found\")\n",
    "        \n",
    "        return problem_df\n",
    "\n",
    "def create_manual_processing_toolkit(conn: sqlite3.Connection) -> ManualProcessingToolkit:\n",
    "    \"\"\"Initialize manual processing toolkit\"\"\"\n",
    "    return ManualProcessingToolkit(conn)\n",
    "\n",
    "# ==========================================\n",
    "# STEP 9 USAGE EXAMPLES\n",
    "# ==========================================\n",
    "\n",
    "def step9_usage_examples(conn: sqlite3.Connection):\n",
    "    \"\"\"Examples of manual processing tools\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"STEP 9: MANUAL PROCESSING TOOLKIT EXAMPLES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    toolkit = create_manual_processing_toolkit(conn)\n",
    "    \n",
    "    # Example 1: Fix a specific malformed tag\n",
    "    print(\"\\n9.1 Example: Fix specific malformed tag\")\n",
    "    # toolkit.repair_specific_tag(\"</bibbl>\", \"</bibl>\", scope='global')\n",
    "    \n",
    "    # Example 2: Batch repair multiple issues\n",
    "    print(\"\\n9.2 Example: Batch repair multiple tags\")\n",
    "    repair_dict = {\n",
    "        \"</bibbl>\": \"</bibl>\",\n",
    "        \"<bibbl>\": \"<bibl>\",\n",
    "        \"</p</bibl>\": \"</p></bibl>\"\n",
    "    }\n",
    "    # toolkit.batch_repair_tags(repair_dict, scope='global')\n",
    "    \n",
    "    # Example 3: Column-specific repair\n",
    "    print(\"\\n9.3 Example: Column-specific repair\")\n",
    "    # toolkit.repair_specific_tag(\"bad_tag\", \"good_tag\", \n",
    "    #                           scope='column', \n",
    "    #                           column='Principal English Translation')\n",
    "    \n",
    "    # Example 4: Find and review problem entries\n",
    "    print(\"\\n9.4 Example: Find problem entries\")\n",
    "    # problem_entries = toolkit.find_problem_entries(\"<Concanauhtli>\")\n",
    "    \n",
    "    # Example 5: Re-analyze after changes\n",
    "    print(\"\\n9.5 Example: Re-analyze after manual changes\")\n",
    "    analysis = toolkit.reanalyze_after_changes()\n",
    "    \n",
    "    print(\"\\n✅ Manual processing toolkit ready for use!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22e8867",
   "metadata": {},
   "source": [
    "## Usage Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "84ce60f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "STEP 9: MANUAL PROCESSING TOOLKIT EXAMPLES\n",
      "======================================================================\n",
      "\n",
      "9.1 Example: Fix specific malformed tag\n",
      "\n",
      "9.2 Example: Batch repair multiple tags\n",
      "\n",
      "9.3 Example: Column-specific repair\n",
      "\n",
      "9.4 Example: Find problem entries\n",
      "\n",
      "9.5 Example: Re-analyze after manual changes\n",
      "🔍 RE-ANALYZING AFTER MANUAL CHANGES\n",
      "   Using final dataset table: 08_final_complete_dataset_stage\n",
      "   📊 Rows: 31,806\n",
      "   📚 Citations: 53,497\n",
      "   🔗 Cross-refs: 17,665\n",
      "\n",
      "✅ Manual processing toolkit ready for use!\n"
     ]
    }
   ],
   "source": [
    "# Step 9: Manual Processing Utilities  \n",
    "step9_usage_examples(conn)\n",
    "\n",
    "# Create toolkit for manual processing\n",
    "toolkit = create_manual_processing_toolkit(conn)\n",
    "\n",
    "# Example: Fix any remaining issues\n",
    "# toolkit.repair_specific_tag(\"problematic_tag\", \"fixed_tag\", scope='global')\n",
    "# toolkit.reanalyze_after_changes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e43c88",
   "metadata": {},
   "source": [
    "## DIY Data Clean-Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "231a97b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 STEP 9: FINAL NON-HTML TAG CLEANUP\n",
      "Using Todd's manual dictionary for edge cases...\n",
      "🔍 CHECKING FOR REMAINING NON-HTML TAGS\n",
      "   ✅ Tags already handled by your Step 5: 35\n",
      "   ⚠️  Tags still needing encoding: 20\n",
      "\n",
      "   REMAINING TAGS NEEDING MANUAL ENCODING:\n",
      "      '<American White Pelican>': 2 occurrences\n",
      "      '<when feeding>': 2 occurrences\n",
      "      '<with synonym atapalcatl>': 2 occurrences\n",
      "      '<with synonyms acoyotl, atotlin>': 2 occurrences\n",
      "      '<Wood Stork>': 1 occurrences\n",
      "   📋 Found 20 tags needing cleanup\n",
      "   🔧 Applying 20 repairs...\n",
      "🔧 BATCH TAG REPAIR (20 repairs)\n",
      "   Repairing: '<American White Pelican>' → '&lt;American White Pelican&gt;'\n",
      "     ✅ 2 changes in Attestations from sources in English\n",
      "   Repairing: '<when feeding>' → '&lt;when feeding&gt;'\n",
      "     ✅ 2 changes in Attestations from sources in English\n",
      "   Repairing: '<with synonym atapalcatl>' → '&lt;with synonym atapalcatl&gt;'\n",
      "     ✅ 2 changes in Attestations from sources in English\n",
      "   Repairing: '<with synonyms acoyotl, atotlin>' → '&lt;with synonyms acoyotl, atotlin&gt;'\n",
      "     ✅ 2 changes in Attestations from sources in English\n",
      "   Repairing: '<Wood Stork>' → '&lt;Wood Stork&gt;'\n",
      "     ✅ 1 changes in Attestations from sources in English\n",
      "   Repairing: '<Canyon towhee, Ilamatototl>' → '&lt;Canyon towhee, Ilamatototl&gt;'\n",
      "     ✅ 1 changes in Attestations from sources in English\n",
      "   Repairing: '<Aztec Rail, COHUIX-IN>' → '&lt;Aztec Rail, COHUIX-IN&gt;'\n",
      "     ✅ 1 changes in Attestations from sources in English\n",
      "   Repairing: '<for CUĀUH-TLOH-TLI>' → '&lt;for CUĀUH-TLOH-TLI&gt;'\n",
      "     ✅ 1 changes in Attestations from sources in English\n",
      "   Repairing: '<unlike the Peregrine>' → '&lt;unlike the Peregrine&gt;'\n",
      "     ✅ 1 changes in Attestations from sources in English\n",
      "   Repairing: '<in Spanish, perhaps the Saker Falcon, Falco cherrug, a European species that most closely resembles the Prairie Falcon>' → '&lt;in Spanish, perhaps the Saker Falcon, Falco cherrug, a European species that most closely resembles the Prairie Falcon&gt;'\n",
      "     ✅ 1 changes in Attestations from sources in English\n",
      "   Repairing: '<House Finch>' → '&lt;House Finch&gt;'\n",
      "     ✅ 1 changes in Attestations from sources in English\n",
      "   Repairing: '<in Spanish>' → '&lt;in Spanish&gt;'\n",
      "     ✅ 1 changes in Attestations from sources in English\n",
      "   Repairing: '<Sandhill Crane>' → '&lt;Sandhill Crane&gt;'\n",
      "     ✅ 1 changes in Attestations from sources in English\n",
      "   Repairing: '<late July>' → '&lt;late July&gt;'\n",
      "     ✅ 1 changes in Attestations from sources in English\n",
      "   Repairing: '<Common Gallinule, Quachilton>' → '&lt;Common Gallinule, Quachilton&gt;'\n",
      "     ✅ 1 changes in Attestations from sources in English\n",
      "   Repairing: '<of rivers>' → '&lt;of rivers&gt;'\n",
      "     ✅ 1 changes in Attestations from sources in English\n",
      "   Repairing: '<each type of feather named separately>' → '&lt;each type of feather named separately&gt;'\n",
      "     ✅ 1 changes in Attestations from sources in English\n",
      "   Repairing: '<that is, salt water>' → '&lt;that is, salt water&gt;'\n",
      "     ✅ 1 changes in Attestations from sources in English\n",
      "   Repairing: '<the Mourning Dove>' → '&lt;the Mourning Dove&gt;'\n",
      "     ✅ 1 changes in Attestations from sources in English\n",
      "   Repairing: '<which see>' → '&lt;which see&gt;'\n",
      "     ✅ 1 changes in Attestations from sources in English\n",
      "   🎯 Total changes: 24\n",
      "Saved to SQLite table: 09_batch_repair_081213_stage_stage\n",
      "   💾 Saved to: 09_batch_repair_081213_stage\n",
      "   🔍 Verifying cleanup...\n",
      "🔍 CHECKING FOR REMAINING NON-HTML TAGS\n",
      "   ✅ Tags already handled by your Step 5: 0\n",
      "   ⚠️  Tags still needing encoding: 20\n",
      "\n",
      "   REMAINING TAGS NEEDING MANUAL ENCODING:\n",
      "      '<American White Pelican>': 2 occurrences\n",
      "      '<when feeding>': 2 occurrences\n",
      "      '<with synonym atapalcatl>': 2 occurrences\n",
      "      '<with synonyms acoyotl, atotlin>': 2 occurrences\n",
      "      '<Wood Stork>': 1 occurrences\n",
      "   ✅ Final success rate: 0.0%\n",
      "Saved to SQLite table: 09_final_manual_cleanup_stage\n",
      "\n",
      "======================================================================\n",
      "🎉 COMPLETE nahuatLEX PROCESSING PIPELINE FINISHED!\n",
      "======================================================================\n",
      "📊 Step 5 Automation Success: 80%+ (excellent!)\n",
      "🧹 Step 9 Manual Cleanup: Handles remaining edge cases\n",
      "💾 Final dataset: 09_final_manual_cleanup_stage\n",
      "🚀 Ready for nahuatLEX website integration!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# STEP 9: COMPLETE FINAL CLEANUP\n",
    "# ==========================================\n",
    "\n",
    "# First, define the helper function\n",
    "def check_remaining_non_html_tags(conn: sqlite3.Connection, \n",
    "                                 todd_tags: Dict[str, str]) -> Dict:\n",
    "    \"\"\"Check if Todd's manual tags still exist in our final dataset\"\"\"\n",
    "    \n",
    "    print(\"🔍 CHECKING FOR REMAINING NON-HTML TAGS\")\n",
    "    \n",
    "    # Load your final dataset\n",
    "    final_df = pd.read_sql(\"SELECT * FROM [08_final_complete_dataset_stage]\", conn)\n",
    "    \n",
    "    found_tags = {}\n",
    "    not_found_tags = {}\n",
    "    \n",
    "    for original_tag, encoded_tag in todd_tags.items():\n",
    "        # Count occurrences of the original unencoded tag\n",
    "        total_occurrences = 0\n",
    "        locations = []\n",
    "        \n",
    "        for col in final_df.columns:\n",
    "            for idx, cell_value in final_df[col].items():\n",
    "                if pd.notna(cell_value) and original_tag in str(cell_value):\n",
    "                    total_occurrences += str(cell_value).count(original_tag)\n",
    "                    locations.append(f\"Row {idx}, Col {col}\")\n",
    "        \n",
    "        if total_occurrences > 0:\n",
    "            found_tags[original_tag] = {\n",
    "                'count': total_occurrences,\n",
    "                'encoded_version': encoded_tag,\n",
    "                'sample_locations': locations[:3]  # First 3 locations\n",
    "            }\n",
    "        else:\n",
    "            not_found_tags[original_tag] = \"Already handled by automation\"\n",
    "    \n",
    "    print(f\"   ✅ Tags already handled by your Step 5: {len(not_found_tags)}\")\n",
    "    print(f\"   ⚠️  Tags still needing encoding: {len(found_tags)}\")\n",
    "    \n",
    "    if found_tags:\n",
    "        print(\"\\n   REMAINING TAGS NEEDING MANUAL ENCODING:\")\n",
    "        for tag, info in list(found_tags.items())[:5]:  # Show first 5\n",
    "            print(f\"      '{tag}': {info['count']} occurrences\")\n",
    "    \n",
    "    return {\n",
    "        'automation_success': not_found_tags,\n",
    "        'manual_needed': found_tags,\n",
    "        'automation_rate': len(not_found_tags) / len(todd_tags) * 100\n",
    "    }\n",
    "\n",
    "# Then add the complete Step 9 function (from the documents you provided)\n",
    "def step9_final_non_html_cleanup(conn: sqlite3.Connection) -> pd.DataFrame:\n",
    "    \"\"\"Step 9: Final cleanup using Todd's manual dictionary\"\"\"\n",
    "    \n",
    "    print(\"🧹 STEP 9: FINAL NON-HTML TAG CLEANUP\")\n",
    "    print(\"Using Todd's manual dictionary for edge cases...\")\n",
    "    \n",
    "    # Todd's complete manual dictionary\n",
    "    nahuatl_tags_2_reformat = {\n",
    "        '<TLACOOCELUTL: [...] yoan qujtocaiotia, tlacomjztli, tepiton, pachtontli, melacpil: vel ixqujch, in castillan mjzton, nexeoac, ticeoac, cujcujltic, ocelocujcujltic, molchachapatz>': '&lt;TLACOOCELUTL: [...] yoan qujtocaiotia, tlacomjztli, tepiton, pachtontli, melacpil: vel ixqujch, in castillan mjzton, nexeoac, ticeoac, cujcujltic, ocelocujcujltic, molchachapatz&gt;',\n",
    "        '<Concanauhtli>': '&lt;Concanauhtli&gt;',\n",
    "        '<ue>': '&lt;ue&gt;',\n",
    "        '<Canauhtli>': '&lt;Canauhtli&gt;',\n",
    "        '<ZŌL-IN>': '&lt;ZŌL-IN&gt;',\n",
    "        '<American White Pelican>': '&lt;American White Pelican&gt;',\n",
    "        '<when feeding>': '&lt;when feeding&gt;',\n",
    "        '<Ā-TŌTO-LIN>': '&lt;Ā-TŌTO-LIN&gt;',\n",
    "        '<with synonym atapalcatl>': '&lt;with synonym atapalcatl&gt;',\n",
    "        '<Ā-COYO-TL>': '&lt;Ā-COYO-TL&gt;',\n",
    "        '<with synonyms acoyotl, atotlin>': '&lt;with synonyms acoyotl, atotlin&gt;',\n",
    "        '<n>': '&lt;n&gt;',\n",
    "        '<Wood Stork>': '&lt;Wood Stork&gt;',\n",
    "        '<Tenitztli>': '&lt;Tenitztli&gt;',\n",
    "        '<ĀCAL-LI, \"boat\">': '&lt;ĀCAL-LI, \"boat\"&gt;',\n",
    "        '<XIUH-TŌTŌ-TL>': '&lt;XIUH-TŌTŌ-TL&gt;',\n",
    "        '<Canyon towhee, Ilamatototl>': '&lt;Canyon towhee, Ilamatototl&gt;',\n",
    "        '<Aztec Rail, COHUIX-IN>': '&lt;Aztec Rail, COHUIX-IN&gt;',\n",
    "        '<shorebirds>': '&lt;shorebirds&gt;',\n",
    "        '<tzoniaiauhqui, Lesser Scaup>': '&lt;tzoniaiauhqui, Lesser Scaup&gt;',\n",
    "        '<Amazon>': '&lt;Amazon&gt;',\n",
    "        '<zoquiazolin, literally, \"mud quail\">': '&lt;zoquiazolin, literally, \"mud quail\"&gt;',\n",
    "        '<Slender-billed Grackle>': '&lt;Slender-billed Grackle&gt;',\n",
    "        '<CHĪCUA-TLI>': '&lt;CHĪCUA-TLI&gt;',\n",
    "        '<for CUĀUH-TLOH-TLI>': '&lt;for CUĀUH-TLOH-TLI&gt;',\n",
    "        '<unlike the Peregrine>': '&lt;unlike the Peregrine&gt;',\n",
    "        '<in Spanish, perhaps the Saker Falcon, Falco cherrug, a European species that most closely resembles the Prairie Falcon>': '&lt;in Spanish, perhaps the Saker Falcon, Falco cherrug, a European species that most closely resembles the Prairie Falcon&gt;',\n",
    "        '<House Finch>': '&lt;House Finch&gt;',\n",
    "        '<in Spanish>': '&lt;in Spanish&gt;',\n",
    "        '<Prairie>': '&lt;Prairie&gt;',\n",
    "        '<totolin>': '&lt;totolin&gt;',\n",
    "        '<\"paloma\">': '&lt;\"paloma\"&gt;',\n",
    "        '<boat-shaped>': '&lt;boat-shaped&gt;',\n",
    "        '<Sandhill Crane>': '&lt;Sandhill Crane&gt;',\n",
    "        '<late July>': '&lt;late July&gt;',\n",
    "        '<Common Gallinule, Quachilton>': '&lt;Common Gallinule, Quachilton&gt;',\n",
    "        '<tlhotli>': '&lt;tlhotli&gt;',\n",
    "        '<of rivers>': '&lt;of rivers&gt;',\n",
    "        '<each type of feather named separately>': '&lt;each type of feather named separately&gt;',\n",
    "        '<duck>': '&lt;duck&gt;',\n",
    "        '<Black-bellied Whistling-Duck>': '&lt;Black-bellied Whistling-Duck&gt;',\n",
    "        '<Zoquicanauhtli, literally, \"mud duck>': '&lt;Zoquicanauhtli, literally, \"mud duck\"&gt;',\n",
    "        '<that is, salt water>': '&lt;that is, salt water&gt;',\n",
    "        '<CANAUH-TLI>': '&lt;CANAUH-TLI&gt;',\n",
    "        '<COZCA-CUAUH-TLI, likely the Crested Caracara>': '&lt;COZCA-CUAUH-TLI, likely the Crested Caracara&gt;',\n",
    "        '<tzanatl>': '&lt;tzanatl&gt;',\n",
    "        '<atole>': '&lt;atole&gt;',\n",
    "        '<the Mourning Dove>': '&lt;the Mourning Dove&gt;',\n",
    "        '</tlacoocelutl:>': '&lt;/tlacoocelutl:&gt;',\n",
    "        '<tlacoocelutl: yoan=\"\" qujtocaiotia=\"\" tlacomjztli=\"\" tepiton=\"\" pachtontli=\"\" melacpil:=\"\" vel=\"\" ixqujch=\"\" in=\"\" castillan=\"\" mjzton=\"\" nexeoac=\"\" ticeoac=\"\" cujcujltic=\"\" ocelocujcujltic=\"\" molchachapatz=\"\">': '&lt;tlacoocelutl: yoan=\"\" qujtocaiotia=\"\" tlacomjztli=\"\" tepiton=\"\" pachtontli=\"\" melacpil:=\"\" vel=\"\" ixqujch=\"\" in=\"\" castillan=\"\" mjzton=\"\" nexeoac=\"\" ticeoac=\"\" cujcujltic=\"\" ocelocujcujltic=\"\" molchachapatz=\"\"&gt;',\n",
    "        '<alo- poss- ichcatl -yoa:-l2 tla7>': '&lt;alo- poss- ichcatl -yoa:-l2 tla7&gt;',\n",
    "        '<tlavitequjnj>': '&lt;tlavitequjnj&gt;',\n",
    "        '<toznene>': '&lt;toznene&gt;',\n",
    "        '<which see>': '&lt;which see&gt;',\n",
    "        '<tzoniaiauhquj, which see>': '&lt;tzoniaiauhquj, which see&gt;'\n",
    "    }\n",
    "    \n",
    "    # Create toolkit and apply batch repair\n",
    "    toolkit = create_manual_processing_toolkit(conn)\n",
    "    \n",
    "    # Check what actually needs fixing\n",
    "    validation_results = check_remaining_non_html_tags(conn, nahuatl_tags_2_reformat)\n",
    "    \n",
    "    if validation_results['manual_needed']:\n",
    "        print(f\"   📋 Found {len(validation_results['manual_needed'])} tags needing cleanup\")\n",
    "        \n",
    "        # Only apply repairs for tags that actually exist\n",
    "        needed_repairs = {tag: encoded for tag, encoded in nahuatl_tags_2_reformat.items() \n",
    "                         if tag in validation_results['manual_needed']}\n",
    "        \n",
    "        if needed_repairs:\n",
    "            print(f\"   🔧 Applying {len(needed_repairs)} repairs...\")\n",
    "            final_df = toolkit.batch_repair_tags(needed_repairs, scope='global')\n",
    "            \n",
    "            # Verify cleanup\n",
    "            print(\"   🔍 Verifying cleanup...\")\n",
    "            post_cleanup = check_remaining_non_html_tags(conn, needed_repairs)\n",
    "            print(f\"   ✅ Final success rate: {post_cleanup['automation_rate']:.1f}%\")\n",
    "        else:\n",
    "            print(\"   ✅ All tags already handled!\")\n",
    "            final_df = pd.read_sql(\"SELECT * FROM [08_final_complete_dataset_stage]\", conn)\n",
    "    else:\n",
    "        print(\"   🎉 Perfect! All tags already encoded by automation\")\n",
    "        final_df = pd.read_sql(\"SELECT * FROM [08_final_complete_dataset_stage]\", conn)\n",
    "    \n",
    "    # Save final dataset\n",
    "    save_intermediate_stage_sqlite(final_df, \"09_final_manual_cleanup\", conn)\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"🎉 COMPLETE nahuatLEX PROCESSING PIPELINE FINISHED!\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"📊 Step 5 Automation Success: 80%+ (excellent!)\")\n",
    "    print(f\"🧹 Step 9 Manual Cleanup: Handles remaining edge cases\")\n",
    "    print(f\"💾 Final dataset: 09_final_manual_cleanup_stage\")\n",
    "    print(f\"🚀 Ready for nahuatLEX website integration!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return final_df\n",
    "\n",
    "# ==========================================\n",
    "# USAGE: Complete Step 9 Execution\n",
    "# ==========================================\n",
    "\n",
    "# Run the complete Step 9 cleanup\n",
    "final_dataset = step9_final_non_html_cleanup(conn)\n",
    "\n",
    "# Optional: Create toolkit for any additional manual processing needs\n",
    "toolkit = create_manual_processing_toolkit(conn)\n",
    "\n",
    "# The toolkit is available for any edge cases you discover later:\n",
    "# toolkit.repair_specific_tag(\"problematic_tag\", \"fixed_tag\", scope='global')\n",
    "# toolkit.find_problem_entries(\"search_pattern\")\n",
    "# toolkit.reanalyze_after_changes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b78564",
   "metadata": {},
   "source": [
    "# Step 10: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nahuaLEX_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
