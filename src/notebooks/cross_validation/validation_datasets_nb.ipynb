{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5346fcb6",
   "metadata": {},
   "source": [
    "# Validation and correction\n",
    "\n",
    "As we've continued working with the scraped data versus the changes we've made from the initial csv files given to the LRC. We've done validation and corrections to the initial csv files that are not reflected in the scraped datasets. So now since we've uploaded our data into SQLite the next step will be to cross validate and update the SQLite. Which will be done below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "cef09ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup and imports\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Optional, Union, Any\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af11e03d",
   "metadata": {},
   "source": [
    "note the original local datasets given to the LRC initially are hosted in the nahuatl_processing.db under sqLiteDb specifically under the table name: checkpoint_after_empty_p_tag_removal_20251002. For the scraped data, that is under scrapedDataDb under the nahuatl.db file. The schema is under config/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "8ed35dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data_dir = Path(\"../../../data/scrapedDataDb/\")\n",
    "local_data_dir = Path(\"../../../data/sqLiteDb/\")\n",
    "\n",
    "if not scraped_data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Scraped database not found: {scraped_data_dir}\")\n",
    "if not local_data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Local database not found: {local_data_dir}\")\n",
    "\n",
    "# Database connection\n",
    "scraped_db = sqlite3.connect(scraped_data_dir / \"nahuatl.db\")\n",
    "local_db = sqlite3.connect(local_data_dir / \"nahuatl_processing.db\")\n",
    "\n",
    "\n",
    "# read in the table(s), for the local_db it's only one table (actually two one for the WHP dataset and one for the IDIEZ dataset) while for the \n",
    "# scraped_db there are multiple tables due to the relationl structure we want to keep\n",
    "tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", local_db)\n",
    "WHP_TABLE = \"checkpoint_after_bibl_restoration_20251030\"\n",
    "IDIEZ_TABLE = \"IDIEZ_modern_nahuatl-all-2024-03-27T09-45-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "dcfb3631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map scraped DB fields to local DB fields for WHP data\n",
    "WHP_FIELD_MAPPING = {\n",
    "    # Scraped field: Local field\n",
    "    'node_id': 'Ref',\n",
    "    'headword': 'Headword',\n",
    "    'orthographic_variants': 'Orthographic Variants',\n",
    "    'translation_english': 'Principal English Translation',\n",
    "    'spanish_loanword': 'Spanish Loanword',\n",
    "    # Authority fields (stored in authority_citations table in scraped DB)\n",
    "    'authority_molina': 'Alonso de Molina',\n",
    "    'authority_karttunen': 'Frances Karttunen',\n",
    "    'authority_carochi': 'Horacio Carochi / English',\n",
    "    'authority_olmos': 'Andrés de Olmos',\n",
    "    'authority_lockhart': \"Lockhart’s Nahuatl as Written\",\n",
    "    # Attestations (stored in attestations table in scraped DB)\n",
    "    'attestations_english': 'Attestations from sources in English',\n",
    "    'attestations_spanish': 'Attestations from sources in Spanish',\n",
    "    # Metadata\n",
    "    'themes': 'themes',\n",
    "}\n",
    "\n",
    "# Map scraped DB fields to local DB fields for IDIEZ data\n",
    "IDIEZ_FIELD_MAPPING = {\n",
    "    'node_id': 'Ref',\n",
    "    'headword_idiez': 'tlahtolli',\n",
    "    'translation_english_idiez': 'IDIEZ traduc. inglés',\n",
    "    'definition_nahuatl_idiez': 'IDIEZ def. náhuatl',\n",
    "    'definition_spanish_idiez': 'IDIEZ def. español',\n",
    "    'morfologia_idiez': 'IDIEZ morfología',\n",
    "    'gramatica_idiez': 'IDIEZ gramática',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "34c55cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dataframes(df_scraped, df_local, field_mapping, key_field=\"node_id\"):\n",
    "\n",
    "    scraped_key = field_mapping.get(key_field, key_field)\n",
    "    df_scraped = df_scraped.copy()\n",
    "    df_local = df_local.copy()\n",
    "    df_scraped[key_field] = df_scraped[key_field].astype(str)\n",
    "    df_local[scraped_key] = df_local[scraped_key].astype(str)\n",
    "    df_local[scraped_key] = (\n",
    "        df_local[scraped_key].astype(str).str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    "    )\n",
    "\n",
    "    merged = df_scraped.merge(\n",
    "        df_local,\n",
    "        left_on=key_field,\n",
    "        right_on=scraped_key,\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_scraped\", \"_local\"),\n",
    "    )\n",
    "    print(f\"Rows in scraped DB: {len(df_scraped):,}\")\n",
    "    print(f\"Rows in local DB: {len(df_local):,}\")\n",
    "    print(f\"Rows matched: {len(merged):,}\")\n",
    "\n",
    "    discrepancies = {\n",
    "        \"field_discrepancies\": {},\n",
    "        \"total_discrepancies\": 0,\n",
    "        \"rows_compared\": len(merged),\n",
    "        \"sample_discrepancies\": [],\n",
    "    }\n",
    "\n",
    "    for scraped_field, local_field in field_mapping.items():\n",
    "        if scraped_field == key_field:\n",
    "            continue\n",
    "        if (\n",
    "            scraped_field not in df_scraped.columns\n",
    "            or local_field not in df_local.columns\n",
    "        ):\n",
    "            print(f\"Skipping {scraped_field} (not in both datasets)\")\n",
    "            continue\n",
    "        scraped_col = (\n",
    "            f\"{scraped_field}_scraped\"\n",
    "            if scraped_field in df_local.columns\n",
    "            else scraped_field\n",
    "        )\n",
    "        local_col = (\n",
    "            f\"{local_field}_local\" if local_field in df_scraped.columns else local_field\n",
    "        )\n",
    "\n",
    "        scraped_values = merged[scraped_col].fillna(\"\").astype(str).str.strip()\n",
    "        local_values = merged[local_col].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "        merged[\"diff\"] = ~(\n",
    "            (scraped_values == local_values)\n",
    "            | (merged[scraped_col].isna() & merged[local_col].isna())\n",
    "        )\n",
    "        diff_count = merged[\"diff\"].sum()\n",
    "\n",
    "        if diff_count > 0:\n",
    "            print(f\"{scraped_field:30} {diff_count:>6,} discrepancies\")\n",
    "            discrepancies[\"field_discrepancies\"][scraped_field] = {\n",
    "                \"count\": int(diff_count),\n",
    "                \"local_field\": local_field,\n",
    "                \"sample_rows\": merged[merged[\"diff\"]][key_field].head(10).tolist(),\n",
    "            }\n",
    "            discrepancies[\"total_discrepancies\"] += int(diff_count)\n",
    "\n",
    "            if len(discrepancies[\"sample_discrepancies\"]) < 5:\n",
    "                sample = merged[merged[\"diff\"]].iloc[0]\n",
    "                discrepancies[\"sample_discrepancies\"].append(\n",
    "                    {\n",
    "                        \"node_id\": sample[key_field],\n",
    "                        \"field\": scraped_field,\n",
    "                        \"scraped_value\": str(sample[scraped_col])[:100],\n",
    "                        \"local_value\": str(sample[local_col])[:100],\n",
    "                    }\n",
    "                )\n",
    "        else:\n",
    "            print(f\"{scraped_field:30} all match\")\n",
    "\n",
    "    print(f\"Total discrepancies: {discrepancies['total_discrepancies']:,}\")\n",
    "    return discrepancies\n",
    "\n",
    "\n",
    "def create_update_dataframe(scraped_df, local_df, field_mapping, key_field=\"node_id\"):\n",
    "    \"\"\"Create dataframe showing what needs updating\"\"\"\n",
    "    scraped_key = field_mapping.get(key_field, key_field)\n",
    "\n",
    "    scraped_df = scraped_df.copy()\n",
    "    local_df = local_df.copy()\n",
    "    scraped_df[key_field] = scraped_df[key_field].astype(str)\n",
    "    local_df[scraped_key] = (\n",
    "        local_df[scraped_key].astype(str).str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    "    )\n",
    "\n",
    "    merged = scraped_df.merge(\n",
    "        local_df,\n",
    "        left_on=key_field,\n",
    "        right_on=scraped_key,\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_scraped\", \"_local\"),\n",
    "    )\n",
    "\n",
    "    updates = []\n",
    "\n",
    "    for scraped_field, local_field in field_mapping.items():\n",
    "        if scraped_field == key_field:\n",
    "            continue\n",
    "\n",
    "        if (\n",
    "            scraped_field not in scraped_df.columns\n",
    "            or local_field not in local_df.columns\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        scraped_col = (\n",
    "            f\"{scraped_field}_scraped\"\n",
    "            if scraped_field in local_df.columns\n",
    "            else scraped_field\n",
    "        )\n",
    "        local_col = (\n",
    "            f\"{local_field}_local\" if local_field in scraped_df.columns else local_field\n",
    "        )\n",
    "        scraped_values = merged[scraped_col].fillna(\"\").astype(str).str.strip()\n",
    "        local_values = merged[local_col].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "        diff_mask = ~(\n",
    "            (scraped_values == local_values)\n",
    "            | (merged[scraped_col].isna() & merged[local_col].isna())\n",
    "        )\n",
    "\n",
    "        diff_rows = merged[diff_mask]\n",
    "\n",
    "        for idx, row in diff_rows.iterrows():\n",
    "\n",
    "            current_stripped = (\n",
    "                str(row[scraped_col]).strip() if pd.notna(row[scraped_col]) else \"\"\n",
    "            )\n",
    "            new_stripped = (\n",
    "                str(row[local_col]).strip() if pd.notna(row[local_col]) else \"\"\n",
    "            )\n",
    "\n",
    "            updates.append(\n",
    "                {\n",
    "                    \"node_id\": row[key_field],\n",
    "                    \"field\": scraped_field,\n",
    "                    \"current_value\": current_stripped,\n",
    "                    \"new_value\": new_stripped,\n",
    "                    \"action\": \"UPDATE\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(updates)\n",
    "\n",
    "\n",
    "def apply_updates(updates_df, conn, table_name=\"dictionary_entries\", dry_run=True):\n",
    "\n",
    "    print(f\"APPLYING UPDATES TO {table_name}\")\n",
    "    print(f\"Mode: {'DRY RUN' if dry_run else 'LIVE UPDATES'}\")\n",
    "    stats = {\n",
    "        \"total_updates\": 0,\n",
    "        \"successful_updates\": 0,\n",
    "        \"failed_updates\": 0,\n",
    "        \"updates_by_field\": defaultdict(int),\n",
    "    }\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    grouped = updates_df.groupby(\"node_id\")\n",
    "\n",
    "    for node_id, group in grouped:\n",
    "        try:\n",
    "            set_clauses = []\n",
    "            values = []\n",
    "\n",
    "            for _, row in group.iterrows():\n",
    "                set_clauses.append(f\"{row['field']} = ?\")\n",
    "                values.append(row[\"new_value\"])\n",
    "                stats[\"updates_by_field\"][row[\"field\"]] += 1\n",
    "\n",
    "            values.append(node_id)\n",
    "            sql = f\"UPDATE {table_name} SET {', '.join(set_clauses)} WHERE node_id = ?\"\n",
    "\n",
    "            if not dry_run:\n",
    "                cursor.execute(sql, values)\n",
    "\n",
    "            stats[\"successful_updates\"] += len(group)\n",
    "            stats[\"total_updates\"] += len(group)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating node_id {node_id}: {e}\")\n",
    "            stats[\"failed_updates\"] += len(group)\n",
    "            stats[\"total_updates\"] += len(group)\n",
    "\n",
    "    if not dry_run:\n",
    "        conn.commit()\n",
    "        print(\"Changes committed\")\n",
    "    else:\n",
    "        print(\"Dry run complete - no changes made\")\n",
    "\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Total updates: {stats['total_updates']:,}\")\n",
    "    print(f\"  Successful: {stats['successful_updates']:,}\")\n",
    "    print(f\"  Failed: {stats['failed_updates']:,}\")\n",
    "    print(f\"\\nUpdates by field:\")\n",
    "    for field, count in stats[\"updates_by_field\"].items():\n",
    "        print(f\"  - {field}: {count:,}\")\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "bf76dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_better(text):\n",
    "    \"\"\"More aggressive HTML stripping\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "    text = re.sub(r'\\w+=\"[^\"]*\"', \"\", text)\n",
    "    text = re.sub(r\"\\w+=\\'[^\\']*\\'\", \"\", text)\n",
    "\n",
    "    text = \" \".join(text.split())\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48516e8d",
   "metadata": {},
   "source": [
    "for me it methodically makes sense to go down the tables that nahuat.db has (check config/schema.sql) and then check each one against the WHP_table_name and the IDIEZ_table_name as such let's begin with the largest tables first and build up from the tables. \n",
    "it would also be smart that as we cross validate we proceed to investigate where the cross references columns actually come from\n",
    "also a side note since we've done no manual corrections or fixes to the IDIEZ fields we can begin with validating local IDIEZ with scraped IDIEZ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "a66c4992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6,846 IDIEZ/HYBRID entries from scraped DB\n",
      "Loaded 6,846 IDIEZ entries from local DB\n",
      "Rows in scraped DB: 6,846\n",
      "Rows in local DB: 6,846\n",
      "Rows matched: 6,844\n",
      "headword_idiez                 all match\n",
      "translation_english_idiez          11 discrepancies\n",
      "definition_nahuatl_idiez           24 discrepancies\n",
      "definition_spanish_idiez          425 discrepancies\n",
      "morfologia_idiez               all match\n",
      "gramatica_idiez                all match\n",
      "Total discrepancies: 460\n",
      "\n",
      "Sample IDIEZ discrepancies:\n",
      "\n",
      "1. node_id=187252, field=translation_english_idiez\n",
      "   Scraped: \n",
      "   Local:   to transport rocks.\n",
      "\n",
      "2. node_id=176130, field=definition_nahuatl_idiez\n",
      "   Scraped: ICPATL. tlat. Tlamalintli iloh, iixnezca chipahuac quitequihuah ica quichihchihuah cantelah o ica tl\n",
      "   Local:   ICPATL. tlat. Tlamalintli iloh, iixnezca chipahuac quitequihuah ica quichihchihuah cantelah o ica tl\n",
      "\n",
      "3. node_id=172003, field=definition_spanish_idiez\n",
      "   Scraped: A.1. se enfrìa. “Se enfría la tierra de noche. 2. Se va la luz. “Cuando hace truenos se enfrìa la ti\n",
      "   Local:   \tA.1. se enfrìa. “Se enfría la tierra de noche. 2. Se va la luz. “Cuando hace truenos se enfrìa la t\n"
     ]
    }
   ],
   "source": [
    "scraped_idiez = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        node_id,\n",
    "        headword_idiez,\n",
    "        translation_english_idiez,\n",
    "        definition_nahuatl_idiez,\n",
    "        definition_spanish_idiez,\n",
    "        morfologia_idiez,\n",
    "        gramatica_idiez,\n",
    "        source_dataset\n",
    "    FROM dictionary_entries\n",
    "    WHERE source_dataset IN ('IDIEZ', 'HYBRID')\n",
    "\"\"\",\n",
    "    scraped_db,\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(scraped_idiez):,} IDIEZ/HYBRID entries from scraped DB\")\n",
    "\n",
    "# Load IDIEZ from local DB\n",
    "local_idiez = pd.read_sql(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        Ref,\n",
    "        tlahtolli,\n",
    "        \"IDIEZ traduc. inglés\",\n",
    "        \"IDIEZ def. náhuatl\",\n",
    "        \"IDIEZ def. español\",\n",
    "        \"IDIEZ morfología\",\n",
    "        \"IDIEZ gramática\"\n",
    "    FROM [{IDIEZ_TABLE}]\n",
    "\"\"\",\n",
    "    local_db,\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(local_idiez):,} IDIEZ entries from local DB\")\n",
    "\n",
    "# Compare\n",
    "idiez_discrepancies = compare_dataframes(\n",
    "    scraped_idiez, local_idiez, IDIEZ_FIELD_MAPPING, key_field=\"node_id\"\n",
    ")\n",
    "\n",
    "# Show samples\n",
    "if idiez_discrepancies[\"sample_discrepancies\"]:\n",
    "    print(\"\\nSample IDIEZ discrepancies:\")\n",
    "    for i, sample in enumerate(idiez_discrepancies[\"sample_discrepancies\"][:5], 1):\n",
    "        print(f\"\\n{i}. node_id={sample['node_id']}, field={sample['field']}\")\n",
    "        print(f\"   Scraped: {sample['scraped_value']}\")\n",
    "        print(f\"   Local:   {sample['local_value']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "6d5a8706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating IDIEZ update report...\n",
      "Total IDIEZ updates needed: 460\n",
      "\n",
      "Updates by field:\n",
      "field\n",
      "definition_spanish_idiez     425\n",
      "definition_nahuatl_idiez      24\n",
      "translation_english_idiez     11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Update report saved to: idiez_updates_needed.csv\n"
     ]
    }
   ],
   "source": [
    "# creating the IDIEZ report\n",
    "if idiez_discrepancies[\"total_discrepancies\"] > 0:\n",
    "    print(\"\\nCreating IDIEZ update report...\")\n",
    "\n",
    "    idiez_updates = create_update_dataframe(\n",
    "        scraped_idiez, local_idiez, IDIEZ_FIELD_MAPPING\n",
    "    )\n",
    "\n",
    "    print(f\"Total IDIEZ updates needed: {len(idiez_updates):,}\")\n",
    "    print(\"\\nUpdates by field:\")\n",
    "    print(idiez_updates[\"field\"].value_counts())\n",
    "\n",
    "    # idiez_updates.to_csv('idiez_updates_needed.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"\\nUpdate report saved to: idiez_updates_needed.csv\")\n",
    "else:\n",
    "    print(\"\\nNo IDIEZ updates needed - data matches perfectly!\")\n",
    "    idiez_updates = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "6a7dbd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 31,742 WHP entries from scraped DB\n",
      "Loaded 31,806 WHP entries from local DB\n",
      "COMPARING WHP DATA\n",
      "Rows in scraped DB: 31,742\n",
      "Rows in local DB: 31,806\n",
      "Rows matched: 31,466\n",
      "headword                           76 discrepancies\n",
      "orthographic_variants             187 discrepancies\n",
      "translation_english            31,466 discrepancies\n",
      "spanish_loanword               31,466 discrepancies\n",
      "Total discrepancies: 63,195\n",
      "\n",
      "Sample WHP discrepancies:\n",
      "\n",
      "1. node_id=172881, field=headword\n",
      "   Scraped: -icxitlan.\n",
      "   Local:   -icxtlan.\n",
      "\n",
      "2. node_id=171987, field=orthographic_variants\n",
      "   Scraped: canah\n",
      "   Local:   None\n",
      "\n",
      "3. node_id=171879, field=translation_english\n",
      "   Scraped: <div class=\"field-item even\"><p>perhaps not (adverb) (see Molina)</p>\n",
      "</div>\n",
      "   Local:   <p>perhaps not (adverb) (see Molina)</p> \n",
      "\n",
      "4. node_id=171879, field=spanish_loanword\n",
      "   Scraped: \n",
      "   Local:   No\n",
      "WHP COMPARISON COMPLETE\n",
      "Total discrepancies found: 63,195\n",
      "Fields with discrepancies: 4\n"
     ]
    }
   ],
   "source": [
    "scraped_whp = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        node_id,\n",
    "        headword,\n",
    "        orthographic_variants,\n",
    "        translation_english,\n",
    "        spanish_loanword,\n",
    "        source_dataset\n",
    "    FROM dictionary_entries\n",
    "    WHERE source_dataset = 'WHP'\n",
    "\"\"\",\n",
    "    scraped_db,\n",
    ")\n",
    "print(f\"Loaded {len(scraped_whp):,} WHP entries from scraped DB\")\n",
    "\n",
    "local_whp = pd.read_sql(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        Ref,\n",
    "        Headword,\n",
    "        \"Orthographic Variants\",\n",
    "        \"Principal English Translation\",\n",
    "        \"Attestations from sources in English\",\n",
    "        \"Attestations from sources in Spanish\",\n",
    "        \"Alonso de Molina\",\n",
    "        \"Frances Karttunen\",\n",
    "        \"Horacio Carochi / English\",\n",
    "        \"Andrés de Olmos\",\n",
    "        \"Lockhart’s Nahuatl as Written\",\n",
    "        \"themes\",\n",
    "        \"Spanish Loanword\",\n",
    "        \"Citations\",\n",
    "        \"Number_of_Citations\",\n",
    "        \"Cross_References\",\n",
    "        \"Number_of_Cross_References\",\n",
    "        \"CrossRef_Types\"\n",
    "    FROM [{WHP_TABLE}]\n",
    "\"\"\",\n",
    "    local_db,\n",
    ")\n",
    "print(f\"Loaded {len(local_whp):,} WHP entries from local DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a16b28",
   "metadata": {},
   "source": [
    "so as u can see from the output some of the changes seen are html differences so we need to look at the actual content of the cells to see if there is any difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "ef75e482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entries in LOCAL but NOT in SCRAPED: 340\n",
      "Entries in SCRAPED but NOT in LOCAL: 276\n",
      "\n",
      "Saved 340 entries to whp_missing_in_scraped.csv\n",
      "After filtering: 276 rows\n",
      "Saved 276 entries to whp_missing_in_local.csv\n",
      "\n",
      "Sample entries:\n",
      "       node_id       headword\n",
      "31466   211071   yecaxochitl.\n",
      "31467   211072     texochitl.\n",
      "31468   211073        teicui.\n",
      "31469   211074  Itzcahuatzin.\n",
      "31470   211075  Itzehecatzin.\n",
      "31471   211076   Tlamatzinco.\n",
      "31472   211077     amo ihual.\n",
      "31473   211078    Techahuatl.\n",
      "31474   211079     hualiloti.\n",
      "31475   211080     Cualaztli.\n"
     ]
    }
   ],
   "source": [
    "# Prepare node_id sets for comparison\n",
    "scraped_ids = set(scraped_whp[\"node_id\"].astype(str))\n",
    "local_ids = set(\n",
    "    local_whp[\"Ref\"].astype(str).str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "# Find missing entries\n",
    "missing_in_scraped = local_ids - scraped_ids\n",
    "missing_in_local = scraped_ids - local_ids\n",
    "\n",
    "print(f\"\\nEntries in LOCAL but NOT in SCRAPED: {len(missing_in_scraped):,}\")\n",
    "print(f\"Entries in SCRAPED but NOT in LOCAL: {len(missing_in_local):,}\")\n",
    "\n",
    "if missing_in_scraped:\n",
    "    missing_scraped_df = local_whp[\n",
    "        local_whp[\"Ref\"]\n",
    "        .astype(str)\n",
    "        .str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    "        .isin(missing_in_scraped)\n",
    "    ]\n",
    "    missing_scraped_df.to_csv('./whp_missing_in_scraped.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nSaved {len(missing_scraped_df):,} entries to whp_missing_in_scraped.csv\")\n",
    "    \n",
    "if missing_in_local:\n",
    "    # Ensure types match\n",
    "    missing_local_df = scraped_whp[\n",
    "        scraped_whp[\"node_id\"].astype(str).isin(missing_in_local)\n",
    "    ]\n",
    "    print(f\"After filtering: {len(missing_local_df)} rows\")\n",
    "\n",
    "    if len(missing_local_df) > 0:\n",
    "        missing_local_df.to_csv('./whp_missing_in_local.csv', index=False, encoding='utf-8-sig')\n",
    "        print(f\"Saved {len(missing_local_df):,} entries to whp_missing_in_local.csv\")\n",
    "        print(f\"\\nSample entries:\")\n",
    "        print(missing_local_df[[\"node_id\", \"headword\"]].head(10))\n",
    "    else:\n",
    "        print(\"No matching rows found - type mismatch issue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "3ddd3ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation mismatches: 1403\n",
      "Authority citation mismatches: 1262\n",
      "  - Punctuation-only differences: 0\n",
      "  - Other differences: 1262\n",
      "Attestation mismatches: 2265\n",
      "  - Punctuation-only differences: 0\n",
      "  - Other differences: 2265\n",
      "\n",
      "Reports generated:\n",
      "  - report_translation_mismatches.csv\n",
      "  - report_authority_mismatches.csv\n",
      "  - report_attestation_mismatches.csv\n"
     ]
    }
   ],
   "source": [
    "# Helper functions\n",
    "def get_authority_citations(node_id, db_conn):\n",
    "    \"\"\"Get all authority citations for a node, grouped by authority\"\"\"\n",
    "    auth_data = pd.read_sql(\n",
    "        f\"\"\"\n",
    "        SELECT authority_name, citation_text, citation_order\n",
    "        FROM authority_citations\n",
    "        WHERE node_id = '{node_id}'\n",
    "        ORDER BY authority_name, citation_order\n",
    "        \"\"\",\n",
    "        db_conn,\n",
    "    )\n",
    "\n",
    "    result = {}\n",
    "    for auth_name in [\"Molina\", \"Karttunen\", \"Carochi\", \"Olmos\", \"Lockhart\"]:\n",
    "        auth_rows = auth_data[auth_data[\"authority_name\"] == auth_name]\n",
    "        if not auth_rows.empty:\n",
    "            result[auth_name] = \" | \".join(auth_rows[\"citation_text\"].tolist())\n",
    "        else:\n",
    "            result[auth_name] = None\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_attestations(node_id, db_conn):\n",
    "    \"\"\"Get attestations for a node, grouped by language\"\"\"\n",
    "    attest_data = pd.read_sql(\n",
    "        f\"\"\"\n",
    "        SELECT language, attestation_text\n",
    "        FROM attestations\n",
    "        WHERE node_id = '{node_id}'\n",
    "        \"\"\",\n",
    "        db_conn,\n",
    "    )\n",
    "\n",
    "    result = {\"English\": None, \"Spanish\": None}\n",
    "\n",
    "    for _, row in attest_data.iterrows():\n",
    "        lang = row[\"language\"]\n",
    "        if lang in result:\n",
    "            if result[lang] is None:\n",
    "                result[lang] = row[\"attestation_text\"]\n",
    "            else:\n",
    "                result[lang] += \" | \" + row[\"attestation_text\"]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_cross_references(node_id, db_conn):\n",
    "    \"\"\"Get cross-references for a node\"\"\"\n",
    "    xref_data = pd.read_sql(\n",
    "        f\"\"\"\n",
    "        SELECT target_node_id, reference_type\n",
    "        FROM entry_cross_references\n",
    "        WHERE source_node_id = '{node_id}'\n",
    "        \"\"\",\n",
    "        db_conn,\n",
    "    )\n",
    "\n",
    "    if xref_data.empty:\n",
    "        return None\n",
    "\n",
    "    return \" | \".join(xref_data[\"target_node_id\"].astype(str).tolist())\n",
    "\n",
    "\n",
    "def strip_div_br_tags(text):\n",
    "    \"\"\"Strip div and br tags but keep everything else intact\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    # Remove div tags with any attributes\n",
    "    text = re.sub(r\"<div[^>]*>\", \"\", text)\n",
    "    text = re.sub(r\"</div>\", \"\", text)\n",
    "\n",
    "    # Remove br tags\n",
    "    text = re.sub(r\"<br\\s*/?>\", \"\", text)\n",
    "\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Remove whitespace before closing tags and after opening tags\n",
    "    text = re.sub(r\"\\s+</\", \"</\", text)\n",
    "    text = re.sub(r\">\\s+\", \">\", text)\n",
    "\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def strip_punctuation_for_comparison(text):\n",
    "    \"\"\"Remove common punctuation for comparison\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    # Remove common punctuation\n",
    "    text = re.sub(r\"[.,;:!?]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# Load base scraped data\n",
    "scraped_enriched = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        de.node_id,\n",
    "        de.headword,\n",
    "        de.orthographic_variants,\n",
    "        de.translation_english,\n",
    "        de.spanish_loanword,\n",
    "        de.source_dataset,\n",
    "        de.url_alias\n",
    "    FROM dictionary_entries de\n",
    "    WHERE de.source_dataset = 'WHP'\n",
    "    \"\"\",\n",
    "    scraped_db,\n",
    ")\n",
    "\n",
    "# Enrich with authority citations, attestations, and cross-references\n",
    "auth_cols = [\n",
    "    \"scraped_molina\",\n",
    "    \"scraped_karttunen\",\n",
    "    \"scraped_carochi\",\n",
    "    \"scraped_olmos\",\n",
    "    \"scraped_lockhart\",\n",
    "]\n",
    "attest_cols = [\"scraped_attest_english\", \"scraped_attest_spanish\"]\n",
    "\n",
    "for col in auth_cols + attest_cols + [\"scraped_crossrefs\"]:\n",
    "    scraped_enriched[col] = None\n",
    "\n",
    "for idx, row in scraped_enriched.iterrows():\n",
    "    node_id = row[\"node_id\"]\n",
    "\n",
    "    auth_cites = get_authority_citations(node_id, scraped_db)\n",
    "    scraped_enriched.at[idx, \"scraped_molina\"] = auth_cites[\"Molina\"] #type: ignore\n",
    "    scraped_enriched.at[idx, \"scraped_karttunen\"] = auth_cites[\"Karttunen\"] #type: ignore\n",
    "    scraped_enriched.at[idx, \"scraped_carochi\"] = auth_cites[\"Carochi\"] #type: ignore\n",
    "    scraped_enriched.at[idx, \"scraped_olmos\"] = auth_cites[\"Olmos\"] #type: ignore\n",
    "    scraped_enriched.at[idx, \"scraped_lockhart\"] = auth_cites[\"Lockhart\"] #type: ignore\n",
    "\n",
    "    attests = get_attestations(node_id, scraped_db)\n",
    "    scraped_enriched.at[idx, \"scraped_attest_english\"] = attests[\"English\"] #type: ignore\n",
    "    scraped_enriched.at[idx, \"scraped_attest_spanish\"] = attests[\"Spanish\"] #type: ignore\n",
    "\n",
    "    xrefs = get_cross_references(node_id, scraped_db)\n",
    "    scraped_enriched.at[idx, \"scraped_crossrefs\"] = xrefs #type: ignore\n",
    "WHP_TABLE_PUNCT_FIXED = \"checkpoint_punctuation_only_diffs_fixed_20251030\"\n",
    "\n",
    "# Load local data with all columns\n",
    "local_enriched = pd.read_sql(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        Ref,\n",
    "        Headword,\n",
    "        \"Orthographic Variants\",\n",
    "        \"Principal English Translation\",\n",
    "        \"Spanish Loanword\",\n",
    "        \"Attestations from sources in English\",\n",
    "        \"Attestations from sources in Spanish\",\n",
    "        \"Alonso de Molina\",\n",
    "        \"Frances Karttunen\",\n",
    "        \"Horacio Carochi / English\",\n",
    "        \"Andrés de Olmos\",\n",
    "        \"Lockhart’s Nahuatl as Written\",\n",
    "        \"Citations\",\n",
    "        \"Cross_References\"\n",
    "    FROM [{WHP_TABLE_PUNCT_FIXED}]\n",
    "    \"\"\",\n",
    "    local_db,\n",
    ")\n",
    "\n",
    "# Prepare for merge\n",
    "scraped_enriched[\"node_id\"] = scraped_enriched[\"node_id\"].astype(str)\n",
    "local_enriched[\"Ref\"] = (\n",
    "    local_enriched[\"Ref\"].astype(str).str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "# Merge datasets\n",
    "merged = scraped_enriched.merge(\n",
    "    local_enriched,\n",
    "    left_on=\"node_id\",\n",
    "    right_on=\"Ref\",\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_scraped\", \"_local\"),\n",
    ")\n",
    "\n",
    "# Apply cleaning to both datasets\n",
    "merged[\"scraped_translation_clean\"] = merged[\"translation_english\"].apply(\n",
    "    strip_div_br_tags\n",
    ")\n",
    "merged[\"local_translation_clean\"] = merged[\"Principal English Translation\"].apply(\n",
    "    strip_div_br_tags\n",
    ")\n",
    "\n",
    "merged[\"scraped_molina_clean\"] = merged[\"scraped_molina\"].apply(strip_div_br_tags)\n",
    "merged[\"local_molina_clean\"] = merged[\"Alonso de Molina\"].apply(strip_div_br_tags)\n",
    "\n",
    "merged[\"scraped_karttunen_clean\"] = merged[\"scraped_karttunen\"].apply(strip_div_br_tags)\n",
    "merged[\"local_karttunen_clean\"] = merged[\"Frances Karttunen\"].apply(strip_div_br_tags)\n",
    "\n",
    "merged[\"scraped_carochi_clean\"] = merged[\"scraped_carochi\"].apply(strip_div_br_tags)\n",
    "merged[\"local_carochi_clean\"] = merged[\"Horacio Carochi / English\"].apply(\n",
    "    strip_div_br_tags\n",
    ")\n",
    "\n",
    "merged[\"scraped_olmos_clean\"] = merged[\"scraped_olmos\"].apply(strip_div_br_tags)\n",
    "merged[\"local_olmos_clean\"] = merged[\"Andrés de Olmos\"].apply(strip_div_br_tags)\n",
    "\n",
    "merged[\"scraped_lockhart_clean\"] = merged[\"scraped_lockhart\"].apply(strip_div_br_tags)\n",
    "merged[\"local_lockhart_clean\"] = merged[\"Lockhart’s Nahuatl as Written\"].apply(\n",
    "    strip_div_br_tags\n",
    ")\n",
    "\n",
    "merged[\"scraped_attest_english_clean\"] = merged[\"scraped_attest_english\"].apply(\n",
    "    strip_div_br_tags\n",
    ")\n",
    "merged[\"local_attest_english_clean\"] = merged[\n",
    "    \"Attestations from sources in English\"\n",
    "].apply(strip_div_br_tags)\n",
    "\n",
    "merged[\"scraped_attest_spanish_clean\"] = merged[\"scraped_attest_spanish\"].apply(\n",
    "    strip_div_br_tags\n",
    ")\n",
    "merged[\"local_attest_spanish_clean\"] = merged[\n",
    "    \"Attestations from sources in Spanish\"\n",
    "].apply(strip_div_br_tags)\n",
    "\n",
    "# Report 1: Translation differences\n",
    "translation_diff = merged[\n",
    "    (merged[\"scraped_translation_clean\"] != merged[\"local_translation_clean\"])\n",
    "    & (merged[\"local_translation_clean\"] != \"None\")\n",
    "].copy()\n",
    "\n",
    "translation_report = translation_diff[\n",
    "    [\n",
    "        \"node_id\",\n",
    "        \"headword\",\n",
    "        \"orthographic_variants\",\n",
    "        \"url_alias\",\n",
    "        \"scraped_translation_clean\",\n",
    "        \"local_translation_clean\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "translation_report.to_csv(\n",
    "    \"./report_translation_mismatches.csv\", index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"Translation mismatches: {len(translation_report)}\")\n",
    "\n",
    "# Report 2: Authority citation differences\n",
    "authority_diffs = []\n",
    "\n",
    "for idx, row in merged.iterrows():\n",
    "    authorities = [\n",
    "        (\"Molina\", \"scraped_molina_clean\", \"local_molina_clean\"),\n",
    "        (\"Karttunen\", \"scraped_karttunen_clean\", \"local_karttunen_clean\"),\n",
    "        (\"Carochi\", \"scraped_carochi_clean\", \"local_carochi_clean\"),\n",
    "        (\"Olmos\", \"scraped_olmos_clean\", \"local_olmos_clean\"),\n",
    "        (\"Lockhart\", \"scraped_lockhart_clean\", \"local_lockhart_clean\"),\n",
    "    ]\n",
    "\n",
    "    for auth_name, scraped_col, local_col in authorities:\n",
    "        if row[scraped_col] != row[local_col] and str(row[local_col]).strip() != \"None\":\n",
    "\n",
    "            # Check if only difference is punctuation\n",
    "            scraped_no_punct = strip_punctuation_for_comparison(row[scraped_col])\n",
    "            local_no_punct = strip_punctuation_for_comparison(row[local_col])\n",
    "\n",
    "            is_punctuation_only = scraped_no_punct == local_no_punct\n",
    "\n",
    "            authority_diffs.append(\n",
    "                {\n",
    "                    \"node_id\": row[\"node_id\"],\n",
    "                    \"headword\": row[\"headword\"],\n",
    "                    \"url_alias\": row[\"url_alias\"],\n",
    "                    \"authority\": auth_name,\n",
    "                    \"scraped_value\": row[scraped_col],\n",
    "                    \"local_value\": row[local_col],\n",
    "                    \"punctuation_only_diff\": is_punctuation_only,\n",
    "                    \"recommended_action\": (\n",
    "                        \"use_scraped\" if is_punctuation_only else \"manual_review\"\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "if authority_diffs:\n",
    "    authority_report = pd.DataFrame(authority_diffs)\n",
    "    authority_report.to_csv(\n",
    "        \"./report_authority_mismatches.csv\", index=False, encoding=\"utf-8-sig\"\n",
    "    )\n",
    "\n",
    "    punct_only_count = authority_report[\"punctuation_only_diff\"].sum()\n",
    "    print(f\"Authority citation mismatches: {len(authority_report)}\")\n",
    "    print(f\"  - Punctuation-only differences: {punct_only_count}\")\n",
    "    print(f\"  - Other differences: {len(authority_report) - punct_only_count}\")\n",
    "else:\n",
    "    print(\"Authority citation mismatches: 0\")\n",
    "\n",
    "# Report 3: Attestation differences\n",
    "attestation_diffs = []\n",
    "\n",
    "for idx, row in merged.iterrows():\n",
    "    attestations = [\n",
    "        (\"English\", \"scraped_attest_english_clean\", \"local_attest_english_clean\"),\n",
    "        (\"Spanish\", \"scraped_attest_spanish_clean\", \"local_attest_spanish_clean\"),\n",
    "    ]\n",
    "\n",
    "    for lang, scraped_col, local_col in attestations:\n",
    "        if row[scraped_col] != row[local_col] and str(row[local_col]).strip() != \"None\":\n",
    "\n",
    "            # Check if only difference is punctuation\n",
    "            scraped_no_punct = strip_punctuation_for_comparison(row[scraped_col])\n",
    "            local_no_punct = strip_punctuation_for_comparison(row[local_col])\n",
    "\n",
    "            is_punctuation_only = scraped_no_punct == local_no_punct\n",
    "\n",
    "            attestation_diffs.append(\n",
    "                {\n",
    "                    \"node_id\": row[\"node_id\"],\n",
    "                    \"headword\": row[\"headword\"],\n",
    "                    \"url_alias\": row[\"url_alias\"],\n",
    "                    \"language\": lang,\n",
    "                    \"scraped_value\": row[scraped_col],\n",
    "                    \"local_value\": row[local_col],\n",
    "                    \"punctuation_only_diff\": is_punctuation_only,\n",
    "                    \"recommended_action\": (\n",
    "                        \"use_scraped\" if is_punctuation_only else \"manual_review\"\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "if attestation_diffs:\n",
    "    attestation_report = pd.DataFrame(attestation_diffs)\n",
    "    attestation_report.to_csv(\n",
    "        \"./report_attestation_mismatches.csv\", index=False, encoding=\"utf-8-sig\"\n",
    "    )\n",
    "\n",
    "    punct_only_count = attestation_report[\"punctuation_only_diff\"].sum()\n",
    "    print(f\"Attestation mismatches: {len(attestation_report)}\")\n",
    "    print(f\"  - Punctuation-only differences: {punct_only_count}\")\n",
    "    print(f\"  - Other differences: {len(attestation_report) - punct_only_count}\")\n",
    "else:\n",
    "    print(\"Attestation mismatches: 0\")\n",
    "\n",
    "print(\"\\nReports generated:\")\n",
    "print(\"  - report_translation_mismatches.csv\")\n",
    "print(\"  - report_authority_mismatches.csv\")\n",
    "print(\"  - report_attestation_mismatches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "8be18d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "BIBL TAG RESTORATION REPORT\n",
      "================================================================================\n",
      "Total entries updated: 22901\n",
      "\n",
      "New checkpoint created: checkpoint_after_bibl_restoration_20251030\n",
      "Original checkpoint preserved: checkpoint_after_citation_crossref_reinsertion_20251030\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "SAMPLE UPDATES (first 10):\n",
      "--------------------------------------------------------------------------------\n",
      "1. Ref: WHP-171879, Headword: acazomo.\n",
      "   Action: Restored bibl tags from raw\n",
      "\n",
      "2. Ref: WHP-171881, Headword: ayac.\n",
      "   Action: Restored bibl tags from raw\n",
      "\n",
      "3. Ref: WHP-171882, Headword: acan.\n",
      "   Action: Restored bibl tags from raw\n",
      "\n",
      "4. Ref: WHP-171883, Headword: acampa.\n",
      "   Action: Restored bibl tags from raw\n",
      "\n",
      "5. Ref: WHP-171884, Headword: acatto.\n",
      "   Action: Restored bibl tags from raw\n",
      "\n",
      "6. Ref: WHP-171885, Headword: achi.\n",
      "   Action: Restored bibl tags from raw\n",
      "\n",
      "7. Ref: WHP-171886, Headword: achitzin.\n",
      "   Action: Restored bibl tags from raw\n",
      "\n",
      "8. Ref: WHP-171887, Headword: achic.\n",
      "   Action: Restored bibl tags from raw\n",
      "\n",
      "9. Ref: WHP-171888, Headword: achica.\n",
      "   Action: Restored bibl tags from raw\n",
      "\n",
      "10. Ref: WHP-171889, Headword: achicahuitl.\n",
      "   Action: Restored bibl tags from raw\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def strip_bibl_br_for_comparison(text):\n",
    "    \"\"\"Strip bibl and br tags, normalize all whitespace including between tags\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    \n",
    "    # Remove bibl tags and their content\n",
    "    text = re.sub(r'<bibl[^>]*>.*?</bibl>', '', text, flags=re.DOTALL)\n",
    "    \n",
    "    # Remove br tags\n",
    "    text = re.sub(r'<br\\s*/?>', '', text)\n",
    "    \n",
    "    # Remove empty tags (like <p></p> or <p> </p>)\n",
    "    text = re.sub(r'<p>\\s*</p>', '', text)\n",
    "    text = re.sub(r'<div[^>]*>\\s*</div>', '', text)\n",
    "    \n",
    "    # Normalize ALL whitespace including between tags\n",
    "    text = re.sub(r'>\\s+<', '><', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    \n",
    "    return text\n",
    "\n",
    "WHP_TABLE = \"checkpoint_after_citation_crossref_reinsertion_20251030\"\n",
    "# Load current checkpoint and raw data\n",
    "current_checkpoint = pd.read_sql(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        Ref,\n",
    "        Headword,\n",
    "        \"Principal English Translation\"\n",
    "    FROM [{WHP_TABLE}]\n",
    "    \"\"\",\n",
    "    local_db\n",
    ")\n",
    "\n",
    "raw_data = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        Ref,\n",
    "        \"Principal English Translation\" as raw_translation\n",
    "    FROM [WHP_EarlyNahuatl_Data]\n",
    "    \"\"\",\n",
    "    local_db\n",
    ")\n",
    "\n",
    "# Merge datasets\n",
    "merged_compare = current_checkpoint.merge(raw_data, on='Ref', how='left')\n",
    "\n",
    "# Track updates\n",
    "updates_log = []\n",
    "\n",
    "# Compare and identify entries to restore\n",
    "for idx, row in merged_compare.iterrows():\n",
    "    ref = row['Ref']\n",
    "    \n",
    "    if pd.notna(row['raw_translation']) and pd.notna(row['Principal English Translation']):\n",
    "        raw_stripped = strip_bibl_br_for_comparison(row['raw_translation'])\n",
    "        current_stripped = strip_bibl_br_for_comparison(row['Principal English Translation'])\n",
    "        \n",
    "        if raw_stripped == current_stripped and row['raw_translation'] != row['Principal English Translation']:\n",
    "            merged_compare.at[idx, 'Principal English Translation'] = row['raw_translation'] #type: ignore\n",
    "            updates_log.append({\n",
    "                'ref': ref,\n",
    "                'headword': row['Headword'],\n",
    "                'action': 'Restored bibl tags from raw'\n",
    "            })\n",
    "\n",
    "# Load full current checkpoint for update\n",
    "full_checkpoint = pd.read_sql(f\"SELECT * FROM [{WHP_TABLE}]\", local_db)\n",
    "\n",
    "# Save original column order\n",
    "original_columns = full_checkpoint.columns.tolist()\n",
    "\n",
    "# Update the Principal English Translation column while preserving order\n",
    "full_checkpoint['Principal English Translation'] = full_checkpoint['Ref'].map(\n",
    "    merged_compare.set_index('Ref')['Principal English Translation']\n",
    ")\n",
    "\n",
    "# Ensure column order is preserved\n",
    "full_checkpoint = full_checkpoint[original_columns]\n",
    "\n",
    "# Create new checkpoint\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "new_table_name = f\"checkpoint_after_bibl_restoration_{timestamp}\"\n",
    "\n",
    "full_checkpoint.to_sql(new_table_name, local_db, if_exists='replace', index=False)\n",
    "\n",
    "# Display report\n",
    "print(\"=\" * 80)\n",
    "print(\"BIBL TAG RESTORATION REPORT\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total entries updated: {len(updates_log)}\")\n",
    "print(f\"\\nNew checkpoint created: {new_table_name}\")\n",
    "print(f\"Original checkpoint preserved: {WHP_TABLE}\")\n",
    "\n",
    "if updates_log:\n",
    "    print(\"\\n\" + \"-\" * 80)\n",
    "    print(\"SAMPLE UPDATES (first 10):\")\n",
    "    print(\"-\" * 80)\n",
    "    for i, update in enumerate(updates_log[:10], 1):\n",
    "        print(f\"{i}. Ref: {update['ref']}, Headword: {update['headword']}\")\n",
    "        print(f\"   Action: {update['action']}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"\\nNo updates needed - all bibl tags already present\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "7e433994",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 19662 authority citation entries\n",
      "Fixed 934 attestation entries\n",
      "\n",
      "================================================================================\n",
      "PUNCTUATION-ONLY FIXES APPLIED\n",
      "================================================================================\n",
      "Total entries fixed: 20596\n",
      "  - Authority citations: 19662\n",
      "  - Attestations: 934\n",
      "\n",
      "New checkpoint created: checkpoint_punctuation_only_diffs_fixed_20251030\n",
      "Original checkpoint preserved: checkpoint_after_bibl_restoration_20251030\n",
      "================================================================================\n",
      "\n",
      "Generating focused review reports...\n",
      "  - review_authority_manual.csv: 1262 entries\n",
      "  - review_attestation_manual.csv: 2265 entries\n",
      "  - review_translation_manual.csv: 1403 entries\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "Auto-fixed: 20596 entries\n",
      "Manual review needed: 4930 entries\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Auto-fix punctuation-only differences (FAST VERSION)\n",
    "\n",
    "# Load full checkpoint\n",
    "full_checkpoint = pd.read_sql(f\"SELECT * FROM [{WHP_TABLE}]\", local_db)\n",
    "\n",
    "# Normalize Ref for matching\n",
    "full_checkpoint['Ref_clean'] = full_checkpoint['Ref'].astype(str).str.replace(r'^(WHP-|IDIEZ-)', '', regex=True)\n",
    "\n",
    "# Get the authority and attestation reports\n",
    "authority_report = pd.DataFrame(authority_diffs)\n",
    "attestation_report = pd.DataFrame(attestation_diffs)\n",
    "\n",
    "# Track updates\n",
    "punctuation_fixes = {\n",
    "    'authority_citations': 0,\n",
    "    'attestations': 0,\n",
    "    'total': 0\n",
    "}\n",
    "\n",
    "# Fix authority citations (punctuation-only) - VECTORIZED\n",
    "authority_punct_only = authority_report[authority_report['punctuation_only_diff'] == True].copy()\n",
    "\n",
    "authority_column_map = {\n",
    "    'Molina': 'Alonso de Molina',\n",
    "    'Karttunen': 'Frances Karttunen',\n",
    "    'Carochi': 'Horacio Carochi / English',\n",
    "    'Olmos': 'Andrés de Olmos',\n",
    "    'Lockhart': 'Lockhart’s Nahuatl as Written'\n",
    "}\n",
    "\n",
    "for authority, local_column in authority_column_map.items():\n",
    "    auth_fixes = authority_punct_only[authority_punct_only['authority'] == authority]\n",
    "    if len(auth_fixes) > 0:\n",
    "        update_dict = dict(zip(auth_fixes['node_id'].astype(str), auth_fixes['scraped_value']))\n",
    "        full_checkpoint.loc[full_checkpoint['Ref_clean'].isin(update_dict.keys()), local_column] = \\\n",
    "            full_checkpoint.loc[full_checkpoint['Ref_clean'].isin(update_dict.keys()), 'Ref_clean'].map(update_dict)\n",
    "        punctuation_fixes['authority_citations'] += len(auth_fixes)\n",
    "\n",
    "print(f\"Fixed {punctuation_fixes['authority_citations']} authority citation entries\")\n",
    "\n",
    "# Fix attestations (punctuation-only) - VECTORIZED\n",
    "attestation_punct_only = attestation_report[attestation_report['punctuation_only_diff'] == True].copy()\n",
    "\n",
    "for language in ['English', 'Spanish']:\n",
    "    local_column = f'Attestations from sources in {language}'\n",
    "    attest_fixes = attestation_punct_only[attestation_punct_only['language'] == language]\n",
    "    \n",
    "    if len(attest_fixes) > 0:\n",
    "        update_dict = dict(zip(attest_fixes['node_id'].astype(str), attest_fixes['scraped_value']))\n",
    "        full_checkpoint.loc[full_checkpoint['Ref_clean'].isin(update_dict.keys()), local_column] = \\\n",
    "            full_checkpoint.loc[full_checkpoint['Ref_clean'].isin(update_dict.keys()), 'Ref_clean'].map(update_dict)\n",
    "        punctuation_fixes['attestations'] += len(attest_fixes)\n",
    "\n",
    "print(f\"Fixed {punctuation_fixes['attestations']} attestation entries\")\n",
    "\n",
    "# Drop helper column\n",
    "full_checkpoint = full_checkpoint.drop(columns=['Ref_clean'])\n",
    "\n",
    "punctuation_fixes['total'] = punctuation_fixes['authority_citations'] + punctuation_fixes['attestations']\n",
    "\n",
    "# Create new checkpoint\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "new_table_name = f\"checkpoint_punctuation_only_diffs_fixed_{timestamp}\"\n",
    "\n",
    "full_checkpoint.to_sql(new_table_name, local_db, if_exists='replace', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PUNCTUATION-ONLY FIXES APPLIED\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Total entries fixed: {punctuation_fixes['total']}\")\n",
    "print(f\"  - Authority citations: {punctuation_fixes['authority_citations']}\")\n",
    "print(f\"  - Attestations: {punctuation_fixes['attestations']}\")\n",
    "print(f\"\\nNew checkpoint created: {new_table_name}\")\n",
    "print(f\"Original checkpoint preserved: {WHP_TABLE}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Create focused review reports\n",
    "print(\"\\nGenerating focused review reports...\")\n",
    "\n",
    "authority_manual_review = authority_report[authority_report['punctuation_only_diff'] == False]\n",
    "authority_manual_review.to_csv('./review_authority_manual.csv', index=False, encoding='utf-8-sig')\n",
    "print(f\"  - review_authority_manual.csv: {len(authority_manual_review)} entries\")\n",
    "\n",
    "attestation_manual_review = attestation_report[attestation_report['punctuation_only_diff'] == False]\n",
    "attestation_manual_review.to_csv('./review_attestation_manual.csv', index=False, encoding='utf-8-sig')\n",
    "print(f\"  - review_attestation_manual.csv: {len(attestation_manual_review)} entries\")\n",
    "\n",
    "translation_report.to_csv('./review_translation_manual.csv', index=False, encoding='utf-8-sig')\n",
    "print(f\"  - review_translation_manual.csv: {len(translation_report)} entries\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Auto-fixed: {punctuation_fixes['total']} entries\")\n",
    "print(f\"Manual review needed: {len(authority_manual_review) + len(attestation_manual_review) + len(translation_report)} entries\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "18f10c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation 3-way differences: 1417\n",
      "Authority 3-way differences: 1333\n",
      "Attestation 3-way differences: 2342\n",
      "3-WAY COMPARISON COMPLETE\n",
      "Reports generated:\n",
      "  - review_translation_3way.csv\n",
      "  - review_authority_3way.csv\n",
      "  - review_attestation_3way.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3-Way Comparison: Scraped vs Local (with punct fixes) vs checkpoint_final_20250914\n",
    "old_db = sqlite3.connect('../../../data/sqLiteDb/please_save_me.db')\n",
    "\n",
    "# 3-Way Comparison: Reload data from punctuation-fixed checkpoint first\n",
    "\n",
    "# Update table names\n",
    "WHP_TABLE_PUNCT_FIXED = \"checkpoint_punctuation_only_diffs_fixed_20251030\"\n",
    "WHP_TABLE_MANUAL_CORRECTIONS = \"checkpoint_final_20250914\"\n",
    "\n",
    "# STEP 1: Reload and re-enrich from punctuation-fixed checkpoint\n",
    "scraped_enriched = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        de.node_id,\n",
    "        de.headword,\n",
    "        de.orthographic_variants,\n",
    "        de.translation_english,\n",
    "        de.spanish_loanword,\n",
    "        de.source_dataset,\n",
    "        de.url_alias\n",
    "    FROM dictionary_entries de\n",
    "    WHERE de.source_dataset = 'WHP'\n",
    "    \"\"\",\n",
    "    scraped_db\n",
    ")\n",
    "\n",
    "# Enrich scraped data\n",
    "auth_cols = ['scraped_molina', 'scraped_karttunen', 'scraped_carochi', 'scraped_olmos', 'scraped_lockhart']\n",
    "attest_cols = ['scraped_attest_english', 'scraped_attest_spanish']\n",
    "\n",
    "for col in auth_cols + attest_cols:\n",
    "    scraped_enriched[col] = None\n",
    "\n",
    "for idx, row in scraped_enriched.iterrows():\n",
    "    node_id = row['node_id']\n",
    "    \n",
    "    auth_cites = get_authority_citations(node_id, scraped_db)\n",
    "    scraped_enriched.at[idx, 'scraped_molina'] = auth_cites['Molina'] #type: ignore\n",
    "    scraped_enriched.at[idx, 'scraped_karttunen'] = auth_cites['Karttunen'] #type: ignore\n",
    "    scraped_enriched.at[idx, 'scraped_carochi'] = auth_cites['Carochi'] #type: ignore\n",
    "    scraped_enriched.at[idx, 'scraped_olmos'] = auth_cites['Olmos'] #type: ignore\n",
    "    scraped_enriched.at[idx, 'scraped_lockhart'] = auth_cites['Lockhart'] #type: ignore\n",
    "    \n",
    "    attests = get_attestations(node_id, scraped_db)\n",
    "    scraped_enriched.at[idx, 'scraped_attest_english'] = attests['English'] #type: ignore\n",
    "    scraped_enriched.at[idx, 'scraped_attest_spanish'] = attests['Spanish'] #type: ignore\n",
    "\n",
    "# Load punctuation-fixed local data\n",
    "local_punct_fixed = pd.read_sql(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        Ref,\n",
    "        Headword,\n",
    "        \"Principal English Translation\" as local_translation,\n",
    "        \"Attestations from sources in English\" as local_attest_english,\n",
    "        \"Attestations from sources in Spanish\" as local_attest_spanish,\n",
    "        \"Alonso de Molina\" as local_molina,\n",
    "        \"Frances Karttunen\" as local_karttunen,\n",
    "        \"Horacio Carochi / English\" as local_carochi,\n",
    "        \"Andrés de Olmos\" as local_olmos,\n",
    "        \"Lockhart’s Nahuatl as Written\" as local_lockhart\n",
    "    FROM [{WHP_TABLE_PUNCT_FIXED}]\n",
    "    \"\"\",\n",
    "    local_db\n",
    ")\n",
    "\n",
    "# Load manual corrections checkpoint\n",
    "manual_corrections = pd.read_sql(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        Ref,\n",
    "        \"Principal English Translation\" as manual_translation,\n",
    "        \"Attestations from sources in English\" as manual_attest_english,\n",
    "        \"Attestations from sources in Spanish\" as manual_attest_spanish,\n",
    "        \"Alonso de Molina\" as manual_molina,\n",
    "        \"Frances Karttunen\" as manual_karttunen,\n",
    "        \"Horacio Carochi / English\" as manual_carochi,\n",
    "        \"Andrés de Olmos\" as manual_olmos,\n",
    "        \"Lockhart’s Nahuatl as Written\" as manual_lockhart\n",
    "    FROM [{WHP_TABLE_MANUAL_CORRECTIONS}]\n",
    "    \"\"\",\n",
    "    old_db\n",
    ")\n",
    "\n",
    "# Normalize Refs\n",
    "scraped_enriched['node_id'] = scraped_enriched['node_id'].astype(str)\n",
    "local_punct_fixed['Ref'] = local_punct_fixed['Ref'].astype(str).str.replace(r'^(WHP-|IDIEZ-)', '', regex=True)\n",
    "manual_corrections['Ref'] = manual_corrections['Ref'].astype(str).str.replace(r'^(WHP-|IDIEZ-)', '', regex=True)\n",
    "\n",
    "# Merge all three\n",
    "merged_3way = scraped_enriched.merge(\n",
    "    local_punct_fixed,\n",
    "    left_on='node_id',\n",
    "    right_on='Ref',\n",
    "    how='inner'\n",
    ").merge(\n",
    "    manual_corrections,\n",
    "    on='Ref',\n",
    "    how='inner',\n",
    "    suffixes=('_local', '_manual')\n",
    ")\n",
    "\n",
    "# Apply cleaning\n",
    "merged_3way['scraped_translation_clean'] = merged_3way['translation_english'].apply(strip_div_br_tags)\n",
    "merged_3way['local_translation_clean'] = merged_3way['local_translation'].apply(strip_div_br_tags)\n",
    "merged_3way['manual_translation_clean'] = merged_3way['manual_translation'].apply(strip_div_br_tags)\n",
    "\n",
    "merged_3way['scraped_molina_clean'] = merged_3way['scraped_molina'].apply(strip_div_br_tags)\n",
    "merged_3way['local_molina_clean'] = merged_3way['local_molina'].apply(strip_div_br_tags)\n",
    "merged_3way['manual_molina_clean'] = merged_3way['manual_molina'].apply(strip_div_br_tags)\n",
    "\n",
    "merged_3way['scraped_karttunen_clean'] = merged_3way['scraped_karttunen'].apply(strip_div_br_tags)\n",
    "merged_3way['local_karttunen_clean'] = merged_3way['local_karttunen'].apply(strip_div_br_tags)\n",
    "merged_3way['manual_karttunen_clean'] = merged_3way['manual_karttunen'].apply(strip_div_br_tags)\n",
    "\n",
    "merged_3way['scraped_carochi_clean'] = merged_3way['scraped_carochi'].apply(strip_div_br_tags)\n",
    "merged_3way['local_carochi_clean'] = merged_3way['local_carochi'].apply(strip_div_br_tags)\n",
    "merged_3way['manual_carochi_clean'] = merged_3way['manual_carochi'].apply(strip_div_br_tags)\n",
    "\n",
    "merged_3way['scraped_olmos_clean'] = merged_3way['scraped_olmos'].apply(strip_div_br_tags)\n",
    "merged_3way['local_olmos_clean'] = merged_3way['local_olmos'].apply(strip_div_br_tags)\n",
    "merged_3way['manual_olmos_clean'] = merged_3way['manual_olmos'].apply(strip_div_br_tags)\n",
    "\n",
    "merged_3way['scraped_lockhart_clean'] = merged_3way['scraped_lockhart'].apply(strip_div_br_tags)\n",
    "merged_3way['local_lockhart_clean'] = merged_3way['local_lockhart'].apply(strip_div_br_tags)\n",
    "merged_3way['manual_lockhart_clean'] = merged_3way['manual_lockhart'].apply(strip_div_br_tags)\n",
    "\n",
    "merged_3way['scraped_attest_english_clean'] = merged_3way['scraped_attest_english'].apply(strip_div_br_tags)\n",
    "merged_3way['local_attest_english_clean'] = merged_3way['local_attest_english'].apply(strip_div_br_tags)\n",
    "merged_3way['manual_attest_english_clean'] = merged_3way['manual_attest_english'].apply(strip_div_br_tags)\n",
    "\n",
    "merged_3way['scraped_attest_spanish_clean'] = merged_3way['scraped_attest_spanish'].apply(strip_div_br_tags)\n",
    "merged_3way['local_attest_spanish_clean'] = merged_3way['local_attest_spanish'].apply(strip_div_br_tags)\n",
    "merged_3way['manual_attest_spanish_clean'] = merged_3way['manual_attest_spanish'].apply(strip_div_br_tags)\n",
    "\n",
    "# Report 1: Translation 3-way comparison\n",
    "translation_3way_diffs = []\n",
    "\n",
    "for idx, row in merged_3way.iterrows():\n",
    "    scraped = row['scraped_translation_clean']\n",
    "    local = row['local_translation_clean']\n",
    "    manual = row['manual_translation_clean']\n",
    "    \n",
    "    if str(local).strip() == 'None':\n",
    "        continue\n",
    "    \n",
    "    values = [scraped, local, manual]\n",
    "    unique_values = set(values)\n",
    "    \n",
    "    if len(unique_values) > 1:\n",
    "        translation_3way_diffs.append({\n",
    "            'node_id': row['node_id'],\n",
    "            'headword': row['headword'],\n",
    "            'url_alias': row['url_alias'],\n",
    "            'scraped_value': scraped,\n",
    "            'local_punct_fixed_value': local,\n",
    "            'manual_corrections_value': manual,\n",
    "            'all_match': False,\n",
    "            'scraped_equals_local': scraped == local,\n",
    "            'scraped_equals_manual': scraped == manual,\n",
    "            'local_equals_manual': local == manual\n",
    "        })\n",
    "\n",
    "if translation_3way_diffs:\n",
    "    translation_3way_report = pd.DataFrame(translation_3way_diffs)\n",
    "    translation_3way_report.to_csv('./review_translation_3way.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"Translation 3-way differences: {len(translation_3way_report)}\")\n",
    "else:\n",
    "    print(\"Translation 3-way differences: 0\")\n",
    "\n",
    "# Report 2: Authority citations 3-way comparison\n",
    "authority_3way_diffs = []\n",
    "\n",
    "authorities = [\n",
    "    ('Molina', 'scraped_molina_clean', 'local_molina_clean', 'manual_molina_clean'),\n",
    "    ('Karttunen', 'scraped_karttunen_clean', 'local_karttunen_clean', 'manual_karttunen_clean'),\n",
    "    ('Carochi', 'scraped_carochi_clean', 'local_carochi_clean', 'manual_carochi_clean'),\n",
    "    ('Olmos', 'scraped_olmos_clean', 'local_olmos_clean', 'manual_olmos_clean'),\n",
    "    ('Lockhart', 'scraped_lockhart_clean', 'local_lockhart_clean', 'manual_lockhart_clean')\n",
    "]\n",
    "\n",
    "for idx, row in merged_3way.iterrows():\n",
    "    for auth_name, scraped_col, local_col, manual_col in authorities:\n",
    "        scraped = row[scraped_col]\n",
    "        local = row[local_col]\n",
    "        manual = row[manual_col]\n",
    "        \n",
    "        if str(local).strip() == 'None':\n",
    "            continue\n",
    "        \n",
    "        values = [scraped, local, manual]\n",
    "        unique_values = set(values)\n",
    "        \n",
    "        if len(unique_values) > 1:\n",
    "            authority_3way_diffs.append({\n",
    "                'node_id': row['node_id'],\n",
    "                'headword': row['headword'],\n",
    "                'url_alias': row['url_alias'],\n",
    "                'authority': auth_name,\n",
    "                'scraped_value': scraped,\n",
    "                'local_punct_fixed_value': local,\n",
    "                'manual_corrections_value': manual,\n",
    "                'all_match': False,\n",
    "                'scraped_equals_local': scraped == local,\n",
    "                'scraped_equals_manual': scraped == manual,\n",
    "                'local_equals_manual': local == manual\n",
    "            })\n",
    "\n",
    "if authority_3way_diffs:\n",
    "    authority_3way_report = pd.DataFrame(authority_3way_diffs)\n",
    "    authority_3way_report.to_csv('./review_authority_3way.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"Authority 3-way differences: {len(authority_3way_report)}\")\n",
    "else:\n",
    "    print(\"Authority 3-way differences: 0\")\n",
    "\n",
    "# Report 3: Attestations 3-way comparison\n",
    "attestation_3way_diffs = []\n",
    "\n",
    "attestations = [\n",
    "    ('English', 'scraped_attest_english_clean', 'local_attest_english_clean', 'manual_attest_english_clean'),\n",
    "    ('Spanish', 'scraped_attest_spanish_clean', 'local_attest_spanish_clean', 'manual_attest_spanish_clean')\n",
    "]\n",
    "\n",
    "for idx, row in merged_3way.iterrows():\n",
    "    for lang, scraped_col, local_col, manual_col in attestations:\n",
    "        scraped = row[scraped_col]\n",
    "        local = row[local_col]\n",
    "        manual = row[manual_col]\n",
    "        \n",
    "        if str(local).strip() == 'None':\n",
    "            continue\n",
    "        \n",
    "        values = [scraped, local, manual]\n",
    "        unique_values = set(values)\n",
    "        \n",
    "        if len(unique_values) > 1:\n",
    "            attestation_3way_diffs.append({\n",
    "                'node_id': row['node_id'],\n",
    "                'headword': row['headword'],\n",
    "                'url_alias': row['url_alias'],\n",
    "                'language': lang,\n",
    "                'scraped_value': scraped,\n",
    "                'local_punct_fixed_value': local,\n",
    "                'manual_corrections_value': manual,\n",
    "                'all_match': False,\n",
    "                'scraped_equals_local': scraped == local,\n",
    "                'scraped_equals_manual': scraped == manual,\n",
    "                'local_equals_manual': local == manual\n",
    "            })\n",
    "\n",
    "if attestation_3way_diffs:\n",
    "    attestation_3way_report = pd.DataFrame(attestation_3way_diffs)\n",
    "    attestation_3way_report.to_csv('./review_attestation_3way.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"Attestation 3-way differences: {len(attestation_3way_report)}\")\n",
    "else:\n",
    "    print(\"Attestation 3-way differences: 0\")\n",
    "\n",
    "print(\"3-WAY COMPARISON COMPLETE\")\n",
    "print(\"Reports generated:\")\n",
    "print(\"  - review_translation_3way.csv\")\n",
    "print(\"  - review_authority_3way.csv\")\n",
    "print(\"  - review_attestation_3way.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "1f68ee10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fixed 734 translation entries (scraped=manual)\n",
      "Fixed 333 authority entries (scraped=manual)\n",
      "Fixed 1598 attestation entries (scraped=manual)\n",
      "SCRAPED=MANUAL FIXES APPLIED\n",
      "Total entries fixed: 2665\n",
      "  - Translations: 734\n",
      "  - Authority citations: 333\n",
      "  - Attestations: 1598\n",
      "\n",
      "New checkpoint created: checkpoint_scraped_manual_match_fixed_20251030\n",
      "Original checkpoint preserved: checkpoint_punctuation_only_diffs_fixed_20251030\n",
      "\n",
      "Generating updated reports (excluding scraped=manual matches)...\n",
      "  - review_translation_3way_final.csv: 683 entries\n",
      "  - review_authority_3way_final.csv: 1000 entries\n",
      "  - review_attestation_3way_final.csv: 744 entries\n",
      "SUMMARY\n",
      "Auto-fixed (scraped=manual): 2665 entries\n",
      "Remaining for manual review: 2427 entries\n"
     ]
    }
   ],
   "source": [
    "# Auto-fix where scraped equals manual, and update reports\n",
    "\n",
    "# Load full punctuation-fixed checkpoint\n",
    "full_checkpoint = pd.read_sql(f\"SELECT * FROM [{WHP_TABLE_PUNCT_FIXED}]\", local_db)\n",
    "full_checkpoint['Ref_clean'] = full_checkpoint['Ref'].astype(str).str.replace(r'^(WHP-|IDIEZ-)', '', regex=True)\n",
    "\n",
    "# Track fixes\n",
    "scraped_manual_match_fixes = {\n",
    "    'translations': 0,\n",
    "    'authorities': 0,\n",
    "    'attestations': 0,\n",
    "    'total': 0\n",
    "}\n",
    "\n",
    "# Fix translations where scraped equals manual\n",
    "translation_to_fix = [d for d in translation_3way_diffs if d['scraped_equals_manual']]\n",
    "for item in translation_to_fix:\n",
    "    node_id = str(item['node_id'])\n",
    "    manual_value = item['manual_corrections_value']\n",
    "    \n",
    "    mask = full_checkpoint['Ref_clean'] == node_id\n",
    "    if mask.any():\n",
    "        full_checkpoint.loc[mask, 'Principal English Translation'] = manual_value\n",
    "        scraped_manual_match_fixes['translations'] += 1\n",
    "\n",
    "print(f\"Fixed {scraped_manual_match_fixes['translations']} translation entries (scraped=manual)\")\n",
    "\n",
    "# Fix authorities where scraped equals manual\n",
    "authority_column_map = {\n",
    "    'Molina': 'Alonso de Molina',\n",
    "    'Karttunen': 'Frances Karttunen',\n",
    "    'Carochi': 'Horacio Carochi / English',\n",
    "    'Olmos': 'Andrés de Olmos',\n",
    "    'Lockhart': 'Lockhart’s Nahuatl as Written'\n",
    "}\n",
    "\n",
    "authority_to_fix = [d for d in authority_3way_diffs if d['scraped_equals_manual']]\n",
    "for item in authority_to_fix:\n",
    "    node_id = str(item['node_id'])\n",
    "    authority = item['authority']\n",
    "    manual_value = item['manual_corrections_value']\n",
    "    local_column = authority_column_map[authority]\n",
    "    \n",
    "    mask = full_checkpoint['Ref_clean'] == node_id\n",
    "    if mask.any():\n",
    "        full_checkpoint.loc[mask, local_column] = manual_value\n",
    "        scraped_manual_match_fixes['authorities'] += 1\n",
    "\n",
    "print(f\"Fixed {scraped_manual_match_fixes['authorities']} authority entries (scraped=manual)\")\n",
    "\n",
    "# Fix attestations where scraped equals manual\n",
    "attestation_to_fix = [d for d in attestation_3way_diffs if d['scraped_equals_manual']]\n",
    "for item in attestation_to_fix:\n",
    "    node_id = str(item['node_id'])\n",
    "    language = item['language']\n",
    "    manual_value = item['manual_corrections_value']\n",
    "    local_column = f'Attestations from sources in {language}'\n",
    "    \n",
    "    mask = full_checkpoint['Ref_clean'] == node_id\n",
    "    if mask.any():\n",
    "        full_checkpoint.loc[mask, local_column] = manual_value\n",
    "        scraped_manual_match_fixes['attestations'] += 1\n",
    "\n",
    "print(f\"Fixed {scraped_manual_match_fixes['attestations']} attestation entries (scraped=manual)\")\n",
    "\n",
    "# Drop helper column\n",
    "full_checkpoint = full_checkpoint.drop(columns=['Ref_clean'])\n",
    "\n",
    "scraped_manual_match_fixes['total'] = (\n",
    "    scraped_manual_match_fixes['translations'] +\n",
    "    scraped_manual_match_fixes['authorities'] +\n",
    "    scraped_manual_match_fixes['attestations']\n",
    ")\n",
    "\n",
    "# Create new checkpoint\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d\")\n",
    "new_table_name = f\"checkpoint_scraped_manual_match_fixed_{timestamp}\"\n",
    "\n",
    "full_checkpoint.to_sql(new_table_name, local_db, if_exists='replace', index=False)\n",
    "\n",
    "print(\"SCRAPED=MANUAL FIXES APPLIED\")\n",
    "print(f\"Total entries fixed: {scraped_manual_match_fixes['total']}\")\n",
    "print(f\"  - Translations: {scraped_manual_match_fixes['translations']}\")\n",
    "print(f\"  - Authority citations: {scraped_manual_match_fixes['authorities']}\")\n",
    "print(f\"  - Attestations: {scraped_manual_match_fixes['attestations']}\")\n",
    "print(f\"\\nNew checkpoint created: {new_table_name}\")\n",
    "print(f\"Original checkpoint preserved: {WHP_TABLE_PUNCT_FIXED}\")\n",
    "\n",
    "# Generate updated reports excluding scraped=manual matches\n",
    "print(\"\\nGenerating updated reports (excluding scraped=manual matches)...\")\n",
    "\n",
    "# Translation report - only genuine differences\n",
    "translation_genuine_diffs = [d for d in translation_3way_diffs if not d['scraped_equals_manual']]\n",
    "if translation_genuine_diffs:\n",
    "    translation_genuine_report = pd.DataFrame(translation_genuine_diffs)\n",
    "    translation_genuine_report.to_csv('./review_translation_3way_final.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"  - review_translation_3way_final.csv: {len(translation_genuine_diffs)} entries\")\n",
    "else:\n",
    "    print(f\"  - review_translation_3way_final.csv: 0 entries\")\n",
    "\n",
    "# Authority report - only genuine differences\n",
    "authority_genuine_diffs = [d for d in authority_3way_diffs if not d['scraped_equals_manual']]\n",
    "if authority_genuine_diffs:\n",
    "    authority_genuine_report = pd.DataFrame(authority_genuine_diffs)\n",
    "    authority_genuine_report.to_csv('./review_authority_3way_final.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"  - review_authority_3way_final.csv: {len(authority_genuine_diffs)} entries\")\n",
    "else:\n",
    "    print(f\"  - review_authority_3way_final.csv: 0 entries\")\n",
    "\n",
    "# Attestation report - only genuine differences\n",
    "attestation_genuine_diffs = [d for d in attestation_3way_diffs if not d['scraped_equals_manual']]\n",
    "if attestation_genuine_diffs:\n",
    "    attestation_genuine_report = pd.DataFrame(attestation_genuine_diffs)\n",
    "    attestation_genuine_report.to_csv('./review_attestation_3way_final.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"  - review_attestation_3way_final.csv: {len(attestation_genuine_diffs)} entries\")\n",
    "else:\n",
    "    print(f\"  - review_attestation_3way_final.csv: 0 entries\")\n",
    "\n",
    "print(\"SUMMARY\")\n",
    "print(f\"Auto-fixed (scraped=manual): {scraped_manual_match_fixes['total']} entries\")\n",
    "print(f\"Remaining for manual review: {len(translation_genuine_diffs) + len(authority_genuine_diffs) + len(attestation_genuine_diffs)} entries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "03b0d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraped_db.close()\n",
    "# local_db.close()\n",
    "# print(\"Database connections closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74d6fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nahuaLEX_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
