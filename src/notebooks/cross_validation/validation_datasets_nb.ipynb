{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5346fcb6",
   "metadata": {},
   "source": [
    "# Validation and correction\n",
    "\n",
    "As we've continued working with the scraped data versus the changes we've made from the initial csv files given to the LRC. We've done validation and corrections to the initial csv files that are not reflected in the scraped datasets. So now since we've uploaded our data into SQLite the next step will be to cross validate and update the SQLite. Which will be done below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cef09ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup and imports\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Optional, Union, Any\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af11e03d",
   "metadata": {},
   "source": [
    "note the original local datasets given to the LRC initially are hosted in the nahuatl_processing.db under sqLiteDb specifically under the table name: checkpoint_after_empty_p_tag_removal_20251002. For the scraped data, that is under scrapedDataDb under the nahuatl.db file. The schema is under config/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8ed35dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data_dir = Path(\"../../../data/scrapedDataDb/\")\n",
    "local_data_dir = Path(\"../../../data/sqLiteDb/\")\n",
    "\n",
    "if not scraped_data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Scraped database not found: {scraped_data_dir}\")\n",
    "if not local_data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Local database not found: {local_data_dir}\")\n",
    "\n",
    "# Database connection\n",
    "scraped_db = sqlite3.connect(scraped_data_dir / \"nahuatl.db\")\n",
    "local_db = sqlite3.connect(local_data_dir / \"nahuatl_processing.db\")\n",
    "\n",
    "\n",
    "# read in the table(s), for the local_db it's only one table (actually two one for the WHP dataset and one for the IDIEZ dataset) while for the \n",
    "# scraped_db there are multiple tables due to the relationl structure we want to keep\n",
    "tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", local_db)\n",
    "WHP_TABLE = \"checkpoint_after_empty_p_tag_removal_20251002\"\n",
    "IDIEZ_TABLE = \"IDIEZ_modern_nahuatl-all-2024-03-27T09-45-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "dcfb3631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map scraped DB fields to local DB fields for WHP data\n",
    "WHP_FIELD_MAPPING = {\n",
    "    # Scraped field: Local field\n",
    "    'node_id': 'Ref',\n",
    "    'headword': 'Headword',\n",
    "    'orthographic_variants': 'Orthographic Variants',\n",
    "    'translation_english': 'Principal English Translation',\n",
    "    'spanish_loanword': 'Spanish Loanword',\n",
    "    # Authority fields (stored in authority_citations table in scraped DB)\n",
    "    'authority_molina': 'Alonso de Molina',\n",
    "    'authority_karttunen': 'Frances Karttunen',\n",
    "    'authority_carochi': 'Horacio Carochi / English',\n",
    "    'authority_olmos': 'Andrés de Olmos',\n",
    "    'authority_lockhart': \"Lockhart’s Nahuatl as Written\",\n",
    "    # Attestations (stored in attestations table in scraped DB)\n",
    "    'attestations_english': 'Attestations from sources in English',\n",
    "    'attestations_spanish': 'Attestations from sources in Spanish',\n",
    "    # Metadata\n",
    "    'themes': 'themes',\n",
    "}\n",
    "\n",
    "# Map scraped DB fields to local DB fields for IDIEZ data\n",
    "IDIEZ_FIELD_MAPPING = {\n",
    "    'node_id': 'Ref',\n",
    "    'headword_idiez': 'tlahtolli',\n",
    "    'translation_english_idiez': 'IDIEZ traduc. inglés',\n",
    "    'definition_nahuatl_idiez': 'IDIEZ def. náhuatl',\n",
    "    'definition_spanish_idiez': 'IDIEZ def. español',\n",
    "    'morfologia_idiez': 'IDIEZ morfología',\n",
    "    'gramatica_idiez': 'IDIEZ gramática',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "34c55cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dataframes(df_scraped, df_local, field_mapping, key_field=\"node_id\"):\n",
    "\n",
    "    scraped_key = field_mapping.get(key_field, key_field)\n",
    "    df_scraped = df_scraped.copy()\n",
    "    df_local = df_local.copy()\n",
    "    df_scraped[key_field] = df_scraped[key_field].astype(str)\n",
    "    df_local[scraped_key] = df_local[scraped_key].astype(str)\n",
    "    df_local[scraped_key] = (\n",
    "        df_local[scraped_key].astype(str).str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    "    )\n",
    "\n",
    "    merged = df_scraped.merge(\n",
    "        df_local,\n",
    "        left_on=key_field,\n",
    "        right_on=scraped_key,\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_scraped\", \"_local\"),\n",
    "    )\n",
    "    print(f\"Rows in scraped DB: {len(df_scraped):,}\")\n",
    "    print(f\"Rows in local DB: {len(df_local):,}\")\n",
    "    print(f\"Rows matched: {len(merged):,}\")\n",
    "\n",
    "    discrepancies = {\n",
    "        \"field_discrepancies\": {},\n",
    "        \"total_discrepancies\": 0,\n",
    "        \"rows_compared\": len(merged),\n",
    "        \"sample_discrepancies\": [],\n",
    "    }\n",
    "\n",
    "    for scraped_field, local_field in field_mapping.items():\n",
    "        if scraped_field == key_field:\n",
    "            continue\n",
    "        if (\n",
    "            scraped_field not in df_scraped.columns\n",
    "            or local_field not in df_local.columns\n",
    "        ):\n",
    "            print(f\"Skipping {scraped_field} (not in both datasets)\")\n",
    "            continue\n",
    "        scraped_col = (\n",
    "            f\"{scraped_field}_scraped\"\n",
    "            if scraped_field in df_local.columns\n",
    "            else scraped_field\n",
    "        )\n",
    "        local_col = (\n",
    "            f\"{local_field}_local\" if local_field in df_scraped.columns else local_field\n",
    "        )\n",
    "\n",
    "        scraped_values = merged[scraped_col].fillna(\"\").astype(str).str.strip()\n",
    "        local_values = merged[local_col].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "        merged[\"diff\"] = ~(\n",
    "            (scraped_values == local_values)\n",
    "            | (merged[scraped_col].isna() & merged[local_col].isna())\n",
    "        )\n",
    "        diff_count = merged[\"diff\"].sum()\n",
    "\n",
    "        if diff_count > 0:\n",
    "            print(f\"{scraped_field:30} {diff_count:>6,} discrepancies\")\n",
    "            discrepancies[\"field_discrepancies\"][scraped_field] = {\n",
    "                \"count\": int(diff_count),\n",
    "                \"local_field\": local_field,\n",
    "                \"sample_rows\": merged[merged[\"diff\"]][key_field].head(10).tolist(),\n",
    "            }\n",
    "            discrepancies[\"total_discrepancies\"] += int(diff_count)\n",
    "\n",
    "            if len(discrepancies[\"sample_discrepancies\"]) < 5:\n",
    "                sample = merged[merged[\"diff\"]].iloc[0]\n",
    "                discrepancies[\"sample_discrepancies\"].append(\n",
    "                    {\n",
    "                        \"node_id\": sample[key_field],\n",
    "                        \"field\": scraped_field,\n",
    "                        \"scraped_value\": str(sample[scraped_col])[:100],\n",
    "                        \"local_value\": str(sample[local_col])[:100],\n",
    "                    }\n",
    "                )\n",
    "        else:\n",
    "            print(f\"{scraped_field:30} all match\")\n",
    "\n",
    "    print(f\"Total discrepancies: {discrepancies['total_discrepancies']:,}\")\n",
    "    return discrepancies\n",
    "\n",
    "\n",
    "def create_update_dataframe(scraped_df, local_df, field_mapping, key_field=\"node_id\"):\n",
    "    \"\"\"Create dataframe showing what needs updating\"\"\"\n",
    "    scraped_key = field_mapping.get(key_field, key_field)\n",
    "\n",
    "    scraped_df = scraped_df.copy()\n",
    "    local_df = local_df.copy()\n",
    "    scraped_df[key_field] = scraped_df[key_field].astype(str)\n",
    "    local_df[scraped_key] = (\n",
    "        local_df[scraped_key].astype(str).str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    "    )\n",
    "\n",
    "    merged = scraped_df.merge(\n",
    "        local_df,\n",
    "        left_on=key_field,\n",
    "        right_on=scraped_key,\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_scraped\", \"_local\"),\n",
    "    )\n",
    "\n",
    "    updates = []\n",
    "\n",
    "    for scraped_field, local_field in field_mapping.items():\n",
    "        if scraped_field == key_field:\n",
    "            continue\n",
    "\n",
    "        if (\n",
    "            scraped_field not in scraped_df.columns\n",
    "            or local_field not in local_df.columns\n",
    "        ):\n",
    "            continue\n",
    "\n",
    "        scraped_col = (\n",
    "            f\"{scraped_field}_scraped\"\n",
    "            if scraped_field in local_df.columns\n",
    "            else scraped_field\n",
    "        )\n",
    "        local_col = (\n",
    "            f\"{local_field}_local\" if local_field in scraped_df.columns else local_field\n",
    "        )\n",
    "        scraped_values = merged[scraped_col].fillna(\"\").astype(str).str.strip()\n",
    "        local_values = merged[local_col].fillna(\"\").astype(str).str.strip()\n",
    "\n",
    "        diff_mask = ~(\n",
    "            (scraped_values == local_values)\n",
    "            | (merged[scraped_col].isna() & merged[local_col].isna())\n",
    "        )\n",
    "\n",
    "        diff_rows = merged[diff_mask]\n",
    "\n",
    "        for idx, row in diff_rows.iterrows():\n",
    "\n",
    "            current_stripped = (\n",
    "                str(row[scraped_col]).strip() if pd.notna(row[scraped_col]) else \"\"\n",
    "            )\n",
    "            new_stripped = (\n",
    "                str(row[local_col]).strip() if pd.notna(row[local_col]) else \"\"\n",
    "            )\n",
    "\n",
    "            updates.append(\n",
    "                {\n",
    "                    \"node_id\": row[key_field],\n",
    "                    \"field\": scraped_field,\n",
    "                    \"current_value\": current_stripped,\n",
    "                    \"new_value\": new_stripped,\n",
    "                    \"action\": \"UPDATE\",\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return pd.DataFrame(updates)\n",
    "\n",
    "\n",
    "def apply_updates(updates_df, conn, table_name=\"dictionary_entries\", dry_run=True):\n",
    "\n",
    "    print(f\"APPLYING UPDATES TO {table_name}\")\n",
    "    print(f\"Mode: {'DRY RUN' if dry_run else 'LIVE UPDATES'}\")\n",
    "    stats = {\n",
    "        \"total_updates\": 0,\n",
    "        \"successful_updates\": 0,\n",
    "        \"failed_updates\": 0,\n",
    "        \"updates_by_field\": defaultdict(int),\n",
    "    }\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    grouped = updates_df.groupby(\"node_id\")\n",
    "\n",
    "    for node_id, group in grouped:\n",
    "        try:\n",
    "            set_clauses = []\n",
    "            values = []\n",
    "\n",
    "            for _, row in group.iterrows():\n",
    "                set_clauses.append(f\"{row['field']} = ?\")\n",
    "                values.append(row[\"new_value\"])\n",
    "                stats[\"updates_by_field\"][row[\"field\"]] += 1\n",
    "\n",
    "            values.append(node_id)\n",
    "            sql = f\"UPDATE {table_name} SET {', '.join(set_clauses)} WHERE node_id = ?\"\n",
    "\n",
    "            if not dry_run:\n",
    "                cursor.execute(sql, values)\n",
    "\n",
    "            stats[\"successful_updates\"] += len(group)\n",
    "            stats[\"total_updates\"] += len(group)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating node_id {node_id}: {e}\")\n",
    "            stats[\"failed_updates\"] += len(group)\n",
    "            stats[\"total_updates\"] += len(group)\n",
    "\n",
    "    if not dry_run:\n",
    "        conn.commit()\n",
    "        print(\"Changes committed\")\n",
    "    else:\n",
    "        print(\"Dry run complete - no changes made\")\n",
    "\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Total updates: {stats['total_updates']:,}\")\n",
    "    print(f\"  Successful: {stats['successful_updates']:,}\")\n",
    "    print(f\"  Failed: {stats['failed_updates']:,}\")\n",
    "    print(f\"\\nUpdates by field:\")\n",
    "    for field, count in stats[\"updates_by_field\"].items():\n",
    "        print(f\"  - {field}: {count:,}\")\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bf76dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bibl_tags(text):\n",
    "    \"\"\"Find all <bibl> tags and their positions\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return []\n",
    "\n",
    "    pattern = re.compile(r\"<bibl[^>]*>.*?</bibl>\", re.DOTALL | re.IGNORECASE)\n",
    "    matches = []\n",
    "    for match in pattern.finditer(str(text)):\n",
    "        matches.append(\n",
    "            {\n",
    "                \"full_tag\": match.group(0),\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end(),\n",
    "                \"before_text\": str(text)[max(0, match.start() - 30) : match.start()],\n",
    "                \"after_text\": str(text)[match.end() : min(len(text), match.end() + 30)],\n",
    "            }\n",
    "        )\n",
    "    return matches\n",
    "\n",
    "\n",
    "def strip_html_better(text):\n",
    "    \"\"\"More aggressive HTML stripping\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "    text = re.sub(r'\\w+=\"[^\"]*\"', \"\", text)\n",
    "    text = re.sub(r\"\\w+=\\'[^\\']*\\'\", \"\", text)\n",
    "\n",
    "    text = \" \".join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def find_crossref_positions_in_scraped_improved(scraped_text):\n",
    "    \"\"\"Find cross-references with better context extraction\"\"\"\n",
    "    if pd.isna(scraped_text):\n",
    "        return []\n",
    "\n",
    "    pattern = re.compile(r\"\\(see\\s+([^)]+)\\)\", re.IGNORECASE)\n",
    "    positions = []\n",
    "\n",
    "    clean_text = strip_html_better(scraped_text)\n",
    "\n",
    "    for match in pattern.finditer(clean_text):\n",
    "        content = match.group(1).strip()\n",
    "\n",
    "        before_start = max(0, match.start() - 30)\n",
    "        before_text = clean_text[before_start : match.start()].strip()\n",
    "\n",
    "        words_before = before_text.split()\n",
    "        anchor = (\n",
    "            \" \".join(words_before[-5:])\n",
    "            if len(words_before) >= 5\n",
    "            else \" \".join(words_before)\n",
    "        )\n",
    "\n",
    "        positions.append(\n",
    "            {\n",
    "                \"full_match\": match.group(0),\n",
    "                \"content\": content,\n",
    "                \"anchor_before\": anchor,\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return positions\n",
    "\n",
    "\n",
    "def reinsert_crossref_improved(local_text, anchor_text, crossref_text):\n",
    "    \"\"\"Insert cross-ref with better matching\"\"\"\n",
    "    if pd.isna(local_text):\n",
    "        return local_text, False\n",
    "\n",
    "    local_str = str(local_text)\n",
    "\n",
    "    local_stripped = strip_html_better(local_str)\n",
    "    anchor_stripped = strip_html_better(anchor_text)\n",
    "\n",
    "    if anchor_stripped and anchor_stripped in local_stripped:\n",
    "\n",
    "        anchor_words = anchor_stripped.split()\n",
    "        if anchor_words:\n",
    "            last_word = anchor_words[-1].strip(\".,;:!?\")\n",
    "\n",
    "            word_pos = local_str.rfind(last_word)\n",
    "            if word_pos != -1:\n",
    "\n",
    "                insertion_point = word_pos + len(last_word)\n",
    "\n",
    "                if not crossref_text.startswith(\"(\"):\n",
    "                    crossref_text = f\" (see {crossref_text})\"\n",
    "                else:\n",
    "                    crossref_text = f\" {crossref_text}\"\n",
    "\n",
    "                reinserted = (\n",
    "                    local_str[:insertion_point]\n",
    "                    + crossref_text\n",
    "                    + local_str[insertion_point:]\n",
    "                )\n",
    "                return reinserted, True\n",
    "\n",
    "    if \"</p>\" in local_str:\n",
    "        insertion_point = local_str.rfind(\"</p>\")\n",
    "    else:\n",
    "        insertion_point = len(local_str)\n",
    "\n",
    "    if not crossref_text.startswith(\"(\"):\n",
    "        crossref_text = f\" (see {crossref_text})\"\n",
    "    else:\n",
    "        crossref_text = f\" {crossref_text}\"\n",
    "\n",
    "    reinserted = (\n",
    "        local_str[:insertion_point] + crossref_text + local_str[insertion_point:]\n",
    "    )\n",
    "    return reinserted, True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48516e8d",
   "metadata": {},
   "source": [
    "for me it methodically makes sense to go down the tables that nahuat.db has (check config/schema.sql) and then check each one against the WHP_table_name and the IDIEZ_table_name as such let's begin with the largest tables first and build up from the tables. \n",
    "it would also be smart that as we cross validate we proceed to investigate where the cross references columns actually come from\n",
    "also a side note since we've done no manual corrections or fixes to the IDIEZ fields we can begin with validating local IDIEZ with scraped IDIEZ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a66c4992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6,846 IDIEZ/HYBRID entries from scraped DB\n",
      "Loaded 6,846 IDIEZ entries from local DB\n",
      "Rows in scraped DB: 6,846\n",
      "Rows in local DB: 6,846\n",
      "Rows matched: 6,844\n",
      "headword_idiez                 all match\n",
      "translation_english_idiez          11 discrepancies\n",
      "definition_nahuatl_idiez           24 discrepancies\n",
      "definition_spanish_idiez          425 discrepancies\n",
      "morfologia_idiez               all match\n",
      "gramatica_idiez                all match\n",
      "Total discrepancies: 460\n",
      "\n",
      "Sample IDIEZ discrepancies:\n",
      "\n",
      "1. node_id=187252, field=translation_english_idiez\n",
      "   Scraped: \n",
      "   Local:   to transport rocks.\n",
      "\n",
      "2. node_id=176130, field=definition_nahuatl_idiez\n",
      "   Scraped: ICPATL. tlat. Tlamalintli iloh, iixnezca chipahuac quitequihuah ica quichihchihuah cantelah o ica tl\n",
      "   Local:   ICPATL. tlat. Tlamalintli iloh, iixnezca chipahuac quitequihuah ica quichihchihuah cantelah o ica tl\n",
      "\n",
      "3. node_id=172003, field=definition_spanish_idiez\n",
      "   Scraped: A.1. se enfrìa. “Se enfría la tierra de noche. 2. Se va la luz. “Cuando hace truenos se enfrìa la ti\n",
      "   Local:   \tA.1. se enfrìa. “Se enfría la tierra de noche. 2. Se va la luz. “Cuando hace truenos se enfrìa la t\n"
     ]
    }
   ],
   "source": [
    "scraped_idiez = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        node_id,\n",
    "        headword_idiez,\n",
    "        translation_english_idiez,\n",
    "        definition_nahuatl_idiez,\n",
    "        definition_spanish_idiez,\n",
    "        morfologia_idiez,\n",
    "        gramatica_idiez,\n",
    "        source_dataset\n",
    "    FROM dictionary_entries\n",
    "    WHERE source_dataset IN ('IDIEZ', 'HYBRID')\n",
    "\"\"\",\n",
    "    scraped_db,\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(scraped_idiez):,} IDIEZ/HYBRID entries from scraped DB\")\n",
    "\n",
    "# Load IDIEZ from local DB\n",
    "local_idiez = pd.read_sql(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        Ref,\n",
    "        tlahtolli,\n",
    "        \"IDIEZ traduc. inglés\",\n",
    "        \"IDIEZ def. náhuatl\",\n",
    "        \"IDIEZ def. español\",\n",
    "        \"IDIEZ morfología\",\n",
    "        \"IDIEZ gramática\"\n",
    "    FROM [{IDIEZ_TABLE}]\n",
    "\"\"\",\n",
    "    local_db,\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(local_idiez):,} IDIEZ entries from local DB\")\n",
    "\n",
    "# Compare\n",
    "idiez_discrepancies = compare_dataframes(\n",
    "    scraped_idiez, local_idiez, IDIEZ_FIELD_MAPPING, key_field=\"node_id\"\n",
    ")\n",
    "\n",
    "# Show samples\n",
    "if idiez_discrepancies[\"sample_discrepancies\"]:\n",
    "    print(\"\\nSample IDIEZ discrepancies:\")\n",
    "    for i, sample in enumerate(idiez_discrepancies[\"sample_discrepancies\"][:5], 1):\n",
    "        print(f\"\\n{i}. node_id={sample['node_id']}, field={sample['field']}\")\n",
    "        print(f\"   Scraped: {sample['scraped_value']}\")\n",
    "        print(f\"   Local:   {sample['local_value']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6d5a8706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating IDIEZ update report...\n",
      "Total IDIEZ updates needed: 460\n",
      "\n",
      "Updates by field:\n",
      "field\n",
      "definition_spanish_idiez     425\n",
      "definition_nahuatl_idiez      24\n",
      "translation_english_idiez     11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Update report saved to: idiez_updates_needed.csv\n"
     ]
    }
   ],
   "source": [
    "# creating the IDIEZ report\n",
    "if idiez_discrepancies[\"total_discrepancies\"] > 0:\n",
    "    print(\"\\nCreating IDIEZ update report...\")\n",
    "\n",
    "    idiez_updates = create_update_dataframe(\n",
    "        scraped_idiez, local_idiez, IDIEZ_FIELD_MAPPING\n",
    "    )\n",
    "\n",
    "    print(f\"Total IDIEZ updates needed: {len(idiez_updates):,}\")\n",
    "    print(\"\\nUpdates by field:\")\n",
    "    print(idiez_updates[\"field\"].value_counts())\n",
    "\n",
    "    # idiez_updates.to_csv('idiez_updates_needed.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"\\nUpdate report saved to: idiez_updates_needed.csv\")\n",
    "else:\n",
    "    print(\"\\nNo IDIEZ updates needed - data matches perfectly!\")\n",
    "    idiez_updates = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6a7dbd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 31,742 WHP entries from scraped DB\n",
      "Loaded 31,806 WHP entries from local DB\n",
      "COMPARING WHP DATA\n",
      "Rows in scraped DB: 31,742\n",
      "Rows in local DB: 31,806\n",
      "Rows matched: 31,466\n",
      "headword                           76 discrepancies\n",
      "orthographic_variants             187 discrepancies\n",
      "translation_english            31,466 discrepancies\n",
      "spanish_loanword               31,466 discrepancies\n",
      "Total discrepancies: 63,195\n",
      "\n",
      "Sample WHP discrepancies:\n",
      "\n",
      "1. node_id=172881, field=headword\n",
      "   Scraped: -icxitlan.\n",
      "   Local:   -icxtlan.\n",
      "\n",
      "2. node_id=171987, field=orthographic_variants\n",
      "   Scraped: canah\n",
      "   Local:   None\n",
      "\n",
      "3. node_id=171879, field=translation_english\n",
      "   Scraped: <div class=\"field-item even\"><p>perhaps not (adverb) (see Molina)</p>\n",
      "</div>\n",
      "   Local:   <p>perhaps not (adverb)</p>\n",
      "\n",
      "4. node_id=171879, field=spanish_loanword\n",
      "   Scraped: \n",
      "   Local:   No\n",
      "WHP COMPARISON COMPLETE\n",
      "Total discrepancies found: 63,195\n",
      "Fields with discrepancies: 4\n"
     ]
    }
   ],
   "source": [
    "scraped_whp = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        node_id,\n",
    "        headword,\n",
    "        orthographic_variants,\n",
    "        translation_english,\n",
    "        spanish_loanword,\n",
    "        source_dataset\n",
    "    FROM dictionary_entries\n",
    "    WHERE source_dataset = 'WHP'\n",
    "\"\"\",\n",
    "    scraped_db,\n",
    ")\n",
    "print(f\"Loaded {len(scraped_whp):,} WHP entries from scraped DB\")\n",
    "\n",
    "local_whp = pd.read_sql(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        Ref,\n",
    "        Headword,\n",
    "        \"Orthographic Variants\",\n",
    "        \"Principal English Translation\",\n",
    "        \"Attestations from sources in English\",\n",
    "        \"Attestations from sources in Spanish\"\n",
    "        \"Alonso de Molina\",\n",
    "        \"Frances Karttunen\",\n",
    "        \"Horacio Carochi / English\",\n",
    "        \"Andrés de Olmos\",\n",
    "        \"Lockhart’s Nahuatl as Written\",\n",
    "        \"themes\",\n",
    "        \"Spanish Loanword\",\n",
    "        \"Citations\",\n",
    "        \"Number_of_Citations\",\n",
    "        \"Cross_References\",\n",
    "        \"Number_of_Cross_References\",\n",
    "        \"CrossRef_Types\"\n",
    "    FROM [{WHP_TABLE}]\n",
    "\"\"\",\n",
    "    local_db,\n",
    ")\n",
    "print(f\"Loaded {len(local_whp):,} WHP entries from local DB\")\n",
    "print(\"COMPARING WHP DATA\")\n",
    "\n",
    "whp_field_mapping = {\n",
    "    \"node_id\": \"Ref\",\n",
    "    \"headword\": \"Headword\",\n",
    "    \"orthographic_variants\": \"Orthographic Variants\",\n",
    "    \"translation_english\": \"Principal English Translation\",\n",
    "    \"spanish_loanword\": \"Spanish Loanword\",\n",
    "}\n",
    "\n",
    "whp_discrepancies = compare_dataframes(\n",
    "    scraped_whp, local_whp, whp_field_mapping, key_field=\"node_id\"\n",
    ")\n",
    "\n",
    "if whp_discrepancies[\"sample_discrepancies\"]:\n",
    "    print(\"\\nSample WHP discrepancies:\")\n",
    "    for i, sample in enumerate(whp_discrepancies[\"sample_discrepancies\"][:5], 1):\n",
    "        print(f\"\\n{i}. node_id={sample['node_id']}, field={sample['field']}\")\n",
    "        print(f\"   Scraped: {sample['scraped_value']}\")\n",
    "        print(f\"   Local:   {sample['local_value']}\")\n",
    "\n",
    "print(\"WHP COMPARISON COMPLETE\")\n",
    "print(f\"Total discrepancies found: {whp_discrepancies['total_discrepancies']:,}\")\n",
    "print(f\"Fields with discrepancies: {len(whp_discrepancies['field_discrepancies'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "ef75e482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entries in LOCAL but NOT in SCRAPED: 340\n",
      "Entries in SCRAPED but NOT in LOCAL: 276\n",
      "\n",
      "Saved 340 entries to whp_missing_in_scraped.csv\n",
      "After filtering: 276 rows\n",
      "Saved 276 entries to whp_missing_in_local.csv\n",
      "\n",
      "Sample entries:\n",
      "       node_id       headword\n",
      "31466   211071   yecaxochitl.\n",
      "31467   211072     texochitl.\n",
      "31468   211073        teicui.\n",
      "31469   211074  Itzcahuatzin.\n",
      "31470   211075  Itzehecatzin.\n",
      "31471   211076   Tlamatzinco.\n",
      "31472   211077     amo ihual.\n",
      "31473   211078    Techahuatl.\n",
      "31474   211079     hualiloti.\n",
      "31475   211080     Cualaztli.\n"
     ]
    }
   ],
   "source": [
    "# Prepare node_id sets for comparison\n",
    "scraped_ids = set(scraped_whp[\"node_id\"].astype(str))\n",
    "local_ids = set(\n",
    "    local_whp[\"Ref\"].astype(str).str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "# Find missing entries\n",
    "missing_in_scraped = local_ids - scraped_ids\n",
    "missing_in_local = scraped_ids - local_ids\n",
    "\n",
    "print(f\"\\nEntries in LOCAL but NOT in SCRAPED: {len(missing_in_scraped):,}\")\n",
    "print(f\"Entries in SCRAPED but NOT in LOCAL: {len(missing_in_local):,}\")\n",
    "\n",
    "if missing_in_scraped:\n",
    "    missing_scraped_df = local_whp[\n",
    "        local_whp[\"Ref\"]\n",
    "        .astype(str)\n",
    "        .str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    "        .isin(missing_in_scraped)\n",
    "    ]\n",
    "    # missing_scraped_df.to_csv('./whp_missing_in_scraped.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nSaved {len(missing_scraped_df):,} entries to whp_missing_in_scraped.csv\")\n",
    "if missing_in_local:\n",
    "    # Ensure types match\n",
    "    missing_local_df = scraped_whp[\n",
    "        scraped_whp[\"node_id\"].astype(str).isin(missing_in_local)\n",
    "    ]\n",
    "    print(f\"After filtering: {len(missing_local_df)} rows\")\n",
    "\n",
    "    if len(missing_local_df) > 0:\n",
    "        # missing_local_df.to_csv('./whp_missing_in_local.csv', index=False, encoding='utf-8-sig')\n",
    "        print(f\"Saved {len(missing_local_df):,} entries to whp_missing_in_local.csv\")\n",
    "        print(f\"\\nSample entries:\")\n",
    "        print(missing_local_df[[\"node_id\", \"headword\"]].head(10))\n",
    "    else:\n",
    "        print(\"No matching rows found - type mismatch issue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ad1652a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows compared: 31,466\n",
      "Actual content differences: 27,474\n",
      "HTML wrapper differences only: 3,992\n",
      "\n",
      "Saved to whp_translation_content_diffs.csv\n"
     ]
    }
   ],
   "source": [
    "# Compare translation_english Content (HTML stripped)\n",
    "\n",
    "\n",
    "scraped_whp_copy = scraped_whp.copy()\n",
    "local_whp_copy = local_whp.copy()\n",
    "scraped_whp_copy[\"node_id\"] = scraped_whp_copy[\"node_id\"].astype(str)\n",
    "local_whp_copy[\"Ref\"] = (\n",
    "    local_whp_copy[\"Ref\"].astype(str).str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "merged = scraped_whp_copy.merge(\n",
    "    local_whp_copy, left_on=\"node_id\", right_on=\"Ref\", how=\"inner\"\n",
    ")\n",
    "\n",
    "merged[\"scraped_text\"] = merged[\"translation_english\"].apply(strip_html_better)\n",
    "merged[\"local_text\"] = merged[\"Principal English Translation\"].apply(strip_html_better)\n",
    "merged[\"content_differs\"] = merged[\"scraped_text\"] != merged[\"local_text\"]\n",
    "\n",
    "content_diff_count = merged[\"content_differs\"].sum()\n",
    "\n",
    "print(f\"Rows compared: {len(merged):,}\")\n",
    "print(f\"Actual content differences: {content_diff_count:,}\")\n",
    "print(f\"HTML wrapper differences only: {len(merged) - content_diff_count:,}\")\n",
    "\n",
    "if content_diff_count > 0:\n",
    "\n",
    "    content_diff_df = merged[merged[\"content_differs\"]][\n",
    "        [\"node_id\", \"headword\", \"scraped_text\", \"local_text\"]\n",
    "    ]\n",
    "    # content_diff_df.to_csv(\"./whp_translation_content_diffs.csv\", index=False)\n",
    "    print(f\"\\nSaved to whp_translation_content_diffs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4b0a6a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0...\n",
      "Processing row 5,000...\n",
      "Processing row 10,000...\n",
      "Processing row 15,000...\n",
      "Processing row 20,000...\n",
      "Processing row 25,000...\n",
      "Processing row 30,000...\n",
      "Total rows processed: 31,805\n",
      "Rows with cross-references: 16,687\n",
      "Total cross-references: 17,286\n",
      "Successful insertions: 17,286\n",
      "Failed insertions: 0\n",
      "Success rate: 100.0%\n",
      "\n",
      "Saved 16,687 changes to whp_crossref_reinsertion_dryrun.csv\n"
     ]
    }
   ],
   "source": [
    "# citation reinsertion dry run\n",
    "local_whp_preview = local_whp.copy()\n",
    "local_whp_preview[\"Ref_clean\"] = (\n",
    "    local_whp_preview[\"Ref\"].astype(str).str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "changes = []\n",
    "stats = {\n",
    "    \"rows_processed\": 0,\n",
    "    \"rows_with_crossrefs\": 0,\n",
    "    \"total_crossrefs\": 0,\n",
    "    \"successful_insertions\": 0,\n",
    "    \"failed_insertions\": 0,\n",
    "}\n",
    "\n",
    "for idx, local_row in local_whp_preview.iterrows():\n",
    "    if idx % 5000 == 0:  # type: ignore\n",
    "        print(f\"Processing row {idx:,}...\")\n",
    "\n",
    "    node_id = local_row[\"Ref_clean\"]\n",
    "    crossrefs = local_row.get(\"Cross_References\", \"\")\n",
    "\n",
    "    if pd.isna(crossrefs) or str(crossrefs).strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    # Get scraped version\n",
    "    scraped_row = scraped_whp[scraped_whp[\"node_id\"].astype(str) == str(node_id)]\n",
    "    if scraped_row.empty:\n",
    "        continue\n",
    "\n",
    "    scraped_row = scraped_row.iloc[0]\n",
    "\n",
    "    # Find positions in scraped\n",
    "    positions = find_crossref_positions_in_scraped_improved(\n",
    "        scraped_row[\"translation_english\"]\n",
    "    )\n",
    "\n",
    "    if not positions:\n",
    "        continue\n",
    "\n",
    "    # Try reinsertion\n",
    "    result_text = local_row[\"Principal English Translation\"]\n",
    "    insertions_successful = 0\n",
    "    insertions_failed = 0\n",
    "\n",
    "    for pos in positions:\n",
    "        result_text, success = reinsert_crossref_improved(\n",
    "            result_text, pos[\"anchor_before\"], pos[\"content\"]\n",
    "        )\n",
    "        if success:\n",
    "            insertions_successful += 1\n",
    "        else:\n",
    "            insertions_failed += 1\n",
    "\n",
    "    # Record change\n",
    "    changes.append(\n",
    "        {\n",
    "            \"node_id\": node_id,\n",
    "            \"headword\": local_row.get(\"Headword\", \"\"),\n",
    "            \"before\": strip_html_better(local_row[\"Principal English Translation\"])[\n",
    "                :200\n",
    "            ],\n",
    "            \"after\": strip_html_better(result_text)[:200],\n",
    "            \"crossrefs_found\": len(positions),\n",
    "            \"successful\": insertions_successful,\n",
    "            \"failed\": insertions_failed,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    stats[\"rows_with_crossrefs\"] += 1\n",
    "    stats[\"total_crossrefs\"] += len(positions)\n",
    "    stats[\"successful_insertions\"] += insertions_successful\n",
    "    stats[\"failed_insertions\"] += insertions_failed\n",
    "    stats[\"rows_processed\"] = idx + 1  # type: ignore\n",
    "\n",
    "print(f\"Total rows processed: {stats['rows_processed']:,}\")\n",
    "print(f\"Rows with cross-references: {stats['rows_with_crossrefs']:,}\")\n",
    "print(f\"Total cross-references: {stats['total_crossrefs']:,}\")\n",
    "print(f\"Successful insertions: {stats['successful_insertions']:,}\")\n",
    "print(f\"Failed insertions: {stats['failed_insertions']:,}\")\n",
    "print(\n",
    "    f\"Success rate: {stats['successful_insertions']/stats['total_crossrefs']*100:.1f}%\"\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "changes_df = pd.DataFrame(changes)\n",
    "changes_df.to_csv(\n",
    "    \"./whp_crossref_reinsertion_dryrun.csv\", index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"\\nSaved {len(changes_df):,} changes to whp_crossref_reinsertion_dryrun.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9eb6ed62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0...\n",
      "Processing row 5,000...\n",
      "Processing row 10,000...\n",
      "Processing row 15,000...\n",
      "Processing row 20,000...\n",
      "Processing row 25,000...\n",
      "Processing row 30,000...\n",
      "Rows updated: 16,687\n",
      "Cross-references reinserted: 17,286\n",
      "Successful insertions: 17,286\n",
      "\n",
      "Saved to whp_with_crossrefs_reinserted.csv\n"
     ]
    }
   ],
   "source": [
    "# Apply Cross-Reference Reinsertion (LIVE RUN)\n",
    "\n",
    "# Create working copy\n",
    "local_whp_updated = local_whp.copy()\n",
    "local_whp_updated[\"Ref_clean\"] = (\n",
    "    local_whp_updated[\"Ref\"].astype(str).str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "stats = {\"rows_updated\": 0, \"crossrefs_reinserted\": 0, \"successful\": 0, \"failed\": 0}\n",
    "\n",
    "for idx, local_row in local_whp_updated.iterrows():\n",
    "    if idx % 5000 == 0:  # type: ignore\n",
    "        print(f\"Processing row {idx:,}...\")\n",
    "\n",
    "    node_id = local_row[\"Ref_clean\"]\n",
    "    crossrefs = local_row.get(\"Cross_References\", \"\")\n",
    "\n",
    "    if pd.isna(crossrefs) or str(crossrefs).strip() == \"\":\n",
    "        continue\n",
    "\n",
    "    scraped_row = scraped_whp[scraped_whp[\"node_id\"].astype(str) == str(node_id)]\n",
    "    if scraped_row.empty:\n",
    "        continue\n",
    "\n",
    "    scraped_row = scraped_row.iloc[0]\n",
    "    positions = find_crossref_positions_in_scraped_improved(\n",
    "        scraped_row[\"translation_english\"]\n",
    "    )\n",
    "\n",
    "    if not positions:\n",
    "        continue\n",
    "\n",
    "    result_text = local_row[\"Principal English Translation\"]\n",
    "    successful = 0\n",
    "\n",
    "    for pos in positions:\n",
    "        result_text, success = reinsert_crossref_improved(\n",
    "            result_text, pos[\"anchor_before\"], pos[\"content\"]\n",
    "        )\n",
    "        if success:\n",
    "            successful += 1\n",
    "\n",
    "    # Update the dataframe\n",
    "    local_whp_updated.at[idx, \"Principal English Translation\"] = result_text  # type: ignore\n",
    "\n",
    "    stats[\"rows_updated\"] += 1\n",
    "    stats[\"crossrefs_reinserted\"] += len(positions)\n",
    "    stats[\"successful\"] += successful\n",
    "\n",
    "print(f\"Rows updated: {stats['rows_updated']:,}\")\n",
    "print(f\"Cross-references reinserted: {stats['crossrefs_reinserted']:,}\")\n",
    "print(f\"Successful insertions: {stats['successful']:,}\")\n",
    "\n",
    "# Save to CSV as checkpoint\n",
    "local_whp_updated.to_csv(\n",
    "    \"./whp_with_crossrefs_reinserted.csv\", index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"\\nSaved to whp_with_crossrefs_reinserted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "34abe256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bibl_positions_with_context(text):\n",
    "    \"\"\"Find <bibl> tags with surrounding context for anchor matching\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return []\n",
    "\n",
    "    pattern = re.compile(r\"<bibl[^>]*>.*?</bibl>\", re.DOTALL | re.IGNORECASE)\n",
    "    positions = []\n",
    "\n",
    "    for match in pattern.finditer(str(text)):\n",
    "        full_tag = match.group(0)\n",
    "\n",
    "        # Get context before the bibl tag (30 chars)\n",
    "        before_start = max(0, match.start() - 50)\n",
    "        before_text = text[before_start : match.start()]\n",
    "\n",
    "        # Get context after the bibl tag (30 chars)\n",
    "        after_end = min(len(text), match.end() + 50)\n",
    "        after_text = text[match.end() : after_end]\n",
    "\n",
    "        # Extract clean anchor words\n",
    "        before_clean = strip_html_better(before_text)\n",
    "        after_clean = strip_html_better(after_text)\n",
    "\n",
    "        # Get last few words before as anchor\n",
    "        words_before = before_clean.split()\n",
    "        anchor_before = (\n",
    "            \" \".join(words_before[-5:])\n",
    "            if len(words_before) >= 5\n",
    "            else \" \".join(words_before)\n",
    "        )\n",
    "\n",
    "        # Get first few words after as secondary anchor\n",
    "        words_after = after_clean.split()\n",
    "        anchor_after = (\n",
    "            \" \".join(words_after[:5])\n",
    "            if len(words_after) >= 5\n",
    "            else \" \".join(words_after)\n",
    "        )\n",
    "\n",
    "        positions.append(\n",
    "            {\n",
    "                \"full_tag\": full_tag,\n",
    "                \"anchor_before\": anchor_before,\n",
    "                \"anchor_after\": anchor_after,\n",
    "                \"start\": match.start(),\n",
    "                \"end\": match.end(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return positions\n",
    "\n",
    "\n",
    "def reinsert_bibl_with_anchor(local_text, anchor_before, anchor_after, bibl_tag):\n",
    "    \"\"\"Insert <bibl> tag using anchor matching\"\"\"\n",
    "    if pd.isna(local_text):\n",
    "        return local_text, False\n",
    "\n",
    "    local_str = str(local_text)\n",
    "    local_stripped = strip_html_better(local_str)\n",
    "\n",
    "    # Try to find position using before anchor\n",
    "    if anchor_before and anchor_before in local_stripped:\n",
    "        # Find the last word of anchor in HTML text\n",
    "        anchor_words = anchor_before.split()\n",
    "        if anchor_words:\n",
    "            last_word = anchor_words[-1].strip(\".,;:!?\")\n",
    "            word_pos = local_str.rfind(last_word)\n",
    "\n",
    "            if word_pos != -1:\n",
    "                insertion_point = word_pos + len(last_word)\n",
    "                reinserted = (\n",
    "                    local_str[:insertion_point]\n",
    "                    + f\" {bibl_tag}\"\n",
    "                    + local_str[insertion_point:]\n",
    "                )\n",
    "                return reinserted, True\n",
    "\n",
    "    # Try after anchor if before failed\n",
    "    if anchor_after and anchor_after in local_stripped:\n",
    "        # Find first word of after anchor\n",
    "        anchor_words = anchor_after.split()\n",
    "        if anchor_words:\n",
    "            first_word = anchor_words[0].strip(\".,;:!?\")\n",
    "            word_pos = local_str.find(first_word)\n",
    "\n",
    "            if word_pos != -1:\n",
    "                # Insert before this word\n",
    "                reinserted = (\n",
    "                    local_str[:word_pos] + f\"{bibl_tag} \" + local_str[word_pos:]\n",
    "                )\n",
    "                return reinserted, True\n",
    "\n",
    "    # Fallback: append before closing tag or at end\n",
    "    if \"</p>\" in local_str:\n",
    "        insertion_point = local_str.rfind(\"</p>\")\n",
    "    else:\n",
    "        insertion_point = len(local_str)\n",
    "\n",
    "    reinserted = (\n",
    "        local_str[:insertion_point] + f\" {bibl_tag}\" + local_str[insertion_point:]\n",
    "    )\n",
    "    return reinserted, True\n",
    "\n",
    "\n",
    "AUTHORITY_MAPPING = {\n",
    "    \"Molina\": \"Alonso de Molina\",\n",
    "    \"Karttunen\": \"Frances Karttunen\",\n",
    "    \"Carochi\": \"Horacio Carochi / English\",\n",
    "    \"Olmos\": \"Andrés de Olmos\",\n",
    "    \"Lockhart\": \"Lockhart’s Nahuatl as Written\",\n",
    "}\n",
    "\n",
    "\n",
    "def get_citations_for_node(node_id, scraped_db):\n",
    "    \"\"\"Get all citations for a node from scraped DB (by column) - UPDATED\"\"\"\n",
    "    citations_by_column = {}\n",
    "\n",
    "    # 1. Get translation_english bibl tags\n",
    "    entry = pd.read_sql(\n",
    "        f\"\"\"\n",
    "        SELECT translation_english \n",
    "        FROM dictionary_entries \n",
    "        WHERE node_id = '{node_id}'\n",
    "    \"\"\",\n",
    "        scraped_db,\n",
    "    )\n",
    "\n",
    "    if not entry.empty and pd.notna(entry.iloc[0][\"translation_english\"]):\n",
    "        bibl_tags = find_bibl_tags(entry.iloc[0][\"translation_english\"])\n",
    "        if bibl_tags:\n",
    "            citations_by_column[\"Principal English Translation\"] = bibl_tags\n",
    "\n",
    "    # 2. Get authority citations from authority_citations table\n",
    "    auth_cites = pd.read_sql(\n",
    "        f\"\"\"\n",
    "        SELECT authority_name, citation_text\n",
    "        FROM authority_citations\n",
    "        WHERE node_id = '{node_id}'\n",
    "        ORDER BY citation_order\n",
    "    \"\"\",\n",
    "        scraped_db,\n",
    "    )\n",
    "\n",
    "    for _, row in auth_cites.iterrows():\n",
    "        auth_name = row[\"authority_name\"]\n",
    "        if auth_name in AUTHORITY_MAPPING:\n",
    "            local_col = AUTHORITY_MAPPING[auth_name]\n",
    "            bibl_tags = find_bibl_tags(row[\"citation_text\"])\n",
    "            if bibl_tags:\n",
    "                if local_col not in citations_by_column:\n",
    "                    citations_by_column[local_col] = []\n",
    "                citations_by_column[local_col].extend(bibl_tags)\n",
    "\n",
    "    # 3. Get attestations with bibl tags\n",
    "    attestations = pd.read_sql(\n",
    "        f\"\"\"\n",
    "        SELECT language, attestation_text\n",
    "        FROM attestations\n",
    "        WHERE node_id = '{node_id}'\n",
    "    \"\"\",\n",
    "        scraped_db,\n",
    "    )\n",
    "\n",
    "    for _, row in attestations.iterrows():\n",
    "        language = row[\"language\"]\n",
    "        attestation_text = row[\"attestation_text\"]\n",
    "\n",
    "        # Map to local column\n",
    "        if language == \"English\":\n",
    "            local_col = \"Attestations from sources in English\"\n",
    "        elif language == \"Spanish\":\n",
    "            local_col = \"Attestations from sources in Spanish\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        bibl_tags = find_bibl_tags(attestation_text)\n",
    "        if bibl_tags:\n",
    "            if local_col not in citations_by_column:\n",
    "                citations_by_column[local_col] = []\n",
    "            citations_by_column[local_col].extend(bibl_tags)\n",
    "\n",
    "    return citations_by_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d448fd54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 31,806 rows\n",
      "Columns: ['Ref', 'Headword', 'Orthographic Variants', 'Principal English Translation', 'Attestations from sources in English', 'Alonso de Molina', 'Frances Karttunen', 'Horacio Carochi / English', 'Andrés de Olmos', 'Lockhart’s Nahuatl as Written']...\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset with cross-refs already reinserted\n",
    "# If you already ran the cross-ref reinsertion and have the CSV:\n",
    "# local_whp_for_citations = pd.read_csv('./whp_with_crossrefs_reinserted.csv', encoding='utf-8-sig')\n",
    "\n",
    "# OR if you want to use the in-memory version from earlier:\n",
    "local_whp_for_citations = local_whp_updated.copy()\n",
    "\n",
    "local_whp_for_citations['Ref_clean'] = local_whp_for_citations['Ref'].astype(str).str.replace(r'^(WHP-|IDIEZ-)', '', regex=True)\n",
    "\n",
    "print(f\"Loaded {len(local_whp_for_citations):,} rows\")\n",
    "print(f\"Columns: {local_whp_for_citations.columns.tolist()[:10]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a23c61e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing row 0... (Found 0 with citations so far)\n",
      "Processing row 5,000... (Found 4473 with citations so far)\n",
      "Processing row 10,000... (Found 9284 with citations so far)\n",
      "Processing row 15,000... (Found 14283 with citations so far)\n",
      "Processing row 20,000... (Found 19281 with citations so far)\n",
      "Processing row 25,000... (Found 24056 with citations so far)\n",
      "Processing row 30,000... (Found 28560 with citations so far)\n",
      "CITATION DRY RUN SUMMARY\n",
      "Total rows processed: 31,806\n",
      "Rows with citations: 29,373\n",
      "Total citations: 44,003\n",
      "Successful insertions: 44,003\n",
      "Failed insertions: 0\n",
      "Success rate: 100.0%\n",
      "\n",
      "Citations by column:\n",
      "  Alonso de Molina: 23,150\n",
      "  Andrés de Olmos: 103\n",
      "  Attestations from sources in English: 12,986\n",
      "  Frances Karttunen: 6,427\n",
      "  Horacio Carochi / English: 414\n",
      "  Lockhart’s Nahuatl as Written: 923\n",
      "\n",
      "Saved to whp_citation_reinsertion_dryrun.csv\n"
     ]
    }
   ],
   "source": [
    "# dry run for citation reinsertion\n",
    "citation_changes = []\n",
    "stats = {\n",
    "    \"rows_processed\": 0,\n",
    "    \"rows_with_citations\": 0,\n",
    "    \"total_citations\": 0,\n",
    "    \"citations_by_column\": {},\n",
    "    \"successful_insertions\": 0,\n",
    "    \"failed_insertions\": 0,\n",
    "}\n",
    "\n",
    "for idx, local_row in local_whp_for_citations.iterrows():\n",
    "    stats[\"rows_processed\"] += 1\n",
    "\n",
    "    if idx % 5000 == 0:  # type: ignore\n",
    "        print(\n",
    "            f\"Processing row {idx:,}... (Found {stats['rows_with_citations']} with citations so far)\"\n",
    "        )\n",
    "\n",
    "    node_id = local_row[\"Ref_clean\"]\n",
    "\n",
    "    # Get citations from scraped DB\n",
    "    citations_by_column = get_citations_for_node(node_id, scraped_db)\n",
    "\n",
    "    if not citations_by_column:\n",
    "        continue\n",
    "\n",
    "    row_updates = {}\n",
    "\n",
    "    for col_name, bibl_list in citations_by_column.items():\n",
    "        if col_name not in local_row.index:\n",
    "            continue\n",
    "\n",
    "        current_value = local_row[col_name]\n",
    "        if pd.isna(current_value):  # type: ignore\n",
    "            current_value = \"\"\n",
    "\n",
    "        # Get scraped text for this column\n",
    "        if col_name == \"Principal English Translation\":\n",
    "            scraped_row = scraped_whp[scraped_whp[\"node_id\"] == str(node_id)]\n",
    "            if scraped_row.empty:\n",
    "                continue\n",
    "            scraped_text = scraped_row.iloc[0][\"translation_english\"]\n",
    "            \n",
    "        elif col_name in [\"Attestations from sources in English\", \"Attestations from sources in Spanish\"]:\n",
    "            # Handle attestation columns\n",
    "            language = \"English\" if \"English\" in col_name else \"Spanish\"\n",
    "            attestation = pd.read_sql(f\"\"\"\n",
    "                SELECT attestation_text \n",
    "                FROM attestations \n",
    "                WHERE node_id = '{node_id}' AND language = '{language}'\n",
    "                LIMIT 1\n",
    "            \"\"\", scraped_db)\n",
    "            \n",
    "            if attestation.empty:\n",
    "                continue\n",
    "            scraped_text = attestation.iloc[0][\"attestation_text\"]\n",
    "            \n",
    "        else:\n",
    "            # Authority column\n",
    "            auth_name = [k for k, v in AUTHORITY_MAPPING.items() if v == col_name]\n",
    "            if not auth_name:\n",
    "                continue\n",
    "\n",
    "            auth_cite = pd.read_sql(\n",
    "                f\"\"\"\n",
    "                SELECT citation_text \n",
    "                FROM authority_citations \n",
    "                WHERE node_id = '{node_id}' AND authority_name = '{auth_name[0]}'\n",
    "                LIMIT 1\n",
    "            \"\"\",\n",
    "                scraped_db,\n",
    "            )\n",
    "\n",
    "            if auth_cite.empty:\n",
    "                continue\n",
    "            scraped_text = auth_cite.iloc[0][\"citation_text\"]\n",
    "\n",
    "        # Find positions\n",
    "        positions = find_bibl_positions_with_context(scraped_text)\n",
    "\n",
    "        if not positions:\n",
    "            continue\n",
    "\n",
    "        # Reinsert each bibl tag\n",
    "        updated_value = str(current_value)\n",
    "        successful = 0\n",
    "        failed = 0\n",
    "\n",
    "        for pos in positions:\n",
    "            updated_value, success = reinsert_bibl_with_anchor(\n",
    "                updated_value,\n",
    "                pos[\"anchor_before\"],\n",
    "                pos[\"anchor_after\"],\n",
    "                pos[\"full_tag\"],\n",
    "            )\n",
    "            if success:\n",
    "                successful += 1\n",
    "            else:\n",
    "                failed += 1\n",
    "\n",
    "        row_updates[col_name] = {\n",
    "            \"before\": str(current_value)[:100],\n",
    "            \"after\": updated_value[:100],\n",
    "            \"citations_added\": len(positions),\n",
    "            \"successful\": successful,\n",
    "            \"failed\": failed,\n",
    "        }\n",
    "\n",
    "        if col_name not in stats[\"citations_by_column\"]:\n",
    "            stats[\"citations_by_column\"][col_name] = 0\n",
    "        stats[\"citations_by_column\"][col_name] += len(positions)\n",
    "        stats[\"total_citations\"] += len(positions)\n",
    "        stats[\"successful_insertions\"] += successful\n",
    "        stats[\"failed_insertions\"] += failed\n",
    "\n",
    "    if row_updates:\n",
    "        citation_changes.append(\n",
    "            {\n",
    "                \"node_id\": node_id,\n",
    "                \"headword\": local_row.get(\"Headword\", \"\"),\n",
    "                \"updates\": row_updates,\n",
    "            }\n",
    "        )\n",
    "        stats[\"rows_with_citations\"] += 1\n",
    "\n",
    "print(\"CITATION DRY RUN SUMMARY\")\n",
    "print(f\"Total rows processed: {stats['rows_processed']:,}\")\n",
    "print(f\"Rows with citations: {stats['rows_with_citations']:,}\")\n",
    "print(f\"Total citations: {stats['total_citations']:,}\")\n",
    "print(f\"Successful insertions: {stats['successful_insertions']:,}\")\n",
    "print(f\"Failed insertions: {stats['failed_insertions']:,}\")\n",
    "if stats[\"total_citations\"] > 0:\n",
    "    print(\n",
    "        f\"Success rate: {stats['successful_insertions']/stats['total_citations']*100:.1f}%\"\n",
    "    )\n",
    "print(f\"\\nCitations by column:\")\n",
    "for col, count in sorted(stats[\"citations_by_column\"].items()):\n",
    "    print(f\"  {col}: {count:,}\")\n",
    "\n",
    "changes_df = pd.DataFrame(citation_changes)\n",
    "changes_df.to_csv(\n",
    "    \"./whp_citation_reinsertion_dryrun.csv\", index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"\\nSaved to whp_citation_reinsertion_dryrun.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "bf633db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APPLYING CITATION REINSERTION (LIVE)\n",
      "Processing row 0...\n",
      "Processing row 5,000...\n",
      "Processing row 10,000...\n",
      "Processing row 15,000...\n",
      "Processing row 20,000...\n",
      "Processing row 25,000...\n",
      "Processing row 30,000...\n",
      "CITATION REINSERTION COMPLETE\n",
      "Rows updated: 29,373\n",
      "Citations inserted: 44,003\n",
      "\n",
      "Saved final dataset to whp_with_all_reinserted.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"APPLYING CITATION REINSERTION (LIVE)\")\n",
    "local_whp_final = local_whp_for_citations.copy()\n",
    "\n",
    "apply_stats = {\"rows_updated\": 0, \"citations_inserted\": 0}\n",
    "\n",
    "for idx, local_row in local_whp_final.iterrows():\n",
    "    if idx % 5000 == 0: #type: ignore\n",
    "        print(f\"Processing row {idx:,}...\")\n",
    "\n",
    "    node_id = local_row[\"Ref_clean\"]\n",
    "    citations_by_column = get_citations_for_node(node_id, scraped_db)\n",
    "\n",
    "    if not citations_by_column:\n",
    "        continue\n",
    "\n",
    "    row_updated = False\n",
    "\n",
    "    for col_name, bibl_list in citations_by_column.items():\n",
    "        if col_name not in local_row.index:\n",
    "            continue\n",
    "\n",
    "        current_value = local_row[col_name]\n",
    "        if pd.isna(current_value): #type: ignore\n",
    "            current_value = \"\"\n",
    "\n",
    "        # Get scraped text\n",
    "        if col_name == \"Principal English Translation\":\n",
    "            scraped_row = scraped_whp[scraped_whp[\"node_id\"] == str(node_id)]\n",
    "            if scraped_row.empty:\n",
    "                continue\n",
    "            scraped_text = scraped_row.iloc[0][\"translation_english\"]\n",
    "            \n",
    "        elif col_name in [\"Attestations from sources in English\", \"Attestations from sources in Spanish\"]:\n",
    "            # Handle attestation columns\n",
    "            language = \"English\" if \"English\" in col_name else \"Spanish\"\n",
    "            attestation = pd.read_sql(f\"\"\"\n",
    "                SELECT attestation_text \n",
    "                FROM attestations \n",
    "                WHERE node_id = '{node_id}' AND language = '{language}'\n",
    "                LIMIT 1\n",
    "            \"\"\", scraped_db)\n",
    "            \n",
    "            if attestation.empty:\n",
    "                continue\n",
    "            scraped_text = attestation.iloc[0][\"attestation_text\"]\n",
    "        else:\n",
    "            auth_name = [k for k, v in AUTHORITY_MAPPING.items() if v == col_name]\n",
    "            if not auth_name:\n",
    "                continue\n",
    "            auth_cite = pd.read_sql(\n",
    "                f\"\"\"\n",
    "                SELECT citation_text \n",
    "                FROM authority_citations \n",
    "                WHERE node_id = '{node_id}' AND authority_name = '{auth_name[0]}'\n",
    "                LIMIT 1\n",
    "            \"\"\",\n",
    "                scraped_db,\n",
    "            )\n",
    "            if auth_cite.empty:\n",
    "                continue\n",
    "            scraped_text = auth_cite.iloc[0][\"citation_text\"]\n",
    "\n",
    "        positions = find_bibl_positions_with_context(scraped_text)\n",
    "        if not positions:\n",
    "            continue\n",
    "\n",
    "        updated_value = str(current_value)\n",
    "        for pos in positions:\n",
    "            updated_value, _ = reinsert_bibl_with_anchor(\n",
    "                updated_value,\n",
    "                pos[\"anchor_before\"],\n",
    "                pos[\"anchor_after\"],\n",
    "                pos[\"full_tag\"],\n",
    "            )\n",
    "            apply_stats[\"citations_inserted\"] += 1\n",
    "\n",
    "        local_whp_final.at[idx, col_name] = updated_value #type: ignore\n",
    "        row_updated = True\n",
    "\n",
    "    if row_updated:\n",
    "        apply_stats[\"rows_updated\"] += 1\n",
    "\n",
    "print(\"CITATION REINSERTION COMPLETE\")\n",
    "print(f\"Rows updated: {apply_stats['rows_updated']:,}\")\n",
    "print(f\"Citations inserted: {apply_stats['citations_inserted']:,}\")\n",
    "\n",
    "# Save final dataset\n",
    "local_whp_final.to_csv(\n",
    "    \"./whp_with_all_reinserted.csv\", index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(\"\\nSaved final dataset to whp_with_all_reinserted.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "03b0d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraped_db.close()\n",
    "# local_db.close()\n",
    "# print(\"Database connections closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nahuaLEX_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
