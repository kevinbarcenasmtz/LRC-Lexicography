{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5346fcb6",
   "metadata": {},
   "source": [
    "# Validation and correction\n",
    "\n",
    "As we've continued working with the scraped data versus the changes we've made from the initial csv files given to the LRC. We've done validation and corrections to the initial csv files that are not reflected in the scraped datasets. So now since we've uploaded our data into SQLite the next step will be to cross validate and update the SQLite. Which will be done below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cef09ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup and imports\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Optional, Union, Any\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af11e03d",
   "metadata": {},
   "source": [
    "note the original local datasets given to the LRC initially are hosted in the nahuatl_processing.db under sqLiteDb specifically under the table name: checkpoint_after_empty_p_tag_removal_20251002. For the scraped data, that is under scrapedDataDb under the nahuatl.db file. The schema is under config/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8ed35dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data_dir = Path(\"../../../data/scrapedDataDb/\")\n",
    "local_data_dir = Path(\"../../../data/sqLiteDb/\")\n",
    "\n",
    "if not scraped_data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Scraped database not found: {scraped_data_dir}\")\n",
    "if not local_data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Local database not found: {local_data_dir}\")\n",
    "\n",
    "# Database connection\n",
    "scraped_db = sqlite3.connect(scraped_data_dir / \"nahuatl.db\")\n",
    "local_db = sqlite3.connect(local_data_dir / \"nahuatl_processing.db\")\n",
    "\n",
    "\n",
    "# read in the table(s), for the local_db it's only one table (actually two one for the WHP dataset and one for the IDIEZ dataset) while for the \n",
    "# scraped_db there are multiple tables due to the relationl structure we want to keep\n",
    "tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", local_db)\n",
    "WHP_TABLE = \"checkpoint_after_missing_entries_added_20251111_184145\"\n",
    "IDIEZ_TABLE = \"IDIEZ_modern_nahuatl-all-2024-03-27T09-45-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dcfb3631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map scraped DB fields to local DB fields for WHP data\n",
    "WHP_FIELD_MAPPING = {\n",
    "    # Scraped field: Local field\n",
    "    'node_id': 'Ref',\n",
    "    'headword': 'Headword',\n",
    "    'orthographic_variants': 'Orthographic Variants',\n",
    "    'translation_english': 'Principal English Translation',\n",
    "    'spanish_loanword': 'Spanish Loanword',\n",
    "    # Authority fields (stored in authority_citations table in scraped DB)\n",
    "    'authority_molina': 'Alonso de Molina',\n",
    "    'authority_karttunen': 'Frances Karttunen',\n",
    "    'authority_carochi': 'Horacio Carochi / English',\n",
    "    'authority_olmos': 'Andrés de Olmos',\n",
    "    'authority_lockhart': \"Lockhart’s Nahuatl as Written\",\n",
    "    # Attestations (stored in attestations table in scraped DB)\n",
    "    'attestations_english': 'Attestations from sources in English',\n",
    "    'attestations_spanish': 'Attestations from sources in Spanish',\n",
    "    # Metadata\n",
    "    'themes': 'themes',\n",
    "}\n",
    "\n",
    "# Map scraped DB fields to local DB fields for IDIEZ data\n",
    "IDIEZ_FIELD_MAPPING = {\n",
    "    'node_id': 'Ref',\n",
    "    'headword_idiez': 'tlahtolli',\n",
    "    'translation_english_idiez': 'IDIEZ traduc. inglés',\n",
    "    'definition_nahuatl_idiez': 'IDIEZ def. náhuatl',\n",
    "    'definition_spanish_idiez': 'IDIEZ def. español',\n",
    "    'morfologia_idiez': 'IDIEZ morfología',\n",
    "    'gramatica_idiez': 'IDIEZ gramática',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34c55cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_dataframes(df_scraped, df_local, field_mapping, key_field=\"node_id\"):\n",
    "    scraped_key = field_mapping.get(key_field, key_field)\n",
    "    df_scraped = df_scraped.copy()\n",
    "    df_local = df_local.copy()\n",
    "    df_scraped[key_field] = df_scraped[key_field].astype(str)\n",
    "    df_local[scraped_key] = df_local[scraped_key].astype(str)\n",
    "    df_local[scraped_key] = (\n",
    "        df_local[scraped_key].astype(str).str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    "    )\n",
    "    merged = df_scraped.merge(\n",
    "        df_local,\n",
    "        left_on=key_field,\n",
    "        right_on=scraped_key,\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_scraped\", \"_local\"),\n",
    "    )\n",
    "    print(f\"Rows in scraped DB: {len(df_scraped):,}\")\n",
    "    print(f\"Rows in local DB: {len(df_local):,}\")\n",
    "    print(f\"Rows matched: {len(merged):,}\")\n",
    "    discrepancies = {\n",
    "        \"field_discrepancies\": {},\n",
    "        \"total_discrepancies\": 0,\n",
    "        \"rows_compared\": len(merged),\n",
    "        \"sample_discrepancies\": [],\n",
    "    }\n",
    "    for scraped_field, local_field in field_mapping.items():\n",
    "        if scraped_field == key_field:\n",
    "            continue\n",
    "        if (\n",
    "            scraped_field not in df_scraped.columns\n",
    "            or local_field not in df_local.columns\n",
    "        ):\n",
    "            print(f\"Skipping {scraped_field} (not in both datasets)\")\n",
    "            continue\n",
    "        scraped_col = (\n",
    "            f\"{scraped_field}_scraped\"\n",
    "            if scraped_field in df_local.columns\n",
    "            else scraped_field\n",
    "        )\n",
    "        local_col = (\n",
    "            f\"{local_field}_local\" if local_field in df_scraped.columns else local_field\n",
    "        )\n",
    "        scraped_values = merged[scraped_col].fillna(\"\").astype(str).str.strip()\n",
    "        local_values = merged[local_col].fillna(\"\").astype(str).str.strip()\n",
    "        \n",
    "        scraped_values = scraped_values.str.split().str.join(' ')\n",
    "        local_values = local_values.str.split().str.join(' ')\n",
    "        \n",
    "        merged[\"diff\"] = ~(\n",
    "            (scraped_values == local_values)\n",
    "            | (merged[scraped_col].isna() & merged[local_col].isna())\n",
    "        )\n",
    "        diff_count = merged[\"diff\"].sum()\n",
    "        if diff_count > 0:\n",
    "            print(f\"{scraped_field:30} {diff_count:>6,} discrepancies\")\n",
    "            discrepancies[\"field_discrepancies\"][scraped_field] = {\n",
    "                \"count\": int(diff_count),\n",
    "                \"local_field\": local_field,\n",
    "                \"sample_rows\": merged[merged[\"diff\"]][key_field].head(10).tolist(),\n",
    "            }\n",
    "            discrepancies[\"total_discrepancies\"] += int(diff_count)\n",
    "            if len(discrepancies[\"sample_discrepancies\"]) < 5:\n",
    "                sample = merged[merged[\"diff\"]].iloc[0]\n",
    "                discrepancies[\"sample_discrepancies\"].append(\n",
    "                    {\n",
    "                        \"node_id\": sample[key_field],\n",
    "                        \"field\": scraped_field,\n",
    "                        \"scraped_value\": str(sample[scraped_col])[:100],\n",
    "                        \"local_value\": str(sample[local_col])[:100],\n",
    "                    }\n",
    "                )\n",
    "        else:\n",
    "            print(f\"{scraped_field:30} all match\")\n",
    "    print(f\"Total discrepancies: {discrepancies['total_discrepancies']:,}\")\n",
    "    return discrepancies\n",
    "\n",
    "def create_update_dataframe(scraped_df, local_df, field_mapping, key_field=\"node_id\"):\n",
    "    scraped_key = field_mapping.get(key_field, key_field)\n",
    "    scraped_df = scraped_df.copy()\n",
    "    local_df = local_df.copy()\n",
    "    scraped_df[key_field] = scraped_df[key_field].astype(str)\n",
    "    local_df[scraped_key] = (\n",
    "        local_df[scraped_key].astype(str).str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    "    )\n",
    "    merged = scraped_df.merge(\n",
    "        local_df,\n",
    "        left_on=key_field,\n",
    "        right_on=scraped_key,\n",
    "        how=\"inner\",\n",
    "        suffixes=(\"_scraped\", \"_local\"),\n",
    "    )\n",
    "    updates = []\n",
    "    for scraped_field, local_field in field_mapping.items():\n",
    "        if scraped_field == key_field:\n",
    "            continue\n",
    "        if (\n",
    "            scraped_field not in scraped_df.columns\n",
    "            or local_field not in local_df.columns\n",
    "        ):\n",
    "            continue\n",
    "        scraped_col = (\n",
    "            f\"{scraped_field}_scraped\"\n",
    "            if scraped_field in local_df.columns\n",
    "            else scraped_field\n",
    "        )\n",
    "        local_col = (\n",
    "            f\"{local_field}_local\" if local_field in scraped_df.columns else local_field\n",
    "        )\n",
    "        scraped_values = merged[scraped_col].fillna(\"\").astype(str).str.strip()\n",
    "        local_values = merged[local_col].fillna(\"\").astype(str).str.strip()\n",
    "        \n",
    "        scraped_values = scraped_values.str.split().str.join(' ')\n",
    "        local_values = local_values.str.split().str.join(' ')\n",
    "        \n",
    "        diff_mask = ~(\n",
    "            (scraped_values == local_values)\n",
    "            | (merged[scraped_col].isna() & merged[local_col].isna())\n",
    "        )\n",
    "        diff_rows = merged[diff_mask]\n",
    "        for idx, row in diff_rows.iterrows():\n",
    "            current_stripped = (\n",
    "                str(row[scraped_col]).strip() if pd.notna(row[scraped_col]) else \"\"\n",
    "            )\n",
    "            new_stripped = (\n",
    "                str(row[local_col]).strip() if pd.notna(row[local_col]) else \"\"\n",
    "            )\n",
    "            updates.append(\n",
    "                {\n",
    "                    \"node_id\": row[key_field],\n",
    "                    \"field\": scraped_field,\n",
    "                    \"current_value\": current_stripped,\n",
    "                    \"new_value\": new_stripped,\n",
    "                    \"action\": \"UPDATE\",\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(updates)\n",
    "\n",
    "def apply_updates(updates_df, conn, table_name=\"dictionary_entries\", dry_run=True):\n",
    "\n",
    "    print(f\"APPLYING UPDATES TO {table_name}\")\n",
    "    print(f\"Mode: {'DRY RUN' if dry_run else 'LIVE UPDATES'}\")\n",
    "    stats = {\n",
    "        \"total_updates\": 0,\n",
    "        \"successful_updates\": 0,\n",
    "        \"failed_updates\": 0,\n",
    "        \"updates_by_field\": defaultdict(int),\n",
    "    }\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "    grouped = updates_df.groupby(\"node_id\")\n",
    "\n",
    "    for node_id, group in grouped:\n",
    "        try:\n",
    "            set_clauses = []\n",
    "            values = []\n",
    "\n",
    "            for _, row in group.iterrows():\n",
    "                set_clauses.append(f\"{row['field']} = ?\")\n",
    "                values.append(row[\"new_value\"])\n",
    "                stats[\"updates_by_field\"][row[\"field\"]] += 1\n",
    "\n",
    "            values.append(node_id)\n",
    "            sql = f\"UPDATE {table_name} SET {', '.join(set_clauses)} WHERE node_id = ?\"\n",
    "\n",
    "            if not dry_run:\n",
    "                cursor.execute(sql, values)\n",
    "\n",
    "            stats[\"successful_updates\"] += len(group)\n",
    "            stats[\"total_updates\"] += len(group)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error updating node_id {node_id}: {e}\")\n",
    "            stats[\"failed_updates\"] += len(group)\n",
    "            stats[\"total_updates\"] += len(group)\n",
    "\n",
    "    if not dry_run:\n",
    "        conn.commit()\n",
    "        print(\"Changes committed\")\n",
    "    else:\n",
    "        print(\"Dry run complete - no changes made\")\n",
    "\n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Total updates: {stats['total_updates']:,}\")\n",
    "    print(f\"  Successful: {stats['successful_updates']:,}\")\n",
    "    print(f\"  Failed: {stats['failed_updates']:,}\")\n",
    "    print(f\"\\nUpdates by field:\")\n",
    "    for field, count in stats[\"updates_by_field\"].items():\n",
    "        print(f\"  - {field}: {count:,}\")\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf76dbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_html_better(text):\n",
    "    \"\"\"More aggressive HTML stripping\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "    text = re.sub(r'\\w+=\"[^\"]*\"', \"\", text)\n",
    "    text = re.sub(r\"\\w+=\\'[^\\']*\\'\", \"\", text)\n",
    "\n",
    "    text = \" \".join(text.split())\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48516e8d",
   "metadata": {},
   "source": [
    "for me it methodically makes sense to go down the tables that nahuat.db has (check config/schema.sql) and then check each one against the WHP_table_name and the IDIEZ_table_name as such let's begin with the largest tables first and build up from the tables. \n",
    "it would also be smart that as we cross validate we proceed to investigate where the cross references columns actually come from\n",
    "also a side note since we've done no manual corrections or fixes to the IDIEZ fields we can begin with validating local IDIEZ with scraped IDIEZ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a66c4992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6,848 IDIEZ/HYBRID entries from scraped DB\n",
      "Loaded 6,848 IDIEZ entries from local DB\n",
      "Rows in scraped DB: 6,848\n",
      "Rows in local DB: 6,848\n",
      "Rows matched: 6,847\n",
      "headword_idiez                      1 discrepancies\n",
      "translation_english_idiez      all match\n",
      "definition_nahuatl_idiez       all match\n",
      "definition_spanish_idiez       all match\n",
      "morfologia_idiez               all match\n",
      "gramatica_idiez                     1 discrepancies\n",
      "Total discrepancies: 2\n",
      "\n",
      "Sample IDIEZ discrepancies:\n",
      "\n",
      "1. node_id=203433, field=headword_idiez\n",
      "   Scraped: chiya\n",
      "   Local:   chiya.\n",
      "\n",
      "2. node_id=203433, field=gramatica_idiez\n",
      "   Scraped: None\n",
      "   Local:   tlach2.\n"
     ]
    }
   ],
   "source": [
    "scraped_idiez = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        node_id,\n",
    "        headword_idiez,\n",
    "        translation_english_idiez,\n",
    "        definition_nahuatl_idiez,\n",
    "        definition_spanish_idiez,\n",
    "        morfologia_idiez,\n",
    "        gramatica_idiez,\n",
    "        source_dataset\n",
    "    FROM dictionary_entries\n",
    "    WHERE source_dataset IN ('IDIEZ', 'HYBRID')\n",
    "\"\"\",\n",
    "    scraped_db,\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(scraped_idiez):,} IDIEZ/HYBRID entries from scraped DB\")\n",
    "\n",
    "# Load IDIEZ from local DB\n",
    "local_idiez = pd.read_sql(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        Ref,\n",
    "        tlahtolli,\n",
    "        \"IDIEZ traduc. inglés\",\n",
    "        \"IDIEZ def. náhuatl\",\n",
    "        \"IDIEZ def. español\",\n",
    "        \"IDIEZ morfología\",\n",
    "        \"IDIEZ gramática\"\n",
    "    FROM [{IDIEZ_TABLE}]\n",
    "\"\"\",\n",
    "    local_db,\n",
    ")\n",
    "\n",
    "print(f\"Loaded {len(local_idiez):,} IDIEZ entries from local DB\")\n",
    "\n",
    "# Compare\n",
    "idiez_discrepancies = compare_dataframes(\n",
    "    scraped_idiez, local_idiez, IDIEZ_FIELD_MAPPING, key_field=\"node_id\"\n",
    ")\n",
    "\n",
    "# Show samples\n",
    "if idiez_discrepancies[\"sample_discrepancies\"]:\n",
    "    print(\"\\nSample IDIEZ discrepancies:\")\n",
    "    for i, sample in enumerate(idiez_discrepancies[\"sample_discrepancies\"][:5], 1):\n",
    "        print(f\"\\n{i}. node_id={sample['node_id']}, field={sample['field']}\")\n",
    "        print(f\"   Scraped: {sample['scraped_value']}\")\n",
    "        print(f\"   Local:   {sample['local_value']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d5a8706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating IDIEZ update report...\n",
      "Total IDIEZ updates needed: 2\n",
      "\n",
      "Updates by field:\n",
      "field\n",
      "headword_idiez     1\n",
      "gramatica_idiez    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Update report saved to: idiez_updates_needed.csv\n"
     ]
    }
   ],
   "source": [
    "# creating the IDIEZ report\n",
    "if idiez_discrepancies[\"total_discrepancies\"] > 0:\n",
    "    print(\"\\nCreating IDIEZ update report...\")\n",
    "\n",
    "    idiez_updates = create_update_dataframe(\n",
    "        scraped_idiez, local_idiez, IDIEZ_FIELD_MAPPING\n",
    "    )\n",
    "\n",
    "    print(f\"Total IDIEZ updates needed: {len(idiez_updates):,}\")\n",
    "    print(\"\\nUpdates by field:\")\n",
    "    print(idiez_updates[\"field\"].value_counts())\n",
    "\n",
    "    idiez_updates.to_csv('idiez_updates_needed.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"\\nUpdate report saved to: idiez_updates_needed.csv\")\n",
    "else:\n",
    "    print(\"\\nNo IDIEZ updates needed - data matches perfectly!\")\n",
    "    idiez_updates = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6a7dbd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 32,057 WHP entries from scraped DB (including HYBRID)\n",
      "Loaded 32,082 WHP entries from local DB\n"
     ]
    }
   ],
   "source": [
    "scraped_whp = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        node_id,\n",
    "        headword,\n",
    "        orthographic_variants,\n",
    "        translation_english,\n",
    "        spanish_loanword,\n",
    "        source_dataset\n",
    "    FROM dictionary_entries\n",
    "    WHERE source_dataset IN ('WHP', 'HYBRID')\n",
    "\"\"\",\n",
    "    scraped_db,\n",
    ")\n",
    "print(f\"Loaded {len(scraped_whp):,} WHP entries from scraped DB (including HYBRID)\")\n",
    "\n",
    "local_whp = pd.read_sql(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        Ref,\n",
    "        Headword,\n",
    "        \"Orthographic Variants\",\n",
    "        \"Principal English Translation\",\n",
    "        \"Attestations from sources in English\",\n",
    "        \"Attestations from sources in Spanish\",\n",
    "        \"Alonso de Molina\",\n",
    "        \"Frances Karttunen\",\n",
    "        \"Horacio Carochi / English\",\n",
    "        \"Andrés de Olmos\",\n",
    "        \"Lockhart’s Nahuatl as Written\",\n",
    "        \"themes\",\n",
    "        \"Spanish Loanword\",\n",
    "        \"Citations\",\n",
    "        \"Number_of_Citations\",\n",
    "        \"Cross_References\",\n",
    "        \"Number_of_Cross_References\",\n",
    "        \"CrossRef_Types\"\n",
    "    FROM [{WHP_TABLE}]\n",
    "\"\"\",\n",
    "    local_db,\n",
    ")\n",
    "print(f\"Loaded {len(local_whp):,} WHP entries from local DB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a16b28",
   "metadata": {},
   "source": [
    "so as u can see from the output some of the changes seen are html differences so we need to look at the actual content of the cells to see if there is any difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef75e482",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WHP COMPARISON ===\n",
      "Entries in LOCAL but NOT in SCRAPED: 26\n",
      "Entries in SCRAPED but NOT in LOCAL: 1\n",
      "Saved 26 WHP entries to whp_missing_in_scraped.csv\n",
      "Saved 1 WHP entries to whp_missing_in_local.csv\n",
      "\n",
      "=== IDIEZ COMPARISON ===\n",
      "Entries in LOCAL but NOT in SCRAPED: 1\n",
      "Entries in SCRAPED but NOT in LOCAL: 1\n",
      "Saved 1 IDIEZ entries to idiez_missing_in_scraped.csv\n",
      "Saved 1 IDIEZ entries to idiez_missing_in_local.csv\n"
     ]
    }
   ],
   "source": [
    "WHP_TABLE = \"checkpoint_after_missing_entries_added_20251111_184145\"\n",
    "IDIEZ_TABLE = \"IDIEZ_modern_nahuatl-all-2024-03-27T09-45-31\"\n",
    "\n",
    "# WHP comparison\n",
    "scraped_whp_ids = set(scraped_whp[\"node_id\"].astype(str))\n",
    "local_whp_ids = set(\n",
    "    local_whp[\"Ref\"].astype(str).str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "missing_in_scraped_whp = local_whp_ids - scraped_whp_ids\n",
    "missing_in_local_whp = scraped_whp_ids - local_whp_ids\n",
    "\n",
    "print(\"=== WHP COMPARISON ===\")\n",
    "print(f\"Entries in LOCAL but NOT in SCRAPED: {len(missing_in_scraped_whp):,}\")\n",
    "print(f\"Entries in SCRAPED but NOT in LOCAL: {len(missing_in_local_whp):,}\")\n",
    "\n",
    "if missing_in_scraped_whp:\n",
    "    missing_scraped_df = local_whp[\n",
    "        local_whp[\"Ref\"]\n",
    "        .astype(str)\n",
    "        .str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    "        .isin(missing_in_scraped_whp)\n",
    "    ]\n",
    "    missing_scraped_df.to_csv('./whp_missing_in_scraped.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved {len(missing_scraped_df):,} WHP entries to whp_missing_in_scraped.csv\")\n",
    "    \n",
    "if missing_in_local_whp:\n",
    "    missing_local_df = scraped_whp[\n",
    "        scraped_whp[\"node_id\"].astype(str).isin(missing_in_local_whp)\n",
    "    ]\n",
    "    if len(missing_local_df) > 0:\n",
    "        missing_local_df.to_csv('./whp_missing_in_local.csv', index=False, encoding='utf-8-sig')\n",
    "        print(f\"Saved {len(missing_local_df):,} WHP entries to whp_missing_in_local.csv\")\n",
    "\n",
    "# IDIEZ comparison\n",
    "scraped_idiez_ids = set(scraped_idiez[\"node_id\"].astype(str))\n",
    "local_idiez_ids = set(\n",
    "    local_idiez[\"Ref\"].astype(str).str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "missing_in_scraped_idiez = local_idiez_ids - scraped_idiez_ids\n",
    "missing_in_local_idiez = scraped_idiez_ids - local_idiez_ids\n",
    "\n",
    "print(\"\\n=== IDIEZ COMPARISON ===\")\n",
    "print(f\"Entries in LOCAL but NOT in SCRAPED: {len(missing_in_scraped_idiez):,}\")\n",
    "print(f\"Entries in SCRAPED but NOT in LOCAL: {len(missing_in_local_idiez):,}\")\n",
    "\n",
    "if missing_in_scraped_idiez:\n",
    "    missing_scraped_df = local_idiez[\n",
    "        local_idiez[\"Ref\"]\n",
    "        .astype(str)\n",
    "        .str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    "        .isin(missing_in_scraped_idiez)\n",
    "    ]\n",
    "    missing_scraped_df.to_csv('./idiez_missing_in_scraped.csv', index=False, encoding='utf-8-sig')\n",
    "    print(f\"Saved {len(missing_scraped_df):,} IDIEZ entries to idiez_missing_in_scraped.csv\")\n",
    "    \n",
    "if missing_in_local_idiez:\n",
    "    missing_local_df = scraped_idiez[\n",
    "        scraped_idiez[\"node_id\"].astype(str).isin(missing_in_local_idiez)\n",
    "    ]\n",
    "    if len(missing_local_df) > 0:\n",
    "        missing_local_df.to_csv('./idiez_missing_in_local.csv', index=False, encoding='utf-8-sig')\n",
    "        print(f\"Saved {len(missing_local_df):,} IDIEZ entries to idiez_missing_in_local.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65e7480e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MissingEntryGatherer:\n",
    "    def __init__(self, scraped_db_path: str, local_db_path: str):\n",
    "        self.scraped_db = sqlite3.connect(scraped_db_path)\n",
    "        self.local_db = sqlite3.connect(local_db_path)\n",
    "        \n",
    "    def get_missing_entry_ids(self, dataset: str = 'WHP') -> List[str]:\n",
    "        if dataset == 'WHP':\n",
    "            scraped_df = pd.read_sql(\n",
    "                \"SELECT node_id FROM dictionary_entries WHERE source_dataset = 'WHP'\",\n",
    "                self.scraped_db\n",
    "            )\n",
    "            local_df = pd.read_sql(\n",
    "                \"SELECT Ref FROM checkpoint_after_manual_review_20251106_150834\",\n",
    "                self.local_db\n",
    "            )\n",
    "            \n",
    "            scraped_ids = set(scraped_df[\"node_id\"].astype(str))\n",
    "            local_ids = set(\n",
    "                local_df[\"Ref\"].astype(str).str.replace(r\"^WHP-\", \"\", regex=True)\n",
    "            )\n",
    "            \n",
    "            missing_ids = scraped_ids - local_ids\n",
    "            \n",
    "        else:\n",
    "            scraped_df = pd.read_sql(\n",
    "                \"SELECT node_id FROM dictionary_entries WHERE source_dataset = 'IDIEZ'\",\n",
    "                self.scraped_db\n",
    "            )\n",
    "            local_df = pd.read_sql(\n",
    "                \"SELECT Ref FROM 'IDIEZ_modern_nahuatl-all-2024-03-27T09-45-31'\",\n",
    "                self.local_db\n",
    "            )\n",
    "            \n",
    "            scraped_ids = set(scraped_df[\"node_id\"].astype(str))\n",
    "            local_ids = set(\n",
    "                local_df[\"Ref\"].astype(str).str.replace(r\"^IDIEZ-\", \"\", regex=True)\n",
    "            )\n",
    "            \n",
    "            missing_ids = scraped_ids - local_ids\n",
    "            \n",
    "        return sorted(list(missing_ids))\n",
    "    \n",
    "    def gather_whp_entry_data(self, node_ids: List[str]) -> pd.DataFrame:\n",
    "        if not node_ids:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        placeholders = ','.join('?' * len(node_ids))\n",
    "        \n",
    "        base_query = f\"\"\"\n",
    "        SELECT \n",
    "            node_id,\n",
    "            headword,\n",
    "            orthographic_variants,\n",
    "            ipa_spelling,\n",
    "            translation_english,\n",
    "            spanish_loanword,\n",
    "            url_alias,\n",
    "            created_timestamp,\n",
    "            scrape_timestamp\n",
    "        FROM dictionary_entries\n",
    "        WHERE node_id IN ({placeholders})\n",
    "        AND source_dataset = 'WHP'\n",
    "        \"\"\"\n",
    "        entries_df = pd.read_sql(base_query, self.scraped_db, params=node_ids)\n",
    "        \n",
    "        authority_query = f\"\"\"\n",
    "        SELECT \n",
    "            node_id,\n",
    "            authority_name,\n",
    "            citation_text\n",
    "        FROM authority_citations\n",
    "        WHERE node_id IN ({placeholders})\n",
    "        ORDER BY node_id, citation_order\n",
    "        \"\"\"\n",
    "        authorities_df = pd.read_sql(authority_query, self.scraped_db, params=node_ids)\n",
    "        \n",
    "        attestations_query = f\"\"\"\n",
    "        SELECT \n",
    "            node_id,\n",
    "            language,\n",
    "            attestation_text\n",
    "        FROM attestations\n",
    "        WHERE node_id IN ({placeholders})\n",
    "        ORDER BY node_id\n",
    "        \"\"\"\n",
    "        attestations_df = pd.read_sql(attestations_query, self.scraped_db, params=node_ids)\n",
    "        \n",
    "        themes_query = f\"\"\"\n",
    "        SELECT \n",
    "            et.entry_node_id as node_id,\n",
    "            GROUP_CONCAT(t.name, '; ') as themes\n",
    "        FROM entry_themes et\n",
    "        JOIN themes t ON et.theme_tid = t.tid\n",
    "        WHERE et.entry_node_id IN ({placeholders})\n",
    "        GROUP BY et.entry_node_id\n",
    "        \"\"\"\n",
    "        themes_df = pd.read_sql(themes_query, self.scraped_db, params=node_ids)\n",
    "        \n",
    "        audio_query = f\"\"\"\n",
    "        SELECT \n",
    "            ea.entry_node_id as node_id,\n",
    "            af.file_mp3,\n",
    "            af.speaker,\n",
    "            ea.reference_type\n",
    "        FROM entry_audio ea\n",
    "        JOIN audio_files af ON ea.audio_node_id = af.node_id\n",
    "        WHERE ea.entry_node_id IN ({placeholders})\n",
    "        ORDER BY ea.entry_node_id, ea.delta\n",
    "        \"\"\"\n",
    "        audio_df = pd.read_sql(audio_query, self.scraped_db, params=node_ids)\n",
    "        \n",
    "        xref_query = f\"\"\"\n",
    "        SELECT \n",
    "            source_node_id as node_id,\n",
    "            GROUP_CONCAT(target_node_id, '; ') as cross_references\n",
    "        FROM entry_cross_references\n",
    "        WHERE source_node_id IN ({placeholders})\n",
    "        GROUP BY source_node_id\n",
    "        \"\"\"\n",
    "        xref_df = pd.read_sql(xref_query, self.scraped_db, params=node_ids)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for _, entry in entries_df.iterrows():\n",
    "            node_id = entry['node_id']\n",
    "            \n",
    "            row_data = {\n",
    "                'Ref': f\"WHP-{node_id}\",\n",
    "                'Headword': entry['headword'],\n",
    "                'Orthographic Variants': entry['orthographic_variants'],\n",
    "                'IPA Spelling': entry['ipa_spelling'],\n",
    "                'Principal English Translation': entry['translation_english'],\n",
    "                'Spanish Loanword': entry['spanish_loanword'],\n",
    "            }\n",
    "            \n",
    "            entry_authorities = authorities_df[authorities_df['node_id'] == node_id]\n",
    "            authority_mapping = {\n",
    "                'Molina': 'Alonso de Molina',\n",
    "                'Karttunen': 'Frances Karttunen',\n",
    "                'Carochi': 'Horacio Carochi / English',\n",
    "                'Olmos': 'Andrés de Olmos',\n",
    "                'Lockhart': \"Lockhart's Nahuatl as Written\"\n",
    "            }\n",
    "            \n",
    "            for auth_name, col_name in authority_mapping.items():\n",
    "                auth_data = entry_authorities[entry_authorities['authority_name'] == auth_name]\n",
    "                row_data[col_name] = '\\n'.join(auth_data['citation_text'].values) if not auth_data.empty else None\n",
    "            \n",
    "            entry_attestations = attestations_df[attestations_df['node_id'] == node_id]\n",
    "            \n",
    "            english_attest = entry_attestations[entry_attestations['language'] == 'English']\n",
    "            row_data['Attestations from sources in English'] = (\n",
    "                '\\n\\n'.join(english_attest['attestation_text'].values) \n",
    "                if not english_attest.empty else None\n",
    "            )\n",
    "            \n",
    "            spanish_attest = entry_attestations[entry_attestations['language'] == 'Spanish']\n",
    "            row_data['Attestations from sources in Spanish'] = (\n",
    "                '\\n\\n'.join(spanish_attest['attestation_text'].values)\n",
    "                if not spanish_attest.empty else None\n",
    "            )\n",
    "            \n",
    "            entry_themes = themes_df[themes_df['node_id'] == node_id]\n",
    "            row_data['themes'] = entry_themes.iloc[0]['themes'] if not entry_themes.empty else None\n",
    "            \n",
    "            entry_audio = audio_df[audio_df['node_id'] == node_id]\n",
    "            if not entry_audio.empty:\n",
    "                row_data['audio_files'] = '; '.join(entry_audio['file_mp3'].dropna().values)\n",
    "                row_data['audio_speakers'] = '; '.join(entry_audio['speaker'].dropna().values)\n",
    "            else:\n",
    "                row_data['audio_files'] = None\n",
    "                row_data['audio_speakers'] = None\n",
    "            \n",
    "            entry_xrefs = xref_df[xref_df['node_id'] == node_id]\n",
    "            row_data['cross_references'] = entry_xrefs.iloc[0]['cross_references'] if not entry_xrefs.empty else None\n",
    "            \n",
    "            row_data['url_alias'] = entry['url_alias']\n",
    "            row_data['created_timestamp'] = entry['created_timestamp']\n",
    "            row_data['scrape_timestamp'] = entry['scrape_timestamp']\n",
    "            \n",
    "            results.append(row_data)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def gather_idiez_entry_data(self, node_ids: List[str]) -> pd.DataFrame:\n",
    "        if not node_ids:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        placeholders = ','.join('?' * len(node_ids))\n",
    "        \n",
    "        base_query = f\"\"\"\n",
    "        SELECT \n",
    "            node_id,\n",
    "            headword_idiez,\n",
    "            translation_english_idiez,\n",
    "            definition_nahuatl_idiez,\n",
    "            definition_spanish_idiez,\n",
    "            morfologia_idiez,\n",
    "            gramatica_idiez,\n",
    "            url_alias,\n",
    "            scrape_timestamp\n",
    "        FROM dictionary_entries\n",
    "        WHERE node_id IN ({placeholders})\n",
    "        AND source_dataset = 'IDIEZ'\n",
    "        \"\"\"\n",
    "        entries_df = pd.read_sql(base_query, self.scraped_db, params=node_ids)\n",
    "        \n",
    "        themes_query = f\"\"\"\n",
    "        SELECT \n",
    "            et.entry_node_id as node_id,\n",
    "            GROUP_CONCAT(t.name, '; ') as themes\n",
    "        FROM entry_themes et\n",
    "        JOIN themes t ON et.theme_tid = t.tid\n",
    "        WHERE et.entry_node_id IN ({placeholders})\n",
    "        GROUP BY et.entry_node_id\n",
    "        \"\"\"\n",
    "        themes_df = pd.read_sql(themes_query, self.scraped_db, params=node_ids)\n",
    "        \n",
    "        audio_query = f\"\"\"\n",
    "        SELECT \n",
    "            ea.entry_node_id as node_id,\n",
    "            GROUP_CONCAT(af.file_mp3, '; ') as audio_files\n",
    "        FROM entry_audio ea\n",
    "        JOIN audio_files af ON ea.audio_node_id = af.node_id\n",
    "        WHERE ea.entry_node_id IN ({placeholders})\n",
    "        GROUP BY ea.entry_node_id\n",
    "        \"\"\"\n",
    "        audio_df = pd.read_sql(audio_query, self.scraped_db, params=node_ids)\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for _, entry in entries_df.iterrows():\n",
    "            node_id = entry['node_id']\n",
    "            \n",
    "            row_data = {\n",
    "                'Ref': f\"IDIEZ-{node_id}\",\n",
    "                'tlahtolli': entry['headword_idiez'],\n",
    "                'IDIEZ traduc. inglés': entry['translation_english_idiez'],\n",
    "                'IDIEZ def. náhuatl': entry['definition_nahuatl_idiez'],\n",
    "                'IDIEZ def. español': entry['definition_spanish_idiez'],\n",
    "                'IDIEZ morfología_idiez': entry['morfologia_idiez'],\n",
    "                'IDIEZ gramática_idiez': entry['gramatica_idiez'],\n",
    "            }\n",
    "            \n",
    "            entry_themes = themes_df[themes_df['node_id'] == node_id]\n",
    "            row_data['themes'] = entry_themes.iloc[0]['themes'] if not entry_themes.empty else None\n",
    "            \n",
    "            entry_audio = audio_df[audio_df['node_id'] == node_id]\n",
    "            row_data['audio_files'] = entry_audio.iloc[0]['audio_files'] if not entry_audio.empty else None\n",
    "            \n",
    "            row_data['url_alias'] = entry['url_alias']\n",
    "            row_data['scrape_timestamp'] = entry['scrape_timestamp']\n",
    "            \n",
    "            results.append(row_data)\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    def insert_into_checkpoint(self, df: pd.DataFrame, source_table: str, \n",
    "                              create_new_checkpoint: bool = True) -> str:\n",
    "        if df.empty:\n",
    "            print(\"No data to insert\")\n",
    "            return source_table\n",
    "        \n",
    "        if create_new_checkpoint:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            new_table = f\"checkpoint_after_missing_entries_added_{timestamp}\"\n",
    "            \n",
    "            print(f\"Creating new checkpoint: {new_table}\")\n",
    "            \n",
    "            existing_df = pd.read_sql(f\"SELECT * FROM [{source_table}]\", self.local_db)\n",
    "            existing_cols = existing_df.columns.tolist()\n",
    "            new_cols = df.columns.tolist()\n",
    "\n",
    "            # Add missing columns to df with None values\n",
    "            for col in existing_cols:\n",
    "                if col not in new_cols:\n",
    "                    df[col] = None\n",
    "\n",
    "            # Reorder df columns to match existing_df\n",
    "            df = df[existing_cols]\n",
    "\n",
    "            combined_df = pd.concat([existing_df, df], ignore_index=True)\n",
    "            \n",
    "            combined_df.to_sql(new_table, self.local_db, if_exists='replace', index=False)\n",
    "            \n",
    "            verification = pd.read_sql(f\"SELECT COUNT(*) as count FROM [{new_table}]\", self.local_db)\n",
    "            count = verification.iloc[0]['count']\n",
    "            \n",
    "            print(f\"Original entries: {len(existing_df):,}\")\n",
    "            print(f\"Added entries: {len(df):,}\")\n",
    "            print(f\"Total entries in new checkpoint: {count:,}\")\n",
    "            \n",
    "            return new_table\n",
    "        else:\n",
    "            print(f\"Appending {len(df):,} entries to {source_table}\")\n",
    "            df.to_sql(source_table, self.local_db, if_exists='append', index=False)\n",
    "            \n",
    "            verification = pd.read_sql(f\"SELECT COUNT(*) as count FROM [{source_table}]\", self.local_db)\n",
    "            print(f\"New total: {verification.iloc[0]['count']:,}\")\n",
    "            \n",
    "            return source_table\n",
    "    \n",
    "    def process_and_populate(self, dataset: str = 'WHP', \n",
    "                           source_table: str = None,\n",
    "                           create_new_checkpoint: bool = True,\n",
    "                           export_csv: bool = True,\n",
    "                           csv_path: str = None):\n",
    "        \n",
    "        if source_table is None:\n",
    "            if dataset == 'WHP':\n",
    "                source_table = \"checkpoint_after_manual_review_20251106_150834\"\n",
    "            else:\n",
    "                source_table = \"IDIEZ_modern_nahuatl-all-2024-03-27T09-45-31\"\n",
    "        \n",
    "        missing_ids = self.get_missing_entry_ids(dataset)\n",
    "        \n",
    "        if not missing_ids:\n",
    "            print(f\"No missing {dataset} entries found\")\n",
    "            return None, source_table\n",
    "        \n",
    "        print(f\"Found {len(missing_ids)} missing {dataset} entries\")\n",
    "        \n",
    "        if dataset == 'WHP':\n",
    "            df = self.gather_whp_entry_data(missing_ids)\n",
    "        else:\n",
    "            df = self.gather_idiez_entry_data(missing_ids)\n",
    "        \n",
    "        if export_csv:\n",
    "            output_path = csv_path or f'./{dataset.lower()}_missing_entries_complete.csv'\n",
    "            df.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "            print(f\"Exported to {output_path}\")\n",
    "        \n",
    "        new_table = self.insert_into_checkpoint(df, source_table, create_new_checkpoint)\n",
    "        \n",
    "        return df, new_table\n",
    "    \n",
    "    def close(self):\n",
    "        self.scraped_db.close()\n",
    "        self.local_db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1f7ad0dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 276 missing WHP entries\n",
      "Exported to ./whp_missing_entries_complete.csv\n",
      "Creating new checkpoint: checkpoint_after_missing_entries_added_20251111_184145\n",
      "Original entries: 31,806\n",
      "Added entries: 276\n",
      "Total entries in new checkpoint: 32,082\n"
     ]
    }
   ],
   "source": [
    "scraped_data_dir = Path(\"../../../data/scrapedDataDb/\")\n",
    "local_data_dir = Path(\"../../../data/sqLiteDb/\")\n",
    "\n",
    "gatherer = MissingEntryGatherer(\n",
    "    scraped_db_path=str(scraped_data_dir / \"nahuatl.db\"),\n",
    "    local_db_path=str(local_data_dir / \"nahuatl_processing.db\")\n",
    ")\n",
    "\n",
    "whp_df, new_whp_table = gatherer.process_and_populate(\n",
    "    dataset='WHP',\n",
    "    source_table='checkpoint_after_manual_review_20251106_150834',\n",
    "    create_new_checkpoint=True,\n",
    "    export_csv=True,\n",
    "    csv_path='./whp_missing_entries_complete.csv'\n",
    ")\n",
    "\n",
    "gatherer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3ddd3ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation mismatches: 11\n",
      "Authority citation mismatches: 4\n",
      "  - Punctuation-only differences: 0\n",
      "  - Other differences: 4\n",
      "Attestation mismatches: 17\n",
      "  - Punctuation-only differences: 0\n",
      "  - Other differences: 17\n",
      "\n",
      "Reports generated:\n",
      "  - report_translation_mismatches.csv\n",
      "  - report_authority_mismatches.csv\n",
      "  - report_attestation_mismatches.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# Existing helper functions\n",
    "def get_authority_citations(node_id, db_conn):\n",
    "    \"\"\"Get all authority citations for a node, grouped by authority\"\"\"\n",
    "    auth_data = pd.read_sql(\n",
    "        f\"\"\"\n",
    "        SELECT authority_name, citation_text, citation_order\n",
    "        FROM authority_citations\n",
    "        WHERE node_id = '{node_id}'\n",
    "        ORDER BY authority_name, citation_order\n",
    "        \"\"\",\n",
    "        db_conn,\n",
    "    )\n",
    "\n",
    "    result = {}\n",
    "    for auth_name in [\"Molina\", \"Karttunen\", \"Carochi\", \"Olmos\", \"Lockhart\"]:\n",
    "        auth_rows = auth_data[auth_data[\"authority_name\"] == auth_name]\n",
    "        if not auth_rows.empty:\n",
    "            result[auth_name] = \" | \".join(auth_rows[\"citation_text\"].tolist())\n",
    "        else:\n",
    "            result[auth_name] = None\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_attestations(node_id, db_conn):\n",
    "    \"\"\"Get attestations for a node, grouped by language\"\"\"\n",
    "    attest_data = pd.read_sql(\n",
    "        f\"\"\"\n",
    "        SELECT language, attestation_text\n",
    "        FROM attestations\n",
    "        WHERE node_id = '{node_id}'\n",
    "        \"\"\",\n",
    "        db_conn,\n",
    "    )\n",
    "\n",
    "    result = {\"English\": None, \"Spanish\": None}\n",
    "\n",
    "    for _, row in attest_data.iterrows():\n",
    "        lang = row[\"language\"]\n",
    "        if lang in result:\n",
    "            if result[lang] is None:\n",
    "                result[lang] = row[\"attestation_text\"]\n",
    "            else:\n",
    "                result[lang] += \" | \" + row[\"attestation_text\"]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_cross_references(node_id, db_conn):\n",
    "    \"\"\"Get cross-references for a node\"\"\"\n",
    "    xref_data = pd.read_sql(\n",
    "        f\"\"\"\n",
    "        SELECT target_node_id, reference_type\n",
    "        FROM entry_cross_references\n",
    "        WHERE source_node_id = '{node_id}'\n",
    "        \"\"\",\n",
    "        db_conn,\n",
    "    )\n",
    "\n",
    "    if xref_data.empty:\n",
    "        return None\n",
    "\n",
    "    return \" | \".join(xref_data[\"target_node_id\"].astype(str).tolist())\n",
    "\n",
    "\n",
    "def strip_div_br_tags(text):\n",
    "    \"\"\"Strip div and br tags but keep everything else intact\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    text = re.sub(r\"<div[^>]*>\", \"\", text)\n",
    "    text = re.sub(r\"</div>\", \"\", text)\n",
    "    text = re.sub(r\"<br\\s*/?>\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = re.sub(r\"\\s+</\", \"</\", text)\n",
    "    text = re.sub(r\">\\s+\", \">\", text)\n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def strip_punctuation_for_comparison(text):\n",
    "    \"\"\"Remove common punctuation for comparison\"\"\"\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = re.sub(r\"[.,;:!?]\", \"\", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "# New diff tracking helper functions\n",
    "def get_item_level_diff(scraped_val, local_val):\n",
    "    \"\"\"Compare pipe-separated values and return what's different\"\"\"\n",
    "    if pd.isna(scraped_val):\n",
    "        scraped_val = \"\"\n",
    "    if pd.isna(local_val):\n",
    "        local_val = \"\"\n",
    "    \n",
    "    scraped_val = str(scraped_val).strip()\n",
    "    local_val = str(local_val).strip()\n",
    "    \n",
    "    if \"|\" not in scraped_val and \"|\" not in local_val:\n",
    "        return None\n",
    "    \n",
    "    scraped_items = set([item.strip() for item in scraped_val.split(\"|\") if item.strip()])\n",
    "    local_items = set([item.strip() for item in local_val.split(\"|\") if item.strip()])\n",
    "    \n",
    "    added = scraped_items - local_items\n",
    "    removed = local_items - scraped_items\n",
    "    \n",
    "    result = []\n",
    "    if added:\n",
    "        result.append(f\"ADDED: {' | '.join(sorted(added))}\")\n",
    "    if removed:\n",
    "        result.append(f\"REMOVED: {' | '.join(sorted(removed))}\")\n",
    "    \n",
    "    return \" || \".join(result) if result else None\n",
    "\n",
    "\n",
    "def get_length_diff(scraped_val, local_val):\n",
    "    \"\"\"Return character length difference\"\"\"\n",
    "    scraped_len = len(str(scraped_val)) if not pd.isna(scraped_val) else 0\n",
    "    local_len = len(str(local_val)) if not pd.isna(local_val) else 0\n",
    "    diff = scraped_len - local_len\n",
    "    return f\"{diff:+d}\" if diff != 0 else \"0\"\n",
    "\n",
    "\n",
    "def get_first_diff_position(scraped_val, local_val):\n",
    "    \"\"\"Find position of first character difference\"\"\"\n",
    "    if pd.isna(scraped_val):\n",
    "        scraped_val = \"\"\n",
    "    if pd.isna(local_val):\n",
    "        local_val = \"\"\n",
    "    \n",
    "    scraped_val = str(scraped_val)\n",
    "    local_val = str(local_val)\n",
    "    \n",
    "    min_len = min(len(scraped_val), len(local_val))\n",
    "    \n",
    "    for i in range(min_len):\n",
    "        if scraped_val[i] != local_val[i]:\n",
    "            return i\n",
    "    \n",
    "    if len(scraped_val) != len(local_val):\n",
    "        return min_len\n",
    "    \n",
    "    return None\n",
    "\n",
    "\n",
    "def get_context_around_diff(scraped_val, local_val, context_chars=30):\n",
    "    \"\"\"Show text around first difference for easier comparison\"\"\"\n",
    "    if pd.isna(scraped_val):\n",
    "        scraped_val = \"\"\n",
    "    if pd.isna(local_val):\n",
    "        local_val = \"\"\n",
    "    \n",
    "    scraped_val = str(scraped_val)\n",
    "    local_val = str(local_val)\n",
    "    \n",
    "    pos = get_first_diff_position(scraped_val, local_val)\n",
    "    if pos is None:\n",
    "        return None\n",
    "    \n",
    "    start = max(0, pos - context_chars)\n",
    "    end = min(len(scraped_val), pos + context_chars)\n",
    "    \n",
    "    scraped_context = scraped_val[start:end]\n",
    "    local_context = local_val[start:end] if pos < len(local_val) else local_val[start:]\n",
    "    \n",
    "    return f\"Scraped: ...{scraped_context}... | Local: ...{local_context}...\"\n",
    "\n",
    "\n",
    "# Load base scraped data\n",
    "scraped_enriched = pd.read_sql(\n",
    "    \"\"\"\n",
    "    SELECT \n",
    "        de.node_id,\n",
    "        de.headword,\n",
    "        de.orthographic_variants,\n",
    "        de.translation_english,\n",
    "        de.spanish_loanword,\n",
    "        de.source_dataset,\n",
    "        de.url_alias\n",
    "    FROM dictionary_entries de\n",
    "    WHERE de.source_dataset = 'WHP'\n",
    "    \"\"\",\n",
    "    scraped_db,\n",
    ")\n",
    "\n",
    "# Enrich with authority citations, attestations, and cross-references\n",
    "auth_cols = [\n",
    "    \"scraped_molina\",\n",
    "    \"scraped_karttunen\",\n",
    "    \"scraped_carochi\",\n",
    "    \"scraped_olmos\",\n",
    "    \"scraped_lockhart\",\n",
    "]\n",
    "attest_cols = [\"scraped_attest_english\", \"scraped_attest_spanish\"]\n",
    "\n",
    "for col in auth_cols + attest_cols + [\"scraped_crossrefs\"]:\n",
    "    scraped_enriched[col] = None\n",
    "\n",
    "for idx, row in scraped_enriched.iterrows():\n",
    "    node_id = row[\"node_id\"]\n",
    "\n",
    "    auth_cites = get_authority_citations(node_id, scraped_db)\n",
    "    scraped_enriched.at[idx, \"scraped_molina\"] = auth_cites[\"Molina\"]\n",
    "    scraped_enriched.at[idx, \"scraped_karttunen\"] = auth_cites[\"Karttunen\"]\n",
    "    scraped_enriched.at[idx, \"scraped_carochi\"] = auth_cites[\"Carochi\"]\n",
    "    scraped_enriched.at[idx, \"scraped_olmos\"] = auth_cites[\"Olmos\"]\n",
    "    scraped_enriched.at[idx, \"scraped_lockhart\"] = auth_cites[\"Lockhart\"]\n",
    "\n",
    "    attests = get_attestations(node_id, scraped_db)\n",
    "    scraped_enriched.at[idx, \"scraped_attest_english\"] = attests[\"English\"]\n",
    "    scraped_enriched.at[idx, \"scraped_attest_spanish\"] = attests[\"Spanish\"]\n",
    "\n",
    "    xrefs = get_cross_references(node_id, scraped_db)\n",
    "    scraped_enriched.at[idx, \"scraped_crossrefs\"] = xrefs\n",
    "\n",
    "WHP_TABLE_PUNCT_FIXED = \"checkpoint_llm_validated_20251030\"\n",
    "\n",
    "# Load local data with all columns\n",
    "local_enriched = pd.read_sql(\n",
    "    f\"\"\"\n",
    "    SELECT \n",
    "        Ref,\n",
    "        Headword,\n",
    "        \"Orthographic Variants\",\n",
    "        \"Principal English Translation\",\n",
    "        \"Spanish Loanword\",\n",
    "        \"Attestations from sources in English\",\n",
    "        \"Attestations from sources in Spanish\",\n",
    "        \"Alonso de Molina\",\n",
    "        \"Frances Karttunen\",\n",
    "        \"Horacio Carochi / English\",\n",
    "        \"Andrés de Olmos\",\n",
    "        \"Lockhart’s Nahuatl as Written\",\n",
    "        \"Citations\",\n",
    "        \"Cross_References\"\n",
    "    FROM [{WHP_TABLE_PUNCT_FIXED}]\n",
    "    \"\"\",\n",
    "    local_db,\n",
    ")\n",
    "\n",
    "# Prepare for merge\n",
    "scraped_enriched[\"node_id\"] = scraped_enriched[\"node_id\"].astype(str)\n",
    "local_enriched[\"Ref\"] = (\n",
    "    local_enriched[\"Ref\"].astype(str).str.replace(r\"^(WHP-|IDIEZ-)\", \"\", regex=True)\n",
    ")\n",
    "\n",
    "# Merge datasets\n",
    "merged = scraped_enriched.merge(\n",
    "    local_enriched,\n",
    "    left_on=\"node_id\",\n",
    "    right_on=\"Ref\",\n",
    "    how=\"inner\",\n",
    "    suffixes=(\"_scraped\", \"_local\"),\n",
    ")\n",
    "\n",
    "# Apply cleaning to both datasets\n",
    "merged[\"scraped_translation_clean\"] = merged[\"translation_english\"].apply(\n",
    "    strip_div_br_tags\n",
    ")\n",
    "merged[\"local_translation_clean\"] = merged[\"Principal English Translation\"].apply(\n",
    "    strip_div_br_tags\n",
    ")\n",
    "\n",
    "merged[\"scraped_molina_clean\"] = merged[\"scraped_molina\"].apply(strip_div_br_tags)\n",
    "merged[\"local_molina_clean\"] = merged[\"Alonso de Molina\"].apply(strip_div_br_tags)\n",
    "\n",
    "merged[\"scraped_karttunen_clean\"] = merged[\"scraped_karttunen\"].apply(strip_div_br_tags)\n",
    "merged[\"local_karttunen_clean\"] = merged[\"Frances Karttunen\"].apply(strip_div_br_tags)\n",
    "\n",
    "merged[\"scraped_carochi_clean\"] = merged[\"scraped_carochi\"].apply(strip_div_br_tags)\n",
    "merged[\"local_carochi_clean\"] = merged[\"Horacio Carochi / English\"].apply(\n",
    "    strip_div_br_tags\n",
    ")\n",
    "\n",
    "merged[\"scraped_olmos_clean\"] = merged[\"scraped_olmos\"].apply(strip_div_br_tags)\n",
    "merged[\"local_olmos_clean\"] = merged[\"Andrés de Olmos\"].apply(strip_div_br_tags)\n",
    "\n",
    "merged[\"scraped_lockhart_clean\"] = merged[\"scraped_lockhart\"].apply(strip_div_br_tags)\n",
    "merged[\"local_lockhart_clean\"] = merged[\"Lockhart’s Nahuatl as Written\"].apply(\n",
    "    strip_div_br_tags\n",
    ")\n",
    "\n",
    "merged[\"scraped_attest_english_clean\"] = merged[\"scraped_attest_english\"].apply(\n",
    "    strip_div_br_tags\n",
    ")\n",
    "merged[\"local_attest_english_clean\"] = merged[\n",
    "    \"Attestations from sources in English\"\n",
    "].apply(strip_div_br_tags)\n",
    "\n",
    "merged[\"scraped_attest_spanish_clean\"] = merged[\"scraped_attest_spanish\"].apply(\n",
    "    strip_div_br_tags\n",
    ")\n",
    "merged[\"local_attest_spanish_clean\"] = merged[\n",
    "    \"Attestations from sources in Spanish\"\n",
    "].apply(strip_div_br_tags)\n",
    "\n",
    "# Report 1: Translation differences\n",
    "translation_diff = merged[\n",
    "    (merged[\"scraped_translation_clean\"] != merged[\"local_translation_clean\"])\n",
    "    & (merged[\"local_translation_clean\"] != \"None\")\n",
    "].copy()\n",
    "\n",
    "translation_report = translation_diff[\n",
    "    [\n",
    "        \"node_id\",\n",
    "        \"headword\",\n",
    "        \"orthographic_variants\",\n",
    "        \"url_alias\",\n",
    "        \"scraped_translation_clean\",\n",
    "        \"local_translation_clean\",\n",
    "    ]\n",
    "].copy()\n",
    "\n",
    "translation_report[\"item_diff\"] = translation_report.apply(\n",
    "    lambda row: get_item_level_diff(row[\"scraped_translation_clean\"], row[\"local_translation_clean\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "translation_report[\"char_length_diff\"] = translation_report.apply(\n",
    "    lambda row: get_length_diff(row[\"scraped_translation_clean\"], row[\"local_translation_clean\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "translation_report[\"first_diff_at\"] = translation_report.apply(\n",
    "    lambda row: get_first_diff_position(row[\"scraped_translation_clean\"], row[\"local_translation_clean\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "translation_report[\"diff_context\"] = translation_report.apply(\n",
    "    lambda row: get_context_around_diff(row[\"scraped_translation_clean\"], row[\"local_translation_clean\"]),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "translation_report.to_csv(\n",
    "    \"./report_translation_mismatches.csv\", index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "print(f\"Translation mismatches: {len(translation_report)}\")\n",
    "\n",
    "# Report 2: Authority citation differences\n",
    "authority_diffs = []\n",
    "\n",
    "for idx, row in merged.iterrows():\n",
    "    authorities = [\n",
    "        (\"Molina\", \"scraped_molina_clean\", \"local_molina_clean\"),\n",
    "        (\"Karttunen\", \"scraped_karttunen_clean\", \"local_karttunen_clean\"),\n",
    "        (\"Carochi\", \"scraped_carochi_clean\", \"local_carochi_clean\"),\n",
    "        (\"Olmos\", \"scraped_olmos_clean\", \"local_olmos_clean\"),\n",
    "        (\"Lockhart\", \"scraped_lockhart_clean\", \"local_lockhart_clean\"),\n",
    "    ]\n",
    "\n",
    "    for auth_name, scraped_col, local_col in authorities:\n",
    "        if row[scraped_col] != row[local_col] and str(row[local_col]).strip() != \"None\":\n",
    "\n",
    "            scraped_no_punct = strip_punctuation_for_comparison(row[scraped_col])\n",
    "            local_no_punct = strip_punctuation_for_comparison(row[local_col])\n",
    "\n",
    "            is_punctuation_only = scraped_no_punct == local_no_punct\n",
    "\n",
    "            authority_diffs.append(\n",
    "                {\n",
    "                    \"node_id\": row[\"node_id\"],\n",
    "                    \"headword\": row[\"headword\"],\n",
    "                    \"url_alias\": row[\"url_alias\"],\n",
    "                    \"authority\": auth_name,\n",
    "                    \"scraped_value\": row[scraped_col],\n",
    "                    \"local_value\": row[local_col],\n",
    "                    \"punctuation_only_diff\": is_punctuation_only,\n",
    "                    \"recommended_action\": (\n",
    "                        \"use_scraped\" if is_punctuation_only else \"manual_review\"\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "if authority_diffs:\n",
    "    authority_report = pd.DataFrame(authority_diffs)\n",
    "    \n",
    "    authority_report[\"item_diff\"] = authority_report.apply(\n",
    "        lambda row: get_item_level_diff(row[\"scraped_value\"], row[\"local_value\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    authority_report[\"char_length_diff\"] = authority_report.apply(\n",
    "        lambda row: get_length_diff(row[\"scraped_value\"], row[\"local_value\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    authority_report[\"first_diff_at\"] = authority_report.apply(\n",
    "        lambda row: get_first_diff_position(row[\"scraped_value\"], row[\"local_value\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    authority_report[\"diff_context\"] = authority_report.apply(\n",
    "        lambda row: get_context_around_diff(row[\"scraped_value\"], row[\"local_value\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    authority_report.to_csv(\n",
    "        \"./report_authority_mismatches.csv\", index=False, encoding=\"utf-8-sig\"\n",
    "    )\n",
    "\n",
    "    punct_only_count = authority_report[\"punctuation_only_diff\"].sum()\n",
    "    print(f\"Authority citation mismatches: {len(authority_report)}\")\n",
    "    print(f\"  - Punctuation-only differences: {punct_only_count}\")\n",
    "    print(f\"  - Other differences: {len(authority_report) - punct_only_count}\")\n",
    "else:\n",
    "    print(\"Authority citation mismatches: 0\")\n",
    "\n",
    "# Report 3: Attestation differences\n",
    "attestation_diffs = []\n",
    "\n",
    "for idx, row in merged.iterrows():\n",
    "    attestations = [\n",
    "        (\"English\", \"scraped_attest_english_clean\", \"local_attest_english_clean\"),\n",
    "        (\"Spanish\", \"scraped_attest_spanish_clean\", \"local_attest_spanish_clean\"),\n",
    "    ]\n",
    "\n",
    "    for lang, scraped_col, local_col in attestations:\n",
    "        if row[scraped_col] != row[local_col] and str(row[local_col]).strip() != \"None\":\n",
    "\n",
    "            scraped_no_punct = strip_punctuation_for_comparison(row[scraped_col])\n",
    "            local_no_punct = strip_punctuation_for_comparison(row[local_col])\n",
    "\n",
    "            is_punctuation_only = scraped_no_punct == local_no_punct\n",
    "\n",
    "            attestation_diffs.append(\n",
    "                {\n",
    "                    \"node_id\": row[\"node_id\"],\n",
    "                    \"headword\": row[\"headword\"],\n",
    "                    \"url_alias\": row[\"url_alias\"],\n",
    "                    \"language\": lang,\n",
    "                    \"scraped_value\": row[scraped_col],\n",
    "                    \"local_value\": row[local_col],\n",
    "                    \"punctuation_only_diff\": is_punctuation_only,\n",
    "                    \"recommended_action\": (\n",
    "                        \"use_scraped\" if is_punctuation_only else \"manual_review\"\n",
    "                    ),\n",
    "                }\n",
    "            )\n",
    "\n",
    "if attestation_diffs:\n",
    "    attestation_report = pd.DataFrame(attestation_diffs)\n",
    "    \n",
    "    attestation_report[\"item_diff\"] = attestation_report.apply(\n",
    "        lambda row: get_item_level_diff(row[\"scraped_value\"], row[\"local_value\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    attestation_report[\"char_length_diff\"] = attestation_report.apply(\n",
    "        lambda row: get_length_diff(row[\"scraped_value\"], row[\"local_value\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    attestation_report[\"first_diff_at\"] = attestation_report.apply(\n",
    "        lambda row: get_first_diff_position(row[\"scraped_value\"], row[\"local_value\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    attestation_report[\"diff_context\"] = attestation_report.apply(\n",
    "        lambda row: get_context_around_diff(row[\"scraped_value\"], row[\"local_value\"]),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    attestation_report.to_csv(\n",
    "        \"./report_attestation_mismatches.csv\", index=False, encoding=\"utf-8-sig\"\n",
    "    )\n",
    "\n",
    "    punct_only_count = attestation_report[\"punctuation_only_diff\"].sum()\n",
    "    print(f\"Attestation mismatches: {len(attestation_report)}\")\n",
    "    print(f\"  - Punctuation-only differences: {punct_only_count}\")\n",
    "    print(f\"  - Other differences: {len(attestation_report) - punct_only_count}\")\n",
    "else:\n",
    "    print(\"Attestation mismatches: 0\")\n",
    "\n",
    "print(\"\\nReports generated:\")\n",
    "print(\"  - report_translation_mismatches.csv\")\n",
    "print(\"  - report_authority_mismatches.csv\")\n",
    "print(\"  - report_attestation_mismatches.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03b0d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraped_db.close()\n",
    "# local_db.close()\n",
    "# print(\"Database connections closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74d6fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nahuaLEX_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
