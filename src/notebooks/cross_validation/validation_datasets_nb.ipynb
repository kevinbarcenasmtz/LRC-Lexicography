{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5346fcb6",
   "metadata": {},
   "source": [
    "# Validation and correction\n",
    "\n",
    "As we've continued working with the scraped data versus the changes we've made from the initial csv files given to the LRC. We've done validation and corrections to the initial csv files that are not reflected in the scraped datasets. So now since we've uploaded our data into SQLite the next step will be to cross validate and update the SQLite. Which will be done below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cef09ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## setup and imports\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Optional, Union, Any\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af11e03d",
   "metadata": {},
   "source": [
    "note the original local datasets given to the LRC initially are hosted in the nahuatl_processing.db under sqLiteDb specifically under the table name: checkpoint_after_empty_p_tag_removal_20251002. For the scraped data, that is under scrapedDataDb under the nahuatl.db file. The schema is under config/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8ed35dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "scraped_data_dir = Path(\"../../../data/scrapedDataDb/\")\n",
    "local_data_dir = Path(\"../../../data/sqLiteDb/\")\n",
    "\n",
    "if not scraped_data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Scraped database not found: {scraped_data_dir}\")\n",
    "if not local_data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Local database not found: {local_data_dir}\")\n",
    "\n",
    "# Database connection\n",
    "scraped_db = sqlite3.connect(scraped_data_dir / \"nahuatl.db\")\n",
    "local_db = sqlite3.connect(local_data_dir / \"nahuatl_processing.db\")\n",
    "\n",
    "\n",
    "# read in the table(s), for the local_db it's only one table (actually two one for the WHP dataset and one for the IDIEZ dataset) while for the \n",
    "# scraped_db there are multiple tables due to the relationl structure we want to keep\n",
    "tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", local_db)\n",
    "WHP_TABLE = \"checkpoint_after_empty_p_tag_removal_20251002\"\n",
    "IDIEZ_TABLE = \"IDIEZ_modern_nahuatl-all-2024-03-27T09-45-31\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "dcfb3631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map scraped DB fields to local DB fields for WHP data\n",
    "WHP_FIELD_MAPPING = {\n",
    "    # Scraped field: Local field\n",
    "    'node_id': 'Ref',\n",
    "    'headword': 'Headword',\n",
    "    'orthographic_variants': 'Orthographic Variants',\n",
    "    'translation_english': 'Principal English Translation',\n",
    "    'spanish_loanword': 'Spanish Loanword',\n",
    "    # Authority fields (stored in authority_citations table in scraped DB)\n",
    "    'authority_molina': 'Alonso de Molina',\n",
    "    'authority_karttunen': 'Frances Karttunen',\n",
    "    'authority_carochi': 'Horacio Carochi / English',\n",
    "    'authority_olmos': 'Andrés de Olmos',\n",
    "    'authority_lockhart': \"Lockhart’s Nahuatl as Written\",\n",
    "    # Attestations (stored in attestations table in scraped DB)\n",
    "    'attestations_english': 'Attestations from sources in English',\n",
    "    'attestations_spanish': 'Attestations from sources in Spanish',\n",
    "    # Metadata\n",
    "    'themes': 'themes',\n",
    "}\n",
    "\n",
    "# Map scraped DB fields to local DB fields for IDIEZ data\n",
    "IDIEZ_FIELD_MAPPING = {\n",
    "    'node_id': 'Ref',\n",
    "    'headword_idiez': 'tlahtolli',\n",
    "    'translation_english_idiez': 'IDIEZ traduc. inglés',\n",
    "    'definition_nahuatl_idiez': 'IDIEZ def. náhuatl',\n",
    "    'definition_spanish_idiez': 'IDIEZ def. español',\n",
    "    'morfologia_idiez': 'IDIEZ morfología',\n",
    "    'gramatica_idiez': 'IDIEZ gramática',\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "2ee651a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValidationReport:\n",
    "    \"\"\"Track validation results across tables\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reports = {}\n",
    "        self.summary = {\n",
    "            'total_tables_validated': 0,\n",
    "            'total_discrepancies': 0,\n",
    "            'total_updates_needed': 0,\n",
    "            'validation_timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def add_table_report(self, table_name: str, report: Dict):\n",
    "        \"\"\"Add validation report for a table\"\"\"\n",
    "        self.reports[table_name] = report\n",
    "        self.summary['total_tables_validated'] += 1\n",
    "        self.summary['total_discrepancies'] += report.get('total_discrepancies', 0)\n",
    "        self.summary['total_updates_needed'] += report.get('updates_needed', 0)\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print overall validation summary\"\"\"\n",
    "        print(\"VALIDATION SUMMARY\")\n",
    "        print(f\"Tables validated: {self.summary['total_tables_validated']}\")\n",
    "        print(f\"Total discrepancies found: {self.summary['total_discrepancies']}\")\n",
    "        print(f\"Total updates needed: {self.summary['total_updates_needed']}\")\n",
    "        print(f\"Validation time: {self.summary['validation_timestamp']}\")\n",
    "        print(\"=\"*70)\n",
    "        \n",
    "        for table_name, report in self.reports.items():\n",
    "            print(f\"\\n{table_name}:\")\n",
    "            print(f\"  - Rows compared: {report.get('rows_compared', 0):,}\")\n",
    "            print(f\"  - Discrepancies: {report.get('total_discrepancies', 0):,}\")\n",
    "            print(f\"  - Updates needed: {report.get('updates_needed', 0):,}\")\n",
    "            if 'field_discrepancies' in report:\n",
    "                print(f\"  - Fields with issues: {', '.join(report['field_discrepancies'].keys())}\")\n",
    "    \n",
    "    def save_report(self, filepath: str):\n",
    "        \"\"\"Save detailed report to JSON\"\"\"\n",
    "        import json\n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump({\n",
    "                'summary': self.summary,\n",
    "                'detailed_reports': self.reports\n",
    "            }, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"Report saved to: {filepath}\")\n",
    "\n",
    "# init validation report\n",
    "validation_report = ValidationReport()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "34c55cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to compare data between scraped and local DBs\n",
    "from turtle import right\n",
    "\n",
    "\n",
    "def compare_dataframes(df_scraped, df_local, field_mapping, key_field='node_id'):\n",
    "    # compares two dfs field by field\n",
    "    scraped_key = field_mapping.get(key_field, key_field)\n",
    "    df_scraped = df_scraped.copy()\n",
    "    df_local = df_local.copy()\n",
    "    df_scraped[key_field] = df_scraped[key_field].astype(str)\n",
    "    df_local[scraped_key] = df_local[scraped_key].astype(str)\n",
    "    df_local[scraped_key] = df_local[scraped_key].astype(str).str.replace(r'^(WHP-|IDIEZ-)', '', regex=True)\n",
    "    # merge \n",
    "    merged = df_scraped.merge(\n",
    "        df_local,\n",
    "        left_on = key_field,\n",
    "        right_on = scraped_key,\n",
    "        how = 'inner',\n",
    "        suffixes=('_scraped', '_local')\n",
    "    )\n",
    "    print(f\"Rows in scraped DB: {len(df_scraped):,}\")\n",
    "    print(f\"Rows in local DB: {len(df_local):,}\")\n",
    "    print(f\"Rows matched: {len(merged):,}\")\n",
    "    \n",
    "    discrepancies = {\n",
    "        'field_discrepancies': {},\n",
    "        'total_discrepancies': 0,\n",
    "        'rows_compared': len(merged),\n",
    "        'sample_discrepancies': []\n",
    "    }\n",
    "    # compare each field\n",
    "    for scraped_field, local_field in field_mapping.items():\n",
    "        if scraped_field == key_field:\n",
    "            continue\n",
    "        if scraped_field not in df_scraped.columns or local_field not in df_local.columns:\n",
    "            print(f\"Skipping {scraped_field} (not in both datasets)\")\n",
    "            continue\n",
    "        scraped_col = f\"{scraped_field}_scraped\" if scraped_field in df_local.columns else scraped_field\n",
    "        local_col = f\"{local_field}_local\" if local_field in df_scraped.columns else local_field\n",
    "        \n",
    "        # Find differences\n",
    "        scraped_values = merged[scraped_col].fillna('').astype(str).str.strip()\n",
    "        local_values = merged[local_col].fillna('').astype(str).str.strip()\n",
    "        \n",
    "        # Find differences\n",
    "        merged['diff'] = ~(\n",
    "            (scraped_values == local_values) |\n",
    "            (merged[scraped_col].isna() & merged[local_col].isna())\n",
    "        )\n",
    "        diff_count = merged['diff'].sum()\n",
    "        \n",
    "        if diff_count > 0:\n",
    "            print(f\"{scraped_field:30} {diff_count:>6,} discrepancies\")\n",
    "            discrepancies['field_discrepancies'][scraped_field] = {\n",
    "                'count': int(diff_count),\n",
    "                'local_field': local_field,\n",
    "                'sample_rows': merged[merged['diff']][key_field].head(10).tolist()\n",
    "            }\n",
    "            discrepancies['total_discrepancies'] += int(diff_count)\n",
    "            \n",
    "            if len(discrepancies['sample_discrepancies']) < 5:\n",
    "                sample = merged[merged['diff']].iloc[0]\n",
    "                discrepancies['sample_discrepancies'].append({\n",
    "                    'node_id': sample[key_field],\n",
    "                    'field': scraped_field,\n",
    "                    'scraped_value': str(sample[scraped_col])[:100],\n",
    "                    'local_value': str(sample[local_col])[:100]\n",
    "                })\n",
    "        else:\n",
    "            print(f\"{scraped_field:30} all match\")\n",
    "    \n",
    "    print(f\"Total discrepancies: {discrepancies['total_discrepancies']:,}\")\n",
    "    return discrepancies\n",
    "\n",
    "def create_update_dataframe(scraped_df, local_df, field_mapping, key_field='node_id'):\n",
    "    \"\"\"Create dataframe showing what needs updating\"\"\"\n",
    "    scraped_key = field_mapping.get(key_field, key_field)\n",
    "    \n",
    "    scraped_df = scraped_df.copy()\n",
    "    local_df = local_df.copy()\n",
    "    scraped_df[key_field] = scraped_df[key_field].astype(str)\n",
    "    local_df[scraped_key] = local_df[scraped_key].astype(str).str.replace(r'^(WHP-|IDIEZ-)', '', regex=True)\n",
    "    \n",
    "    merged = scraped_df.merge(\n",
    "        local_df,\n",
    "        left_on=key_field,\n",
    "        right_on=scraped_key,\n",
    "        how='inner',\n",
    "        suffixes=('_scraped', '_local')\n",
    "    )\n",
    "    \n",
    "    updates = []\n",
    "    \n",
    "    for scraped_field, local_field in field_mapping.items():\n",
    "        if scraped_field == key_field:\n",
    "            continue\n",
    "        \n",
    "        if scraped_field not in scraped_df.columns or local_field not in local_df.columns:\n",
    "            continue\n",
    "        \n",
    "        scraped_col = f\"{scraped_field}_scraped\" if scraped_field in local_df.columns else scraped_field\n",
    "        local_col = f\"{local_field}_local\" if local_field in scraped_df.columns else local_field\n",
    "        scraped_values = merged[scraped_col].fillna('').astype(str).str.strip()\n",
    "        local_values = merged[local_col].fillna('').astype(str).str.strip()\n",
    "        \n",
    "        diff_mask = ~(\n",
    "            (scraped_values == local_values) |\n",
    "            (merged[scraped_col].isna() & merged[local_col].isna())\n",
    "        )\n",
    "        \n",
    "        diff_rows = merged[diff_mask]\n",
    "        \n",
    "        for idx, row in diff_rows.iterrows():\n",
    "            # Store STRIPPED values\n",
    "            current_stripped = str(row[scraped_col]).strip() if pd.notna(row[scraped_col]) else ''\n",
    "            new_stripped = str(row[local_col]).strip() if pd.notna(row[local_col]) else ''\n",
    "            \n",
    "            updates.append({\n",
    "                'node_id': row[key_field],\n",
    "                'field': scraped_field,\n",
    "                'current_value': current_stripped,\n",
    "                'new_value': new_stripped,\n",
    "                'action': 'UPDATE'\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(updates)  \n",
    "\n",
    "def apply_updates(updates_df, conn, table_name='dictionary_entries', dry_run=True):\n",
    "    # apply updates to db\n",
    "    print(f\"APPLYING UPDATES TO {table_name}\")\n",
    "    print(f\"Mode: {'DRY RUN' if dry_run else 'LIVE UPDATES'}\")\n",
    "    stats = {\n",
    "        'total_updates': 0,\n",
    "        'successful_updates': 0,\n",
    "        'failed_updates': 0,\n",
    "        'updates_by_field': defaultdict(int)\n",
    "    }\n",
    "    \n",
    "    cursor = conn.cursor()\n",
    "    grouped = updates_df.groupby('node_id')\n",
    "    \n",
    "    for node_id, group in grouped:\n",
    "        try:\n",
    "            set_clauses = []\n",
    "            values = []\n",
    "            \n",
    "            for _, row in group.iterrows():\n",
    "                set_clauses.append(f\"{row['field']} = ?\")\n",
    "                values.append(row['new_value'])\n",
    "                stats['updates_by_field'][row['field']] += 1\n",
    "            \n",
    "            values.append(node_id)\n",
    "            sql = f\"UPDATE {table_name} SET {', '.join(set_clauses)} WHERE node_id = ?\"\n",
    "            \n",
    "            if not dry_run:\n",
    "                cursor.execute(sql, values)\n",
    "            \n",
    "            stats['successful_updates'] += len(group)\n",
    "            stats['total_updates'] += len(group)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error updating node_id {node_id}: {e}\")\n",
    "            stats['failed_updates'] += len(group)\n",
    "            stats['total_updates'] += len(group)\n",
    "    \n",
    "    if not dry_run:\n",
    "        conn.commit()\n",
    "        print(\"Changes committed\")\n",
    "    else:\n",
    "        print(\"Dry run complete - no changes made\")\n",
    "    \n",
    "    print(f\"\\nStatistics:\")\n",
    "    print(f\"  Total updates: {stats['total_updates']:,}\")\n",
    "    print(f\"  Successful: {stats['successful_updates']:,}\")\n",
    "    print(f\"  Failed: {stats['failed_updates']:,}\")\n",
    "    print(f\"\\nUpdates by field:\")\n",
    "    for field, count in stats['updates_by_field'].items():\n",
    "        print(f\"  - {field}: {count:,}\")\n",
    "    \n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48516e8d",
   "metadata": {},
   "source": [
    "for me it methodically makes sense to go down the tables that nahuat.db has (check config/schema.sql) and then check each one against the WHP_table_name and the IDIEZ_table_name as such let's begin with the largest tables first and build up from the tables. \n",
    "it would also be smart that as we cross validate we proceed to investigate where the cross references columns actually come from\n",
    "also a side note since we've done no manual corrections or fixes to the IDIEZ fields we can begin with validating local IDIEZ with scraped IDIEZ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a66c4992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6,846 IDIEZ/HYBRID entries from scraped DB\n",
      "Loaded 6,846 IDIEZ entries from local DB\n",
      "Rows in scraped DB: 6,846\n",
      "Rows in local DB: 6,846\n",
      "Rows matched: 6,844\n",
      "headword_idiez                 all match\n",
      "translation_english_idiez          11 discrepancies\n",
      "definition_nahuatl_idiez           24 discrepancies\n",
      "definition_spanish_idiez          425 discrepancies\n",
      "morfologia_idiez               all match\n",
      "gramatica_idiez                all match\n",
      "Total discrepancies: 460\n",
      "\n",
      "Sample IDIEZ discrepancies:\n",
      "\n",
      "1. node_id=187252, field=translation_english_idiez\n",
      "   Scraped: \n",
      "   Local:   to transport rocks.\n",
      "\n",
      "2. node_id=176130, field=definition_nahuatl_idiez\n",
      "   Scraped: ICPATL. tlat. Tlamalintli iloh, iixnezca chipahuac quitequihuah ica quichihchihuah cantelah o ica tl\n",
      "   Local:   ICPATL. tlat. Tlamalintli iloh, iixnezca chipahuac quitequihuah ica quichihchihuah cantelah o ica tl\n",
      "\n",
      "3. node_id=172003, field=definition_spanish_idiez\n",
      "   Scraped: A.1. se enfrìa. “Se enfría la tierra de noche. 2. Se va la luz. “Cuando hace truenos se enfrìa la ti\n",
      "   Local:   \tA.1. se enfrìa. “Se enfría la tierra de noche. 2. Se va la luz. “Cuando hace truenos se enfrìa la t\n"
     ]
    }
   ],
   "source": [
    "scraped_idiez = pd.read_sql(\"\"\"\n",
    "    SELECT \n",
    "        node_id,\n",
    "        headword_idiez,\n",
    "        translation_english_idiez,\n",
    "        definition_nahuatl_idiez,\n",
    "        definition_spanish_idiez,\n",
    "        morfologia_idiez,\n",
    "        gramatica_idiez,\n",
    "        source_dataset\n",
    "    FROM dictionary_entries\n",
    "    WHERE source_dataset IN ('IDIEZ', 'HYBRID')\n",
    "\"\"\", scraped_db)\n",
    "\n",
    "print(f\"Loaded {len(scraped_idiez):,} IDIEZ/HYBRID entries from scraped DB\")\n",
    "\n",
    "# Load IDIEZ from local DB\n",
    "local_idiez = pd.read_sql(f\"\"\"\n",
    "    SELECT \n",
    "        Ref,\n",
    "        tlahtolli,\n",
    "        \"IDIEZ traduc. inglés\",\n",
    "        \"IDIEZ def. náhuatl\",\n",
    "        \"IDIEZ def. español\",\n",
    "        \"IDIEZ morfología\",\n",
    "        \"IDIEZ gramática\"\n",
    "    FROM [{IDIEZ_TABLE}]\n",
    "\"\"\", local_db)\n",
    "\n",
    "print(f\"Loaded {len(local_idiez):,} IDIEZ entries from local DB\")\n",
    "\n",
    "# Compare\n",
    "idiez_discrepancies = compare_dataframes(\n",
    "    scraped_idiez,\n",
    "    local_idiez,\n",
    "    IDIEZ_FIELD_MAPPING,\n",
    "    key_field='node_id'\n",
    ")\n",
    "\n",
    "# Show samples\n",
    "if idiez_discrepancies['sample_discrepancies']:\n",
    "    print(\"\\nSample IDIEZ discrepancies:\")\n",
    "    for i, sample in enumerate(idiez_discrepancies['sample_discrepancies'][:5], 1):\n",
    "        print(f\"\\n{i}. node_id={sample['node_id']}, field={sample['field']}\")\n",
    "        print(f\"   Scraped: {sample['scraped_value']}\")\n",
    "        print(f\"   Local:   {sample['local_value']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6d5a8706",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating IDIEZ update report...\n",
      "Total IDIEZ updates needed: 460\n",
      "\n",
      "Updates by field:\n",
      "field\n",
      "definition_spanish_idiez     425\n",
      "definition_nahuatl_idiez      24\n",
      "translation_english_idiez     11\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Update report saved to: idiez_updates_needed.csv\n"
     ]
    }
   ],
   "source": [
    "# creating the IDIEZ report\n",
    "if idiez_discrepancies['total_discrepancies'] > 0:\n",
    "    print(\"\\nCreating IDIEZ update report...\")\n",
    "    \n",
    "    idiez_updates = create_update_dataframe(\n",
    "        scraped_idiez,\n",
    "        local_idiez,\n",
    "        IDIEZ_FIELD_MAPPING\n",
    "    )\n",
    "    \n",
    "    print(f\"Total IDIEZ updates needed: {len(idiez_updates):,}\")\n",
    "    print(\"\\nUpdates by field:\")\n",
    "    print(idiez_updates['field'].value_counts())\n",
    "    \n",
    "    # Save report\n",
    "    idiez_updates.to_csv('idiez_updates_needed.csv', index=False, encoding='utf-8-sig')\n",
    "    print(\"\\nUpdate report saved to: idiez_updates_needed.csv\")\n",
    "else:\n",
    "    print(\"\\nNo IDIEZ updates needed - data matches perfectly!\")\n",
    "    idiez_updates = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "32fddbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now will be validating whp entries\n",
    "# scraped_whp = pd.read_sql(\"\"\"\n",
    "#     SELECT \n",
    "#         node_id,\n",
    "#         headword,\n",
    "#         orthographic_variants,\n",
    "#         translation_english,\n",
    "#         spanish_loanword,\n",
    "#         source_dataset\n",
    "#     FROM dictionary_entries\n",
    "#     WHERE source_dataset IN ('WHP', 'HYBRID')\n",
    "# \"\"\", scraped_db)\n",
    "\n",
    "# print(f\"Loaded {len(scraped_whp):,} WHP/HYBRID entries from scraped DB\")\n",
    "\n",
    "# # Load WHP from local DB\n",
    "# local_whp = pd.read_sql(f\"\"\"\n",
    "#     SELECT \n",
    "#         Ref,\n",
    "#         Headword,\n",
    "#         \"Orthographic Variants\",\n",
    "#         \"Principal English Translation\",\n",
    "#         \"Spanish Loanword\"\n",
    "#     FROM [{WHP_TABLE}]\n",
    "# \"\"\", local_db)\n",
    "\n",
    "# print(f\"Loaded {len(local_whp):,} WHP entries from local DB\")\n",
    "\n",
    "# # Compare\n",
    "# whp_discrepancies = compare_dataframes(\n",
    "#     scraped_whp,\n",
    "#     local_whp,\n",
    "#     WHP_FIELD_MAPPING,\n",
    "#     key_field='node_id'\n",
    "# )\n",
    "\n",
    "# # Show samples\n",
    "# if whp_discrepancies['sample_discrepancies']:\n",
    "#     print(\"\\nSample WHP discrepancies:\")\n",
    "#     for i, sample in enumerate(whp_discrepancies['sample_discrepancies'][:5], 1):\n",
    "#         print(f\"\\n{i}. node_id={sample['node_id']}, field={sample['field']}\")\n",
    "#         print(f\"   Scraped: {sample['scraped_value']}\")\n",
    "#         print(f\"   Local:   {sample['local_value']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "03b0d98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database connections closed\n"
     ]
    }
   ],
   "source": [
    "scraped_db.close()\n",
    "local_db.close()\n",
    "print(\"Database connections closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nahuaLEX_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
