{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df95143d",
   "metadata": {},
   "source": [
    "# Nahuatl Notebook for the WHP_EarlyNahuatl_Dataset\n",
    "\n",
    "This notebook processes Nahuatl dictionary data, analyzing HTML tags, repairing malformed tags, and extracting citations and cross-references. This is a merged version of Todd's version and I where there is a SQLite-based data management approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8105d6",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f542ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Optional, Union, Any\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Character encoding detection\n",
    "import chardet\n",
    "from unidecode import unidecode\n",
    "\n",
    "# HTML/XML processing\n",
    "from bs4 import BeautifulSoup\n",
    "from inscriptis import get_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "60814645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration constants\n",
    "WORKING_DIR = 'working_files'\n",
    "DB_PATH = '../../data/sqLiteDb/nahuatl_processing.db'  # Single database for all operations\n",
    "DEFAULT_ENCODING = 'utf-8-sig'  # This encoding is the encoding that works when outputing to CSV files to encode certain Nahuatl characters properly.\n",
    "CHECKPOINT_STAGES = ['initial', 'cleaned', 'final']  # Only save these to SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1d64e",
   "metadata": {},
   "source": [
    "## Step 2: Data Import and Working Copy Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0c34f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Unified data loader for CSV and SQLite sources\"\"\"\n",
    "    \n",
    "    def __init__(self, encoding: str = DEFAULT_ENCODING):\n",
    "        self.encoding = encoding\n",
    "        \n",
    "    def detect_encoding(self, filepath: str) -> str:\n",
    "        \"\"\"Auto-detect file encoding if needed\"\"\"\n",
    "        with open(filepath, 'rb') as file:\n",
    "            raw_data = file.read(10000)  # Read first 10KB\n",
    "            result = chardet.detect(raw_data)\n",
    "            return result['encoding'] or 'utf-8'\n",
    "    \n",
    "    def load_from_csv(self, filepath: str, auto_detect_encoding: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Load data from CSV with encoding protection\"\"\"\n",
    "        \n",
    "        # Detect encoding if requested\n",
    "        encoding = self.detect_encoding(filepath) if auto_detect_encoding else self.encoding\n",
    "        \n",
    "        print(f\"Loading data from CSV: {filepath}\")\n",
    "        print(f\"Encoding: {encoding}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                filepath,\n",
    "                encoding=encoding,\n",
    "                na_values=[''],\n",
    "                keep_default_na=True,\n",
    "                dtype=str,  # Keep everything as strings initially\n",
    "                low_memory=False\n",
    "            )\n",
    "            \n",
    "            print(f\"Successfully loaded {len(df):,} rows × {len(df.columns)} columns\")\n",
    "            print(f\"Columns: {list(df.columns)[:5]}...\" if len(df.columns) > 5 else f\"   Columns: {list(df.columns)}\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"Encoding error: {e}\")\n",
    "            print(\"Attempting with auto-detected encoding...\")\n",
    "            return self.load_from_csv(filepath, auto_detect_encoding=True)\n",
    "    \n",
    "    def load_from_sqlite(self, db_path: str, table_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Load data from SQLite database\"\"\"\n",
    "        \n",
    "        print(f\"Loading data from SQLite: {db_path}\")\n",
    "        print(f\"Table: {table_name}\")\n",
    "        \n",
    "        with sqlite3.connect(db_path) as conn:\n",
    "            # Check if table exists\n",
    "            tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", conn)\n",
    "            if table_name not in tables['name'].values:\n",
    "                raise ValueError(f\"Table '{table_name}' not found in database\")\n",
    "            \n",
    "            df = pd.read_sql(f\"SELECT * FROM [{table_name}]\", conn)\n",
    "            \n",
    "        print(f\"Successfully loaded {len(df):,} rows × {len(df.columns)} columns\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def create_working_copy(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Create a working copy while preserving the original\"\"\"\n",
    "        original_df = df.copy(deep=True)\n",
    "        working_df = df.copy(deep=True)\n",
    "        \n",
    "        print(f\"Created working copy of data\")\n",
    "        \n",
    "        return original_df, working_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f10329",
   "metadata": {},
   "source": [
    "## Step 3: Intermediate Save Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "aa38701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSaver:\n",
    "    \"\"\"Handles saving data at various stages with encoding protection\"\"\"\n",
    "    \n",
    "    def __init__(self, working_dir: str = WORKING_DIR, encoding: str = DEFAULT_ENCODING):\n",
    "        self.working_dir = working_dir\n",
    "        self.encoding = encoding\n",
    "        os.makedirs(working_dir, exist_ok=True)\n",
    "        \n",
    "    def save_to_csv(self, df: pd.DataFrame, filename: str, add_timestamp: bool = True) -> str:\n",
    "        \"\"\"Save DataFrame to CSV with encoding protection\"\"\"\n",
    "        \n",
    "        if add_timestamp:\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            base_name = filename.replace('.csv', '')\n",
    "            filename = f\"{base_name}_{timestamp}.csv\"\n",
    "        \n",
    "        filepath = os.path.join(self.working_dir, filename)\n",
    "        \n",
    "        df.to_csv(\n",
    "            filepath,\n",
    "            index=False,\n",
    "            encoding=self.encoding,\n",
    "            na_rep='',  # Explicit NA representation\n",
    "            quoting=csv.QUOTE_MINIMAL\n",
    "        )\n",
    "        \n",
    "        print(f\"Saved to CSV: {filepath}\")\n",
    "        return filepath\n",
    "    \n",
    "    def save_checkpoint_to_sqlite(self, df: pd.DataFrame, checkpoint_name: str, conn: sqlite3.Connection) -> None:\n",
    "        \"\"\"Save checkpoint to SQLite (only for critical stages)\"\"\"\n",
    "        \n",
    "        table_name = f\"checkpoint_{checkpoint_name}_{datetime.now().strftime('%Y%m%d')}\"\n",
    "        \n",
    "        df.to_sql(\n",
    "            table_name,\n",
    "            conn,\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            dtype='text'  # Store everything as text to preserve formatting\n",
    "        )\n",
    "        \n",
    "        print(f\"Checkpoint saved to SQLite: {table_name}\")\n",
    "        \n",
    "        # Also save metadata\n",
    "        metadata = pd.DataFrame([{\n",
    "            'checkpoint_name': checkpoint_name,\n",
    "            'table_name': table_name,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'row_count': len(df),\n",
    "            'column_count': len(df.columns),\n",
    "            'columns': ','.join(df.columns)\n",
    "        }])\n",
    "        \n",
    "        metadata.to_sql(\n",
    "            'checkpoint_metadata',\n",
    "            conn,\n",
    "            if_exists='append',\n",
    "            index=False\n",
    "        )\n",
    "    \n",
    "    def save_to_excel(self, data_dict: Dict[str, pd.DataFrame], filename: str) -> str:\n",
    "        \"\"\"Save multiple DataFrames to Excel file\"\"\"\n",
    "        \n",
    "        filepath = os.path.join(self.working_dir, filename)\n",
    "        \n",
    "        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n",
    "            for sheet_name, df in data_dict.items():\n",
    "                # Excel sheet name limit is 31 characters\n",
    "                clean_sheet_name = sheet_name[:31] if len(sheet_name) > 31 else sheet_name\n",
    "                df.to_excel(writer, sheet_name=clean_sheet_name, index=False)\n",
    "        \n",
    "        print(f\"Saved to Excel: {filepath}\")\n",
    "        return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e845f65",
   "metadata": {},
   "source": [
    "## Step 4: Database Initialization and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "fb9a1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseManager:\n",
    "    \"\"\"Manages SQLite database connection and operations\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = DB_PATH):\n",
    "        self.db_path = db_path\n",
    "        self.conn = None\n",
    "        self._initialize_database()\n",
    "    \n",
    "    def _initialize_database(self):\n",
    "        \"\"\"Initialize database and create necessary tables\"\"\"\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        db_dir = os.path.dirname(self.db_path)\n",
    "        if db_dir:\n",
    "            os.makedirs(db_dir, exist_ok=True)\n",
    "        \n",
    "        # Connect to database\n",
    "        self.conn = sqlite3.connect(self.db_path)\n",
    "        \n",
    "        print(f\"Database initialized: {self.db_path}\")\n",
    "        \n",
    "        # Create metadata tables if they don't exist\n",
    "        self._create_metadata_tables()\n",
    "        \n",
    "        # Show existing tables\n",
    "        self._show_database_info()\n",
    "    \n",
    "    def _create_metadata_tables(self):\n",
    "        \"\"\"Create metadata tables for tracking processing\"\"\"\n",
    "        \n",
    "        # Checkpoint metadata table\n",
    "        if self.conn is None:\n",
    "            raise ValueError(\"Database connection is not initialized.\")\n",
    "        \n",
    "        self.conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS checkpoint_metadata (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            checkpoint_name TEXT,\n",
    "            table_name TEXT,\n",
    "            timestamp TEXT,\n",
    "            row_count INTEGER,\n",
    "            column_count INTEGER,\n",
    "            columns TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Processing log table\n",
    "        self.conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS processing_log (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                stage TEXT,\n",
    "                status TEXT,\n",
    "                timestamp TEXT,\n",
    "                duration_seconds REAL,\n",
    "                rows_processed INTEGER,\n",
    "                notes TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        self.conn.commit()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def import_initial_dataset(self, csv_path: str, table_name: str = 'WHP_EarlyNahuatl_Data', \n",
    "                              encoding: str = DEFAULT_ENCODING, replace: bool = False) -> bool:\n",
    "        \"\"\"\n",
    "        Import initial CSV dataset into SQLite as the base reference.\n",
    "        This is a ONE-TIME operation to establish the source data in the database.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"INITIAL DATASET IMPORT TO SQLITE\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Check if table already exists\n",
    "        existing_tables = pd.read_sql(\n",
    "            \"SELECT name FROM sqlite_master WHERE type='table'\",\n",
    "            self.conn\n",
    "        )\n",
    "        \n",
    "        if table_name in existing_tables['name'].values:\n",
    "            if not replace:\n",
    "                print(f\"Table '{table_name}' already exists!\")\n",
    "                response = input(\"Do you want to replace it? (yes/no): \").lower()\n",
    "                if response != 'yes':\n",
    "                    print(\"Import cancelled\")\n",
    "                    return False\n",
    "            else:\n",
    "                print(f\" Replacing existing table '{table_name}'\")\n",
    "        \n",
    "        try:\n",
    "            # Load CSV with encoding protection\n",
    "            print(f\"Reading CSV file: {csv_path}\")\n",
    "            print(f\"   Encoding: {encoding}\")\n",
    "            \n",
    "            # First, detect actual encoding if needed\n",
    "            with open(csv_path, 'rb') as file:\n",
    "                raw_data = file.read(10000)\n",
    "                detected = chardet.detect(raw_data)\n",
    "                print(f\"   Detected encoding: {detected['encoding']} (confidence: {detected['confidence']:.2%})\")\n",
    "            \n",
    "            # Load the CSV\n",
    "            df = pd.read_csv(\n",
    "                csv_path,\n",
    "                encoding=encoding,\n",
    "                na_values=[''],\n",
    "                keep_default_na=True,\n",
    "                dtype=str,  # Import everything as text to preserve formatting\n",
    "                low_memory=False\n",
    "            )\n",
    "            \n",
    "            print(f\"Loaded {len(df):,} rows × {len(df.columns)} columns\")\n",
    "            \n",
    "            # Show sample of data\n",
    "            print(\"\\nData Sample (first 3 rows):\")\n",
    "            print(df.head(3).to_string(max_cols=5))\n",
    "            \n",
    "            # Import to SQLite\n",
    "            print(f\"\\nImporting to SQLite table: {table_name}\")\n",
    "            \n",
    "            df.to_sql(\n",
    "                table_name,\n",
    "                self.conn,\n",
    "                if_exists='replace' if replace else 'fail',\n",
    "                index=False,\n",
    "                dtype='text',  # Store as text to preserve all characters\n",
    "                chunksize=5000  # Process in chunks for large datasets\n",
    "            )\n",
    "            \n",
    "            # Create indexes for common query columns\n",
    "            print(\"Creating indexes...\")\n",
    "            \n",
    "            if self.conn is None:\n",
    "                raise ValueError(\"Database connection is not initialized.\")\n",
    "        \n",
    "            # Add index on Ref column if it exists\n",
    "            if 'Ref' in df.columns:\n",
    "                self.conn.execute(f\"CREATE INDEX IF NOT EXISTS idx_{table_name}_ref ON {table_name}(Ref)\")\n",
    "            \n",
    "            # Add index on Headword column if it exists  \n",
    "            if 'Headword' in df.columns:\n",
    "                self.conn.execute(f\"CREATE INDEX IF NOT EXISTS idx_{table_name}_headword ON {table_name}(Headword)\")\n",
    "            \n",
    "            self.conn.commit()\n",
    "            \n",
    "            # Verify import\n",
    "            verify_count = pd.read_sql(f\"SELECT COUNT(*) as cnt FROM {table_name}\", self.conn).iloc[0, 0]\n",
    "            \n",
    "            if verify_count == len(df):\n",
    "                print(f\"Successfully imported {verify_count:,} rows\")\n",
    "                \n",
    "                # Log the import\n",
    "                import_metadata = pd.DataFrame([{\n",
    "                    'operation': 'initial_import',\n",
    "                    'source_file': csv_path,\n",
    "                    'table_name': table_name,\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'row_count': verify_count,\n",
    "                    'column_count': len(df.columns),\n",
    "                    'columns': ','.join(df.columns),\n",
    "                    'encoding_used': encoding\n",
    "                }])\n",
    "                \n",
    "                import_metadata.to_sql(\n",
    "                    'import_history',\n",
    "                    self.conn,\n",
    "                    if_exists='append',\n",
    "                    index=False\n",
    "                )\n",
    "                \n",
    "                print(\"Import logged to metadata\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Row count mismatch! Expected {len(df)}, got {verify_count}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Import failed: {e}\")\n",
    "            return False\n",
    "        \n",
    "        \n",
    "    def _show_database_info(self):\n",
    "        \"\"\"Display information about the database\"\"\"\n",
    "        \n",
    "        tables = pd.read_sql(\n",
    "            \"SELECT name, type FROM sqlite_master WHERE type IN ('table', 'view')\",\n",
    "            self.conn\n",
    "        )\n",
    "        \n",
    "        if not tables.empty:\n",
    "            print(f\"Existing database objects: {len(tables)}\")\n",
    "            for _, row in tables.iterrows():\n",
    "                # Get row count for tables\n",
    "                if row['type'] == 'table':\n",
    "                    try:\n",
    "                        count = pd.read_sql(f\"SELECT COUNT(*) as cnt FROM [{row['name']}]\", self.conn).iloc[0, 0]\n",
    "                        print(f\"   - {row['name']} ({row['type']}): {count:,} rows\")\n",
    "                    except:\n",
    "                        print(f\"   - {row['name']} ({row['type']})\")\n",
    "        else:\n",
    "            print(\"No existing tables in database\")\n",
    "    \n",
    "    def log_processing_stage(self, stage: str, status: str, \n",
    "                        duration: Optional[float] = None, \n",
    "                        rows_processed: Optional[int] = None, \n",
    "                        notes: Optional[str] = None):\n",
    "        \"\"\"Log processing stage to database\"\"\"\n",
    "        \n",
    "        log_entry = pd.DataFrame([{\n",
    "            'stage': stage,\n",
    "            'status': status,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'duration_seconds': duration,\n",
    "            'rows_processed': rows_processed,\n",
    "            'notes': notes\n",
    "        }])\n",
    "        \n",
    "        log_entry.to_sql('processing_log', self.conn, if_exists='append', index=False)\n",
    "    \n",
    "    \n",
    "    def verify_base_dataset(self, table_name: str = 'WHP_EarlyNahuatl_Data') -> bool:\n",
    "        \"\"\"Verify that base dataset exists and is valid\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Check if table exists\n",
    "            tables = pd.read_sql(\n",
    "                f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}'\",\n",
    "                self.conn\n",
    "            )\n",
    "            \n",
    "            if tables.empty:\n",
    "                print(f\"Base dataset table '{table_name}' not found\")\n",
    "                return False\n",
    "            \n",
    "            # Get basic info\n",
    "            count = pd.read_sql(f\"SELECT COUNT(*) as cnt FROM {table_name}\", self.conn).iloc[0, 0]\n",
    "            cols = pd.read_sql(f\"PRAGMA table_info({table_name})\", self.conn)\n",
    "            \n",
    "            print(f\"   Base dataset verified:\")\n",
    "            print(f\"   Table: {table_name}\")\n",
    "            print(f\"   Rows: {count:,}\")\n",
    "            print(f\"   Columns: {len(cols)}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Verification failed: {e}\")\n",
    "            return False\n",
    "        \n",
    "        \n",
    "    def get_checkpoint_list(self) -> pd.DataFrame:\n",
    "        \"\"\"Get list of available checkpoints\"\"\"\n",
    "        \n",
    "        try:\n",
    "            checkpoints = pd.read_sql(\n",
    "                \"SELECT * FROM checkpoint_metadata ORDER BY timestamp DESC\",\n",
    "                self.conn\n",
    "            )\n",
    "            return checkpoints\n",
    "        except:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_name: Optional[str] = None, \n",
    "                   table_name: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Load a specific checkpoint\"\"\"\n",
    "        \n",
    "        if table_name:\n",
    "            return pd.read_sql(f\"SELECT * FROM [{table_name}]\", self.conn)\n",
    "        elif checkpoint_name:\n",
    "            # Get most recent checkpoint with this name\n",
    "            query = \"\"\"\n",
    "                SELECT table_name \n",
    "                FROM checkpoint_metadata \n",
    "                WHERE checkpoint_name = ? \n",
    "                ORDER BY timestamp DESC \n",
    "                LIMIT 1\n",
    "            \"\"\"\n",
    "            result = pd.read_sql(query, self.conn, params=[checkpoint_name])\n",
    "            if not result.empty:\n",
    "                table_name = str(result.iloc[0, 0])\n",
    "                return pd.read_sql(f\"SELECT * FROM [{table_name}]\", self.conn)\n",
    "        \n",
    "        raise ValueError(\"No checkpoint found\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close database connection\"\"\"\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            print(\"Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24e398",
   "metadata": {},
   "source": [
    "## Step 5: HTML Tag Handler (Detection + Repair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2c084831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLTagHandler:\n",
    "    \"\"\"Handles HTML tag detection and repair\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Valid HTML tags\n",
    "        self.valid_tags = {\n",
    "            'p', 'br', 'div', 'span', 'a', 'b', 'i', 'u', 'strong', 'em',\n",
    "            'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'li', 'table',\n",
    "            'tr', 'td', 'th', 'img', 'link', 'meta', 'head', 'body', 'html',\n",
    "            'bibl', 'title', 'sup', 'sub', 'del'\n",
    "        }\n",
    "        \n",
    "        # Known malformations and their fixes\n",
    "        self.malformed_fixes = {\n",
    "            '</p</bibl>': '</p></bibl>',       \n",
    "            '<bibl<': '<bibl>',\n",
    "            '</bibbl>': '</bibl>',\n",
    "            '<bibbl>': '<bibl>',\n",
    "            '<bobl>': '<bibl>',\n",
    "            '</bobl>': '</bibl>',\n",
    "            '<b9bl>': '<bibl>',\n",
    "            '<bibi>': '<bibl>',\n",
    "            '<bibl></p>': '</bibl></p>',\n",
    "        }\n",
    "    \n",
    "    def find_all_tags(self, text: str) -> List[str]:\n",
    "        \"\"\"Find all angle bracket patterns\"\"\"\n",
    "        if pd.isna(text) or not text:\n",
    "            return []\n",
    "        return re.findall(r'</?[^<>]+/?>', str(text))\n",
    "    \n",
    "    def classify_tag(self, tag: str) -> Tuple[str, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Classify a tag as: valid_html, malformed_html, or non_html\n",
    "        Returns: (tag_type, suggested_fix)\n",
    "        \"\"\"\n",
    "        # Check if it's a known malformation\n",
    "        if tag in self.malformed_fixes:\n",
    "            return 'malformed_html', self.malformed_fixes[tag]\n",
    "        \n",
    "        # Extract tag name\n",
    "        match = re.match(r'^</?([^>\\s/]+)', tag)\n",
    "        if match:\n",
    "            tag_name = match.group(1).lower()\n",
    "            if tag_name in self.valid_tags:\n",
    "                return 'valid_html', None\n",
    "            \n",
    "            # Check if it looks like HTML but misspelled\n",
    "            if any(valid in tag_name for valid in ['bibl', 'p', 'br', 'div']):\n",
    "                # Suggest a fix based on what it looks like\n",
    "                if 'bibl' in tag_name:\n",
    "                    return 'malformed_html', '</bibl>' if tag.startswith('</') else '<bibl>'\n",
    "                return 'malformed_html', tag  # Can't auto-fix\n",
    "        \n",
    "        # Not HTML at all\n",
    "        return 'non_html', None\n",
    "    \n",
    "    def repair_text(self, text: str) -> Tuple[str, int]:\n",
    "        \"\"\"\n",
    "        Repair all malformed HTML tags in text\n",
    "        Returns: (repaired_text, number_of_repairs)\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or not text:\n",
    "            return text, 0\n",
    "        \n",
    "        text_str = str(text)\n",
    "        repairs_made = 0\n",
    "        \n",
    "        # Apply known fixes\n",
    "        for malformed, fix in self.malformed_fixes.items():\n",
    "            if malformed in text_str:\n",
    "                count = text_str.count(malformed)\n",
    "                text_str = text_str.replace(malformed, fix)\n",
    "                repairs_made += count\n",
    "        \n",
    "        # Apply pattern-based fixes\n",
    "        text_str = re.sub(r'<<(\\w+)>', r'<\\1>', text_str)  # Double opening\n",
    "        text_str = re.sub(r'<(\\w+)>>', r'<\\1>', text_str)  # Double closing\n",
    "        \n",
    "        return text_str, repairs_made\n",
    "    \n",
    "    def encode_non_html(self, text: str) -> str:\n",
    "        \"\"\"Encode non-HTML angle brackets\"\"\"\n",
    "        if pd.isna(text) or not text:\n",
    "            return text\n",
    "        \n",
    "        text_str = str(text)\n",
    "        tags = self.find_all_tags(text_str)\n",
    "        \n",
    "        for tag in tags:\n",
    "            tag_type, _ = self.classify_tag(tag)\n",
    "            if tag_type == 'non_html':\n",
    "                encoded = tag.replace('<', '&lt;').replace('>', '&gt;')\n",
    "                text_str = text_str.replace(tag, encoded)\n",
    "        \n",
    "        return text_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def01c3",
   "metadata": {},
   "source": [
    "### Step 5.1 HTML Malformed Tag Repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "5c383d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MalformedTagRepairer:\n",
    "    \"\"\"Specialized handler for malformed HTML tag repair\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.html_analyzer = HTMLTagHandler() \n",
    "        self.html_tags = {\n",
    "            'p', 'br', 'div', 'span', 'a', 'b', 'i', 'u', 'strong', 'em',\n",
    "            'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'li', 'table',\n",
    "            'tr', 'td', 'th', 'img', 'bibl', 'title', 'sup', 'sub', 'del'\n",
    "        }\n",
    "        self.self_closing_tags = {'br', 'hr', 'img', 'input', 'meta', 'link'}\n",
    "        # Specific patterns for actual HTML malformations\n",
    "        self.html_malformation_patterns = {\n",
    "            '</bibbl>': '</bibl>',\n",
    "            '<bibbl>': '<bibl>',\n",
    "            '</bobl>': '</bibl>',\n",
    "            '<bobl>': '<bibl>',\n",
    "            '<b9bl>': '<bibl>',\n",
    "            '<bibi>': '<bibl>',\n",
    "            '</p</bibl>': '</p></bibl>',\n",
    "            '<p<': '<p>',\n",
    "            '</p>p>': '</p>',\n",
    "        }\n",
    "        \n",
    "        self.regex_patterns = {\n",
    "            r'<<(\\w+)>': r'<\\1>',        # Double opening\n",
    "            r'<(\\w+)>>': r'<\\1>',        # Double closing\n",
    "        }\n",
    "        \n",
    "    def detect_mismatches(self, text: str) -> List[tuple]:\n",
    "        \"\"\"Detect tag count mismatches - THIS IS WHAT YOU'RE MISSING\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return []\n",
    "        \n",
    "        mismatches = []\n",
    "        text_str = str(text)\n",
    "        \n",
    "        for tag_name in self.html_tags:\n",
    "            if tag_name in self.self_closing_tags:\n",
    "                continue\n",
    "            \n",
    "            # Count opening and closing tags\n",
    "            open_pattern = f\"<{tag_name}(?:\\\\s+[^>]*)?>\"\n",
    "            close_pattern = f'</{tag_name}>'\n",
    "            \n",
    "            open_count = len(re.findall(open_pattern, text_str, re.IGNORECASE))\n",
    "            close_count = len(re.findall(close_pattern, text_str, re.IGNORECASE))\n",
    "            \n",
    "            if open_count != close_count:\n",
    "                mismatches.append({\n",
    "                    'tag': tag_name,\n",
    "                    'open_count': open_count,\n",
    "                    'close_count': close_count,\n",
    "                    'issue': f'Mismatch: {open_count} open, {close_count} closed'\n",
    "                })\n",
    "        \n",
    "        return mismatches\n",
    "    \n",
    "    \n",
    "    def analyze_malformed_tags(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Analyze both malformed patterns AND mismatches\"\"\"\n",
    "        from collections import Counter\n",
    "        \n",
    "        results = {\n",
    "            'malformed_by_row': [],\n",
    "            'malformed_summary': []\n",
    "        }\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            for col in df.columns:\n",
    "                cell_value = row[col]\n",
    "                if pd.notna(cell_value) and cell_value != '':\n",
    "                    text_str = str(cell_value)\n",
    "                    \n",
    "                    # Check for literal malformed patterns\n",
    "                    for pattern, replacement in self.html_malformation_patterns.items():\n",
    "                        if pattern in text_str:\n",
    "                            results['malformed_by_row'].append({\n",
    "                                'Row': idx,\n",
    "                                'Column': col,\n",
    "                                'Malformed_Tag': pattern,\n",
    "                                'Suggested_Repair': replacement,\n",
    "                                'Context': text_str[:100]\n",
    "                            })\n",
    "                    \n",
    "                    # ADD MISMATCH DETECTION\n",
    "                    mismatches = self.detect_mismatches(text_str)\n",
    "                    for mismatch in mismatches:\n",
    "                        results['malformed_by_row'].append({\n",
    "                            'Row': idx,\n",
    "                            'Column': col,\n",
    "                            'Malformed_Tag': f\"<{mismatch['tag']}>\",\n",
    "                            'Suggested_Repair': mismatch['issue'],\n",
    "                            'Context': text_str[:100]\n",
    "                        })\n",
    "                        \n",
    "        for idx, row in df.iterrows():\n",
    "            for col in df.columns:\n",
    "                cell_value = row[col]\n",
    "                if pd.notna(cell_value) and cell_value != '':\n",
    "                    text_str = str(cell_value)\n",
    "                    \n",
    "                    # Check regex patterns\n",
    "                    for pattern, replacement in self.regex_patterns.items():\n",
    "                        if re.search(pattern, text_str):\n",
    "                            results['malformed_by_row'].append({\n",
    "                                'Row': idx,\n",
    "                                'Column': col,\n",
    "                                'Malformed_Tag': f\"REGEX:{pattern}\",\n",
    "                                'Suggested_Repair': replacement,\n",
    "                                'Context': text_str[:100]\n",
    "                            })\n",
    "        # Create summary\n",
    "        if results['malformed_by_row']:\n",
    "            pattern_counts = Counter(item['Malformed_Tag'] for item in results['malformed_by_row'])\n",
    "            \n",
    "            for pattern, count in pattern_counts.items():\n",
    "                first_occurrence = next(item for item in results['malformed_by_row'] \n",
    "                                       if item['Malformed_Tag'] == pattern)\n",
    "                \n",
    "                results['malformed_summary'].append({\n",
    "                    'Malformed_Tag': pattern,\n",
    "                    'Count': count,\n",
    "                    'Suggested_Repair': first_occurrence['Suggested_Repair'],\n",
    "                    'Sample_Context': first_occurrence['Context']\n",
    "                })\n",
    "        \n",
    "        malformed_by_row_df = pd.DataFrame(results['malformed_by_row'])\n",
    "        malformed_summary_df = pd.DataFrame(results['malformed_summary'])\n",
    "        \n",
    "        if not malformed_summary_df.empty:\n",
    "            malformed_summary_df = malformed_summary_df.sort_values('Count', ascending=False)\n",
    "        \n",
    "        return {\n",
    "            'Malformed_Tags_by_Row': malformed_by_row_df,\n",
    "            'Malformed_Tags_Summary': malformed_summary_df\n",
    "        }\n",
    "    \n",
    "    def repair_tags(self, df: pd.DataFrame, malformed_analysis: Dict) -> pd.DataFrame:\n",
    "        \"\"\"Apply malformed tag repairs with better handling\"\"\"\n",
    "        df_repaired = df.copy()\n",
    "        \n",
    "        if malformed_analysis['Malformed_Tags_Summary'].empty:\n",
    "            return df_repaired\n",
    "        \n",
    "        for _, row in malformed_analysis['Malformed_Tags_Summary'].iterrows():\n",
    "            pattern = row['Malformed_Tag']\n",
    "            repair = row['Suggested_Repair']\n",
    "            \n",
    "            # Handle different types of issues\n",
    "            if \"Mismatch:\" in str(repair):\n",
    "                # For mismatches, we can't auto-fix but we can log them\n",
    "                print(f\"  WARNING: {pattern} has {repair} - manual review needed\")\n",
    "                continue\n",
    "            \n",
    "            # Apply literal pattern replacements\n",
    "            if pattern in self.html_malformation_patterns:\n",
    "                for col in df_repaired.columns:\n",
    "                    if df_repaired[col].dtype == 'object':\n",
    "                        df_repaired[col] = df_repaired[col].astype(str).str.replace(\n",
    "                            pattern, repair, regex=False\n",
    "                        )\n",
    "                        \n",
    "        return df_repaired\n",
    "\n",
    "    def generate_mismatch_report(self, malformed_analysis: Dict) -> pd.DataFrame:\n",
    "            \"\"\"Generate a report of mismatches for manual review\"\"\"\n",
    "            mismatch_rows = []\n",
    "            \n",
    "            for _, row in malformed_analysis['Malformed_Tags_Summary'].iterrows():\n",
    "                if \"Mismatch:\" in str(row['Suggested_Repair']):\n",
    "                    mismatch_rows.append({\n",
    "                        'Tag': row['Malformed_Tag'],\n",
    "                        'Issue': row['Suggested_Repair'],\n",
    "                        'Occurrences': row['Count'],\n",
    "                        'Sample_Context': row['Sample_Context'],\n",
    "                        'Action_Required': 'Manual review needed'\n",
    "                    })\n",
    "            \n",
    "            return pd.DataFrame(mismatch_rows)\n",
    "    \n",
    "    def get_repair_statistics(self, malformed_analysis: Dict) -> Dict:\n",
    "        \"\"\"Get statistics about the types of issues found\"\"\"\n",
    "        stats = {\n",
    "            'total_issues': len(malformed_analysis['Malformed_Tags_by_Row']),\n",
    "            'unique_patterns': len(malformed_analysis['Malformed_Tags_Summary']),\n",
    "            'auto_fixable': 0,\n",
    "            'manual_review_needed': 0,\n",
    "            'literal_malformations': 0,\n",
    "            'tag_mismatches': 0\n",
    "        }\n",
    "        \n",
    "        for _, row in malformed_analysis['Malformed_Tags_Summary'].iterrows():\n",
    "            if \"Mismatch:\" in str(row['Suggested_Repair']):\n",
    "                stats['manual_review_needed'] += row['Count']\n",
    "                stats['tag_mismatches'] += 1\n",
    "            else:\n",
    "                stats['auto_fixable'] += row['Count']\n",
    "                stats['literal_malformations'] += 1\n",
    "        \n",
    "        return stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f577a9",
   "metadata": {},
   "source": [
    "### Step 5.2 NonHTMLTagProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "6bf446f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonHTMLTagProcessor:\n",
    "    \"\"\"Handles encoding of non-HTML angle brackets - OPTIMIZED VERSION\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.html_analyzer = HTMLTagHandler()\n",
    "        \n",
    "        # Known HTML tags that should never be encoded\n",
    "        self.valid_html_tags = {\n",
    "            'p', 'br', 'div', 'span', 'a', 'b', 'i', 'u', 'strong', 'em',\n",
    "            'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'li', 'table',\n",
    "            'tr', 'td', 'th', 'img', 'bibl', 'title', 'sup', 'sub', 'del'\n",
    "        }\n",
    "    \n",
    "    def find_non_html_tags(self, text: str) -> List[str]:\n",
    "        \"\"\"Find tags that are definitely not HTML\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return []\n",
    "        \n",
    "        # Find all < ... > patterns\n",
    "        pattern = r'<[^<>]*>'\n",
    "        all_brackets = re.findall(pattern, str(text))\n",
    "        \n",
    "        non_html_tags = []\n",
    "        for tag in all_brackets:\n",
    "            # Check if it's valid HTML\n",
    "            tag_type, _ = self.html_analyzer.classify_tag(tag)\n",
    "            if tag_type == 'non_html':\n",
    "                non_html_tags.append(tag)\n",
    "        \n",
    "        return non_html_tags\n",
    "    \n",
    "    def analyze_non_html_tags(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"OPTIMIZED: Analyze non-HTML tags across DataFrame efficiently\"\"\"\n",
    "        results = {\n",
    "            'non_html_by_row': [],\n",
    "            'non_html_summary': []\n",
    "        }\n",
    "        \n",
    "        # Collect all unique non-HTML tags first\n",
    "        unique_non_html_tags = set()\n",
    "        tag_locations = defaultdict(list)\n",
    "        \n",
    "        # Process in chunks for better memory efficiency\n",
    "        for idx, row in df.iterrows():\n",
    "            # Process multiple columns at once\n",
    "            for col in df.columns:\n",
    "                cell_value = row[col]\n",
    "                if pd.notna(cell_value) and cell_value != '':\n",
    "                    non_html_tags = self.find_non_html_tags(cell_value)\n",
    "                    for tag in non_html_tags:\n",
    "                        unique_non_html_tags.add(tag)\n",
    "                        tag_locations[tag].append({\n",
    "                            'Row': idx,\n",
    "                            'Column': col,\n",
    "                            'Context': str(cell_value)[:100]\n",
    "                        })\n",
    "        \n",
    "        # Build results from collected data\n",
    "        for tag, locations in tag_locations.items():\n",
    "            for loc in locations:\n",
    "                results['non_html_by_row'].append({\n",
    "                    'Row': loc['Row'],\n",
    "                    'Column': loc['Column'],\n",
    "                    'Non_HTML_Tag': tag,\n",
    "                    'Context': loc['Context']\n",
    "                })\n",
    "        \n",
    "        # Create summary\n",
    "        if unique_non_html_tags:\n",
    "            summary_data = []\n",
    "            for tag in unique_non_html_tags:\n",
    "                summary_data.append({\n",
    "                    'Tag': tag,\n",
    "                    'Count': len(tag_locations[tag]),\n",
    "                    'Encoded_Version': tag.replace('<', '&lt;').replace('>', '&gt;')\n",
    "                })\n",
    "            results['non_html_summary'] = summary_data\n",
    "        \n",
    "        non_html_by_row_df = pd.DataFrame(results['non_html_by_row'])\n",
    "        non_html_summary_df = pd.DataFrame(results['non_html_summary'])\n",
    "        \n",
    "        return {\n",
    "            'Non_HTML_Tags_by_Row': non_html_by_row_df,\n",
    "            'Non_HTML_Tags_Summary': non_html_summary_df\n",
    "        }\n",
    "    \n",
    "    def encode_brackets(self, df: pd.DataFrame, non_html_analysis: Dict) -> pd.DataFrame:\n",
    "        \"\"\"OPTIMIZED: Apply non-HTML bracket encoding using vectorized operations\"\"\"\n",
    "        df_encoded = df.copy()\n",
    "        \n",
    "        # Get unique non-HTML tags from analysis\n",
    "        if not non_html_analysis['Non_HTML_Tags_Summary'].empty:\n",
    "            # Process all replacements at once using vectorized operations\n",
    "            for _, row in non_html_analysis['Non_HTML_Tags_Summary'].iterrows():\n",
    "                tag = row['Tag']\n",
    "                encoded_tag = row['Encoded_Version']\n",
    "                \n",
    "                # Apply to all string columns at once\n",
    "                for col in df_encoded.columns:\n",
    "                    if df_encoded[col].dtype == 'object':  # Only process string columns\n",
    "                        # Use vectorized string replacement\n",
    "                        mask = df_encoded[col].astype(str).str.contains(\n",
    "                            re.escape(tag), regex=True, na=False\n",
    "                        )\n",
    "                        if mask.any():\n",
    "                            df_encoded.loc[mask, col] = df_encoded.loc[mask, col].astype(str).str.replace(\n",
    "                                tag, encoded_tag, regex=False\n",
    "                            )\n",
    "        \n",
    "        return df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ac2e75",
   "metadata": {},
   "source": [
    "### Step 5.3 Link Validator `<a>` tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "a8e5fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkValidator:\n",
    "    def __init__(self):\n",
    "        # regex for raw <http://...> or <https://...>\n",
    "        self.raw_link_pattern = re.compile(r'<(https?://[^>\\s]+)>')\n",
    "        # regex for opening <a> tags\n",
    "        self.anchor_open_pattern = re.compile(r'<a([^>]*)>', flags=re.IGNORECASE)\n",
    "        # regex for closing </a>\n",
    "        self.anchor_close_pattern = re.compile(r'</a>', flags=re.IGNORECASE)\n",
    "\n",
    "    def validate_links(self, text: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Validates links inside a text string.\n",
    "        - Finds raw URLs in <> that should be <a href=\"\">\n",
    "        - Ensures <a> tags have href\n",
    "        - Ensures <a> tags are properly closed\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return []\n",
    "\n",
    "        issues = []\n",
    "        text_str = str(text)\n",
    "\n",
    "        # 1. Detect raw <http://...>\n",
    "        for url in self.raw_link_pattern.findall(text_str):\n",
    "            issues.append({\n",
    "                \"Issue\": \"Raw URL in angle brackets\",\n",
    "                \"URL\": url,\n",
    "                \"Suggested_Fix\": f'<a href=\"{url}\">{url}</a>'\n",
    "            })\n",
    "\n",
    "        # 2. Detect <a> tags without href\n",
    "        for attrs in self.anchor_open_pattern.findall(text_str):\n",
    "            if \"href=\" not in attrs.lower():\n",
    "                issues.append({\n",
    "                    \"Issue\": \"Anchor tag missing href\",\n",
    "                    \"Tag\": f\"<a{attrs}>\",\n",
    "                    \"Suggested_Fix\": '<a href=\"URL_HERE\">'\n",
    "                })\n",
    "\n",
    "        # 3. Check open/close balance\n",
    "        open_count = len(self.anchor_open_pattern.findall(text_str))\n",
    "        close_count = len(self.anchor_close_pattern.findall(text_str))\n",
    "        if open_count != close_count:\n",
    "            issues.append({\n",
    "                \"Issue\": \"Anchor tag mismatch\",\n",
    "                \"Opens\": open_count,\n",
    "                \"Closes\": close_count,\n",
    "                \"Suggested_Fix\": \"Ensure each <a> has matching </a>\"\n",
    "            })\n",
    "\n",
    "        return issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e678a5",
   "metadata": {},
   "source": [
    "## Step 6 HTML Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "69e35f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLProcessor:\n",
    "    \"\"\"Main processor that coordinates all operations - OPTIMIZED\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Use YOUR existing comprehensive classes\n",
    "        self.tag_handler = HTMLTagHandler()\n",
    "        self.malformed_repairer = MalformedTagRepairer()  # Your comprehensive repairer\n",
    "        self.non_html_processor = NonHTMLTagProcessor()  # Your comprehensive processor\n",
    "        self.link_validator = LinkValidator()\n",
    "        self.stats = {}\n",
    "        self.content_columns = [\n",
    "            'Principal English Translation',\n",
    "            'Attestations from sources in English',\n",
    "            'Attestations from sources in Spanish',\n",
    "            'Alonso de Molina',\n",
    "            'Frances Karttunen', \n",
    "            'Horacio Carochi / English',\n",
    "            'Andrés de Olmos',\n",
    "            \"Lockhart’s Nahuatl as Written\",\n",
    "            \n",
    "        ]\n",
    "    \n",
    "    def process_dataframe(\n",
    "        self, df: pd.DataFrame, \n",
    "        repair_malformed: bool = True,\n",
    "        validate_links: bool = True,\n",
    "        extract_citations: bool = True,\n",
    "        extract_crossrefs: bool = True,\n",
    "        encode_non_html: bool = True\n",
    "    ) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \n",
    "        \"\"\"\n",
    "        Process entire dataframe efficiently, including:\n",
    "        - Malformed HTML repair\n",
    "        - Non-HTML tag encoding\n",
    "        - <a> link validation\n",
    "        Returns: (processed_df, analysis_report)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"HTML PROCESSING PIPELINE\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        result_df = df.copy()\n",
    "        \n",
    "        # Add extraction columns if needed\n",
    "        if extract_citations:\n",
    "            result_df['Citations'] = ''\n",
    "            result_df['Number_of_Citations'] = 0\n",
    "        if extract_crossrefs:\n",
    "            result_df['Cross_References'] = ''\n",
    "            result_df['Number_of_Cross_References'] = 0\n",
    "        \n",
    "        analysis = {\n",
    "            'repairs_made': 0,\n",
    "            'citations_extracted': 0,\n",
    "            'crossrefs_extracted': 0,\n",
    "            'non_html_encoded': 0,\n",
    "            'tag_analysis': [],\n",
    "            'processing_details': []\n",
    "        }\n",
    "        report = {}\n",
    "        reports = []  # collect all issues for unified mismatch report\n",
    "        total_rows = len(result_df)\n",
    "        print(f\"\\nProcessing {total_rows:,} rows...\")\n",
    "\n",
    "        # -------------------------------\n",
    "        # STEP 1: Malformed HTML tags\n",
    "        # -------------------------------\n",
    "        if repair_malformed:\n",
    "            print(\"  Analyzing malformed tags...\")\n",
    "            malformed_analysis = self.malformed_repairer.analyze_malformed_tags(result_df[self.content_columns])\n",
    "\n",
    "            if not malformed_analysis['Malformed_Tags_Summary'].empty:\n",
    "                stats = self.malformed_repairer.get_repair_statistics(malformed_analysis)\n",
    "                print(f\"  Found {stats['total_issues']} malformed tag instances\")\n",
    "                print(f\"  Found {stats['unique_patterns']} unique malformed patterns\")\n",
    "                print(f\"    - Auto-fixable: {stats['auto_fixable']}\")\n",
    "                print(f\"    - Manual review needed: {stats['manual_review_needed']}\")\n",
    "\n",
    "                mismatch_report = self.malformed_repairer.generate_mismatch_report(malformed_analysis)\n",
    "                if not mismatch_report.empty:\n",
    "                    mismatch_report = mismatch_report.copy()\n",
    "                    mismatch_report[\"Issue_Type\"] = \"Malformed Tag\"\n",
    "                    reports.append(mismatch_report)\n",
    "\n",
    "                print(\"  Repairing malformed tags...\")\n",
    "                result_df[self.content_columns] = self.malformed_repairer.repair_tags(\n",
    "                    result_df[self.content_columns],\n",
    "                    malformed_analysis\n",
    "                )\n",
    "\n",
    "                analysis['repairs_made'] = stats['auto_fixable']\n",
    "                analysis['repair_stats'] = stats\n",
    "                # optional raw malformed data\n",
    "                analysis['malformed_repairs'] = malformed_analysis\n",
    "            else:\n",
    "                print(\"  No malformed tags found\")\n",
    "\n",
    "        # -------------------------------\n",
    "        # STEP 2: Non-HTML tags\n",
    "        # -------------------------------\n",
    "        if encode_non_html:\n",
    "            print(\"  Analyzing non-HTML tags...\")\n",
    "            non_html_analysis = self.non_html_processor.analyze_non_html_tags(result_df[self.content_columns])\n",
    "\n",
    "            if not non_html_analysis['Non_HTML_Tags_Summary'].empty:\n",
    "                print(f\"  Found {len(non_html_analysis['Non_HTML_Tags_by_Row'])} non-HTML tag instances\")\n",
    "                print(f\"  Found {len(non_html_analysis['Non_HTML_Tags_Summary'])} unique non-HTML tags\")\n",
    "\n",
    "                # Encode brackets in the dataframe\n",
    "                result_df[self.content_columns] = self.non_html_processor.encode_brackets(\n",
    "                    result_df[self.content_columns],\n",
    "                    non_html_analysis\n",
    "                )\n",
    "\n",
    "                analysis['non_html_encoded'] = len(non_html_analysis['Non_HTML_Tags_by_Row'])\n",
    "                analysis['non_html_encoding'] = non_html_analysis\n",
    "\n",
    "                # Prepare report entries\n",
    "                non_html_rows = []\n",
    "                non_html_df = non_html_analysis['Non_HTML_Tags_by_Row']\n",
    "                for _, row in non_html_df.iterrows():\n",
    "                    non_html_rows.append({\n",
    "                        \"Row\": row['Row'],\n",
    "                        \"Column\": row['Column'],\n",
    "                        \"Issue\": row['Non_HTML_Tag'],\n",
    "                        \"Issue_Type\": \"Non-HTML Tag\"\n",
    "                    })\n",
    "                if non_html_rows:\n",
    "                    reports.append(pd.DataFrame(non_html_rows))\n",
    "            else:\n",
    "                print(\"  No non-HTML tags found\")\n",
    "\n",
    "        # -------------------------------\n",
    "        # STEP 3: Link validation\n",
    "        # -------------------------------\n",
    "        if validate_links:\n",
    "            print(\"  Validating links and <a> tags...\")\n",
    "            link_issues = []\n",
    "\n",
    "            for col in self.content_columns:\n",
    "                if col not in result_df.columns:\n",
    "                    continue\n",
    "\n",
    "                for idx, value in result_df[col].items():\n",
    "                    issues = self.link_validator.validate_links(value)\n",
    "                    for issue in issues:\n",
    "                        issue[\"Row\"] = idx\n",
    "                        issue[\"Column\"] = col\n",
    "                        issue[\"Issue_Type\"] = \"Link Issue\"\n",
    "                        link_issues.append(issue)\n",
    "\n",
    "            if link_issues:\n",
    "                reports.append(pd.DataFrame(link_issues))\n",
    "                print(f\"  Found {len(link_issues)} link issues across {len(self.content_columns)} columns\")\n",
    "            else:\n",
    "                print(\"  No link issues found\")\n",
    "\n",
    "        # -------------------------------\n",
    "        # STEP 4: Unified mismatch report\n",
    "        # -------------------------------\n",
    "        if reports:\n",
    "            unified_report = pd.concat(reports, ignore_index=True)\n",
    "        else:\n",
    "            unified_report = pd.DataFrame(columns=[\"Row\", \"Column\", \"Issue\", \"Issue_Type\"])\n",
    "\n",
    "        report['analysis'] = analysis\n",
    "        report[\"mismatch_report\"] = unified_report\n",
    "\n",
    "        self._print_summary(analysis)\n",
    "        return result_df, report\n",
    "\n",
    "    def _print_summary(self, analysis: Dict):\n",
    "        \"\"\"Print processing summary\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"PROCESSING COMPLETE\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Repairs made:        {analysis['repairs_made']:,}\")\n",
    "        print(f\"Citations extracted: {analysis['citations_extracted']:,}\")\n",
    "        print(f\"Cross-refs extracted: {analysis['crossrefs_extracted']:,}\")\n",
    "        print(f\"Non-HTML encoded:    {analysis['non_html_encoded']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dacf8c",
   "metadata": {},
   "source": [
    "## Main Processor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "aabd3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NahuatlProcessor:\n",
    "    \"\"\"Main processor class that coordinates all operations\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = DB_PATH, working_dir: str = WORKING_DIR):\n",
    "        \"\"\"Initialize the processor with all necessary components\"\"\"\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(\"NAHUATL DATA PROCESSOR - HYBRID APPROACH\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.html_processor = HTMLProcessor()\n",
    "        self.loader = DataLoader()\n",
    "        self.saver = DataSaver(working_dir)\n",
    "        self.db = DatabaseManager(db_path)\n",
    "        \n",
    "        # Data containers\n",
    "        self.original_df = None\n",
    "        self.working_df = None\n",
    "        \n",
    "        # Processing state\n",
    "        self.current_stage = 'initialized'\n",
    "        self.processing_history = []\n",
    "        \n",
    "        # Verify base dataset exists\n",
    "        if not self.db.verify_base_dataset():\n",
    "            print(\"\\n No base dataset found in SQLite\")\n",
    "        print(\"   Run processor.initial_import() to import your CSV data first\")\n",
    "        print(\"Processor initialized and ready\")\n",
    "        print(\"=\" * 70)\n",
    "    \n",
    "    def initial_import(self, csv_path: str, table_name: str = 'WHP_EarlyNahuatl_Data', \n",
    "                      encoding: str = DEFAULT_ENCODING, replace: bool = False):\n",
    "        \"\"\"\n",
    "        Perform initial import of CSV data into SQLite.\n",
    "        This establishes the base dataset in the database.\n",
    "        \"\"\"\n",
    "        \n",
    "        success = self.db.import_initial_dataset(\n",
    "            csv_path=csv_path,\n",
    "            table_name=table_name,\n",
    "            encoding=encoding,\n",
    "            replace=replace\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            print(\"\\nInitial import complete!\")\n",
    "            print(\"   You can now proceed with data processing\")\n",
    "        else:\n",
    "            print(\"\\nInitial import failed\")\n",
    "            \n",
    "        return success\n",
    "    \n",
    "    \n",
    "    def load_data(self, source: str, source_type: str = 'auto', **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"Load data from any source\"\"\"\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Auto-detect source type if needed\n",
    "        if source_type == 'auto':\n",
    "            source_type = 'csv' if source.endswith('.csv') else 'sqlite'\n",
    "        \n",
    "        # Load based on source type\n",
    "        if source_type == 'csv':\n",
    "            df = self.loader.load_from_csv(source, **kwargs)\n",
    "        elif source_type == 'sqlite':\n",
    "            table_name = kwargs.get('table_name', 'WHP_EarlyNahuatl_Data')\n",
    "            df = self.loader.load_from_sqlite(source, table_name)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown source type: {source_type}\")\n",
    "        \n",
    "        # Create working copies\n",
    "        self.original_df, self.working_df = self.loader.create_working_copy(df)\n",
    "        \n",
    "        # Save initial checkpoint\n",
    "        self.save_checkpoint('initial')\n",
    "        \n",
    "        # Log the operation\n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        self.db.log_processing_stage(\n",
    "            stage='data_loading',\n",
    "            status='completed',\n",
    "            duration=duration,\n",
    "            rows_processed=len(df),\n",
    "            notes=f\"Loaded from {source_type}: {source}\"\n",
    "        )\n",
    "        \n",
    "        self.current_stage = 'data_loaded'\n",
    "        \n",
    "        return self.working_df\n",
    "    \n",
    "    def save_checkpoint(self, checkpoint_name: str, additional_data: Dict = None):\n",
    "        \"\"\"Enhanced checkpoint saving with additional reports\"\"\"\n",
    "        if self.working_df is not None and self.db.conn is not None:\n",
    "            # Save main checkpoint\n",
    "            self.saver.save_checkpoint_to_sqlite(\n",
    "                self.working_df,\n",
    "                checkpoint_name,\n",
    "                self.db.conn\n",
    "            )\n",
    "            \n",
    "            # Save additional data if provided (like mismatch reports)\n",
    "            if additional_data:\n",
    "                for data_name, data_df in additional_data.items():\n",
    "                    if isinstance(data_df, pd.DataFrame) and not data_df.empty:\n",
    "                        table_name = f'{data_name}_{checkpoint_name}_{datetime.now().strftime(\"%Y%m%d\")}'\n",
    "                        data_df.to_sql(\n",
    "                            table_name,\n",
    "                            self.db.conn,\n",
    "                            if_exists='replace',\n",
    "                            index=False\n",
    "                        )\n",
    "                        print(f\"  Saved {data_name} to {table_name}\")\n",
    "            \n",
    "            self.processing_history.append({\n",
    "                'checkpoint': checkpoint_name,\n",
    "                'timestamp': datetime.now(),\n",
    "                'rows': len(self.working_df),\n",
    "                'columns': len(self.working_df.columns)\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: Cannot save checkpoint '{checkpoint_name}' - no data or connection available\")\n",
    "    \n",
    "    def export_final_results(self, filename_base: str = 'final_nahuatl_data'):\n",
    "        \"\"\"Export final results to CSV with encoding protection\"\"\"\n",
    "        \n",
    "        if self.working_df is None:\n",
    "            print(\"Error: No working data to export\")\n",
    "            return\n",
    "        \n",
    "        if self.original_df is None:\n",
    "            print(\"Warning: No original data for comparison\")\n",
    "            original_rows = 0\n",
    "            original_cols = 0\n",
    "        else:\n",
    "            original_rows = len(self.original_df)\n",
    "            original_cols = len(self.original_df.columns)\n",
    "        \n",
    "        # Save to CSV\n",
    "        csv_path = self.saver.save_to_csv(self.working_df, f\"{filename_base}.csv\")\n",
    "        \n",
    "        # Save final checkpoint\n",
    "        self.save_checkpoint('final')\n",
    "        \n",
    "        # Create summary report\n",
    "        summary = {\n",
    "            'processing_summary': pd.DataFrame(self.processing_history),\n",
    "            'data_info': pd.DataFrame([{\n",
    "                'original_rows': original_rows,\n",
    "                'final_rows': len(self.working_df),\n",
    "                'original_columns': original_cols,\n",
    "                'final_columns': len(self.working_df.columns),\n",
    "                'stages_completed': len(self.processing_history)\n",
    "            }])\n",
    "        }\n",
    "        \n",
    "        # Save summary to Excel\n",
    "        excel_path = self.saver.save_to_excel(\n",
    "            summary,\n",
    "            f\"{filename_base}_summary_{datetime.now().strftime('%Y%m%d')}.xlsx\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"EXPORT COMPLETE\")\n",
    "        print(f\"CSV: {csv_path}\")\n",
    "        print(f\"Summary: {excel_path}\")\n",
    "        print(\"=\" * 70)\n",
    "    \n",
    "    def load_data_from_base(self, table_name: str = 'WHP_EarlyNahuatl_Data') -> pd.DataFrame:\n",
    "        \"\"\"Convenience method to load from the base SQLite dataset\"\"\"\n",
    "        \n",
    "        if not self.db.verify_base_dataset(table_name):\n",
    "            raise ValueError(f\"Base dataset '{table_name}' not found. Run initial_import() first.\")\n",
    "        \n",
    "        return self.load_data(\n",
    "            source=self.db.db_path,\n",
    "            source_type='sqlite',\n",
    "            table_name=table_name\n",
    "        )\n",
    "        \n",
    "    def process_html_tags(self, save_checkpoint: bool = True):\n",
    "        \"\"\"Step 4-5: HTML tag processing\"\"\"\n",
    "        if self.working_df is None:\n",
    "            raise ValueError(\"No data loaded. Run load_data() first.\")\n",
    "        \n",
    "        # Delegate to HTML processor\n",
    "        self.working_df, report = self.html_processor.process_dataframe(\n",
    "            self.working_df,\n",
    "            repair_malformed=True,\n",
    "            encode_non_html=True,\n",
    "            validate_links=True,\n",
    "        )\n",
    "        \n",
    "        if save_checkpoint:\n",
    "            additional_data = {}\n",
    "            \n",
    "            # The HTMLProcessor already creates a unified mismatch_report\n",
    "            # that includes ALL issues (malformed tags, non-HTML tags, link issues)\n",
    "            # Each row already has an 'Issue_Type' column\n",
    "            if 'mismatch_report' in report and not report['mismatch_report'].empty:\n",
    "                additional_data['mismatch_report'] = report['mismatch_report']\n",
    "                \n",
    "                # Print summary by issue type\n",
    "                print(\"\\nIssue Summary:\")\n",
    "                issue_counts = report['mismatch_report']['Issue_Type'].value_counts()\n",
    "                for issue_type, count in issue_counts.items():\n",
    "                    print(f\"  - {issue_type}: {count} issues\")\n",
    "                    \n",
    "                print(f\"\\nTotal issues found: {len(report['mismatch_report'])}\")\n",
    "            else:\n",
    "                print(\"\\nNo issues found - data is clean!\")\n",
    "            \n",
    "            # Save checkpoint with the unified report\n",
    "            self.save_checkpoint('after_html_processing', additional_data)\n",
    "            \n",
    "        return report    \n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        self.db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "59ae4e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NAHUATL DATA PROCESSOR - HYBRID APPROACH\n",
      "======================================================================\n",
      "Database initialized: ../../data/sqLiteDb/nahuatl_processing.db\n",
      "Existing database objects: 8\n",
      "   - checkpoint_metadata (table): 23 rows\n",
      "   - sqlite_sequence (table): 2 rows\n",
      "   - processing_log (table): 13 rows\n",
      "   - WHP_EarlyNahuatl_Data (table): 31,806 rows\n",
      "   - import_history (table): 1 rows\n",
      "   - checkpoint_initial_20250913 (table): 31,806 rows\n",
      "   - checkpoint_after_html_processing_20250913 (table): 31,806 rows\n",
      "   - mismatch_report_after_html_processing_20250913 (table): 136 rows\n",
      "   Base dataset verified:\n",
      "   Table: WHP_EarlyNahuatl_Data\n",
      "   Rows: 31,806\n",
      "   Columns: 13\n",
      "   Run processor.initial_import() to import your CSV data first\n",
      "Processor initialized and ready\n",
      "======================================================================\n",
      "   Base dataset verified:\n",
      "   Table: WHP_EarlyNahuatl_Data\n",
      "   Rows: 31,806\n",
      "   Columns: 13\n",
      "Loading data from SQLite: ../../data/sqLiteDb/nahuatl_processing.db\n",
      "Table: WHP_EarlyNahuatl_Data\n",
      "Successfully loaded 31,806 rows × 13 columns\n",
      "Created working copy of data\n",
      "Checkpoint saved to SQLite: checkpoint_initial_20250913\n",
      "======================================================================\n",
      "HTML PROCESSING PIPELINE\n",
      "======================================================================\n",
      "\n",
      "Processing 31,806 rows...\n",
      "  Analyzing malformed tags...\n",
      "  Found 180 malformed tag instances\n",
      "  Found 11 unique malformed patterns\n",
      "    - Auto-fixable: 13\n",
      "    - Manual review needed: 167\n",
      "  Repairing malformed tags...\n",
      "  WARNING: <bibl> has Mismatch: 1 open, 0 closed - manual review needed\n",
      "  WARNING: <em> has Mismatch: 5 open, 3 closed - manual review needed\n",
      "  WARNING: <sup> has Mismatch: 21 open, 23 closed - manual review needed\n",
      "  WARNING: <strong> has Mismatch: 5 open, 4 closed - manual review needed\n",
      "  WARNING: <p> has Mismatch: 1 open, 2 closed - manual review needed\n",
      "  Analyzing non-HTML tags...\n",
      "  Found 67 non-HTML tag instances\n",
      "  Found 52 unique non-HTML tags\n",
      "  Validating links and <a> tags...\n",
      "  Found 64 link issues across 8 columns\n",
      "\n",
      "======================================================================\n",
      "PROCESSING COMPLETE\n",
      "======================================================================\n",
      "Repairs made:        13\n",
      "Citations extracted: 0\n",
      "Cross-refs extracted: 0\n",
      "Non-HTML encoded:    67\n",
      "\n",
      "Issue Summary:\n",
      "  - Non-HTML Tag: 67 issues\n",
      "  - Link Issue: 64 issues\n",
      "  - Malformed Tag: 5 issues\n",
      "\n",
      "Total issues found: 136\n",
      "Checkpoint saved to SQLite: checkpoint_after_html_processing_20250913\n",
      "  Saved mismatch_report to mismatch_report_after_html_processing_20250913\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize processor\n",
    "    processor = NahuatlProcessor()\n",
    "    \n",
    "    # processor.initial_import(csv_path='../../data/raw/WHP_EarlyNahuatl_data_2024-03-26T17-22-58.csv')\n",
    "    \n",
    "    processor.load_data_from_base()\n",
    "    processor.process_html_tags()\n",
    "    # Example 1: Load from existing SQLite\n",
    "    # processor.load_data(\n",
    "    #     source='../../data/sqLiteDb/Whp_Raw_Dataset.db',\n",
    "    #     source_type='sqlite',\n",
    "    #     table_name='WHP_EarlyNahuatl_Data'\n",
    "    # )\n",
    "    \n",
    "    # Example 2: Load from CSV\n",
    "    # processor.load_data(\n",
    "    #     source='data/nahuatl_data.csv',\n",
    "    #     source_type='csv',\n",
    "    #     auto_detect_encoding=True\n",
    "    # )\n",
    "    \n",
    "    # Your processing steps would go here...\n",
    "    # processor.working_df = repair_malformed_characters(processor.working_df)\n",
    "    # processor.save_checkpoint('after_repair')\n",
    "    \n",
    "    # processor.working_df = extract_citations(processor.working_df)\n",
    "    # processor.save_checkpoint('after_citations')\n",
    "    \n",
    "    # Export final results\n",
    "    # processor.export_final_results()\n",
    "    \n",
    "    # Clean up\n",
    "    # processor.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3519d6",
   "metadata": {},
   "source": [
    "At this point we've done enough thorough analysis and for the sake of correctness, manual correction will be done on the checkpoint table in the above log. After the corrections are made we will begin extracting citations and cross references."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nahuaLEX_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
