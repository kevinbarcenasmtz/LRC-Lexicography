{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df95143d",
   "metadata": {},
   "source": [
    "# Nahuatl Notebook for the WHP_EarlyNahuatl_Dataset\n",
    "\n",
    "This notebook processes Nahuatl dictionary data, analyzing HTML tags, repairing malformed tags, and extracting citations and cross-references. This is a merged version of Todd's version and I where there is a SQLite-based data management approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8105d6",
   "metadata": {},
   "source": [
    "## Step 1: Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f542ca7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import sqlite3\n",
    "import hashlib\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "from typing import List, Dict, Tuple, Optional, Union, Any\n",
    "\n",
    "# Data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Character encoding detection\n",
    "import chardet\n",
    "from unidecode import unidecode\n",
    "\n",
    "# HTML/XML processing\n",
    "from bs4 import BeautifulSoup\n",
    "from inscriptis import get_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60814645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration constants\n",
    "WORKING_DIR = 'working_files'\n",
    "DB_PATH = '../../data/sqLiteDb/nahuatl_processing.db'  # Single database for all operations\n",
    "DEFAULT_ENCODING = 'utf-8-sig'  # This encoding is the encoding that works when outputing to CSV files to encode certain Nahuatl characters properly.\n",
    "CHECKPOINT_STAGES = ['initial', 'cleaned', 'final']  # Only save these to SQLite"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f1d64e",
   "metadata": {},
   "source": [
    "## Step 2: Data Import and Working Copy Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c34f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"Unified data loader for CSV and SQLite sources\"\"\"\n",
    "    \n",
    "    def __init__(self, encoding: str = DEFAULT_ENCODING):\n",
    "        self.encoding = encoding\n",
    "        \n",
    "    def detect_encoding(self, filepath: str) -> str:\n",
    "        \"\"\"Auto-detect file encoding if needed\"\"\"\n",
    "        with open(filepath, 'rb') as file:\n",
    "            raw_data = file.read(10000)  # Read first 10KB\n",
    "            result = chardet.detect(raw_data)\n",
    "            return result['encoding'] or 'utf-8'\n",
    "    \n",
    "    def load_from_csv(self, filepath: str, auto_detect_encoding: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"Load data from CSV with encoding protection\"\"\"\n",
    "        \n",
    "        # Detect encoding if requested\n",
    "        encoding = self.detect_encoding(filepath) if auto_detect_encoding else self.encoding\n",
    "        \n",
    "        print(f\"Loading data from CSV: {filepath}\")\n",
    "        print(f\"Encoding: {encoding}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                filepath,\n",
    "                encoding=encoding,\n",
    "                na_values=[''],\n",
    "                keep_default_na=True,\n",
    "                dtype=str,  # Keep everything as strings initially\n",
    "                low_memory=False\n",
    "            )\n",
    "            \n",
    "            print(f\"Successfully loaded {len(df):,} rows × {len(df.columns)} columns\")\n",
    "            print(f\"Columns: {list(df.columns)[:5]}...\" if len(df.columns) > 5 else f\"   Columns: {list(df.columns)}\")\n",
    "            \n",
    "            return df\n",
    "            \n",
    "        except UnicodeDecodeError as e:\n",
    "            print(f\"Encoding error: {e}\")\n",
    "            print(\"Attempting with auto-detected encoding...\")\n",
    "            return self.load_from_csv(filepath, auto_detect_encoding=True)\n",
    "    \n",
    "    def load_from_sqlite(self, db_path: str, table_name: str) -> pd.DataFrame:\n",
    "        \"\"\"Load data from SQLite database\"\"\"\n",
    "        \n",
    "        print(f\"Loading data from SQLite: {db_path}\")\n",
    "        print(f\"Table: {table_name}\")\n",
    "        \n",
    "        with sqlite3.connect(db_path) as conn:\n",
    "            # Check if table exists\n",
    "            tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table'\", conn)\n",
    "            if table_name not in tables['name'].values:\n",
    "                raise ValueError(f\"Table '{table_name}' not found in database\")\n",
    "            \n",
    "            df = pd.read_sql(f\"SELECT * FROM [{table_name}]\", conn)\n",
    "            \n",
    "        print(f\"Successfully loaded {len(df):,} rows × {len(df.columns)} columns\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def create_working_copy(self, df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"Create a working copy while preserving the original\"\"\"\n",
    "        original_df = df.copy(deep=True)\n",
    "        working_df = df.copy(deep=True)\n",
    "        \n",
    "        print(f\"Created working copy of data\")\n",
    "        \n",
    "        return original_df, working_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f10329",
   "metadata": {},
   "source": [
    "## Step 3: Intermediate Save Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa38701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSaver:\n",
    "    \"\"\"Handles saving data at various stages with encoding protection\"\"\"\n",
    "    \n",
    "    def __init__(self, working_dir: str = WORKING_DIR, encoding: str = DEFAULT_ENCODING):\n",
    "        self.working_dir = working_dir\n",
    "        self.encoding = encoding\n",
    "        os.makedirs(working_dir, exist_ok=True)\n",
    "        \n",
    "    def save_to_csv(self, df: pd.DataFrame, filename: str, add_timestamp: bool = True) -> str:\n",
    "        \"\"\"Save DataFrame to CSV with encoding protection\"\"\"\n",
    "        \n",
    "        df_copy = df.copy()\n",
    "    \n",
    "        # Headers that need formula escaping - ONLY these two columns\n",
    "        headers_to_escape = ['Headword', 'Orthographic Variants']\n",
    "        \n",
    "        # Add single quote prefix ONLY for specified headers to prevent Excel formula interpretation\n",
    "        for header in headers_to_escape:\n",
    "            if header in df_copy.columns:\n",
    "                df_copy[header] = df_copy[header].astype(str).apply(\n",
    "                    lambda x: f\"'{x}\"\n",
    "                )\n",
    "            \n",
    "        if add_timestamp:\n",
    "            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "            base_name = filename.replace('.csv', '')\n",
    "            filename = f\"{base_name}_{timestamp}.csv\"\n",
    "        \n",
    "        filepath = os.path.join(self.working_dir, filename)\n",
    "        \n",
    "        df.to_csv(\n",
    "            filepath,\n",
    "            index=False,\n",
    "            encoding=self.encoding,\n",
    "            na_rep='',  # Explicit NA representation\n",
    "            quoting=csv.QUOTE_MINIMAL\n",
    "        )\n",
    "        \n",
    "        print(f\"Saved to CSV: {filepath}\")\n",
    "        return filepath\n",
    "    \n",
    "    def save_checkpoint_to_sqlite(self, df: pd.DataFrame, checkpoint_name: str, conn: sqlite3.Connection) -> None:\n",
    "        \"\"\"Save checkpoint to SQLite (only for critical stages)\"\"\"\n",
    "        \n",
    "        table_name = f\"checkpoint_{checkpoint_name}_{datetime.now().strftime('%Y%m%d')}\"\n",
    "        \n",
    "        df.to_sql(\n",
    "            table_name,\n",
    "            conn,\n",
    "            if_exists='replace',\n",
    "            index=False,\n",
    "            dtype='text'  # Store everything as text to preserve formatting\n",
    "        )\n",
    "        \n",
    "        print(f\"Checkpoint saved to SQLite: {table_name}\")\n",
    "        \n",
    "        # Also save metadata\n",
    "        metadata = pd.DataFrame([{\n",
    "            'checkpoint_name': checkpoint_name,\n",
    "            'table_name': table_name,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'row_count': len(df),\n",
    "            'column_count': len(df.columns),\n",
    "            'columns': ','.join(df.columns)\n",
    "        }])\n",
    "        \n",
    "        metadata.to_sql(\n",
    "            'checkpoint_metadata',\n",
    "            conn,\n",
    "            if_exists='append',\n",
    "            index=False\n",
    "        )\n",
    "    \n",
    "    def save_to_excel(self, data_dict: Dict[str, pd.DataFrame], filename: str) -> str:\n",
    "        \"\"\"Save multiple DataFrames to Excel file\"\"\"\n",
    "        \n",
    "        filepath = os.path.join(self.working_dir, filename)\n",
    "        \n",
    "        with pd.ExcelWriter(filepath, engine='openpyxl') as writer:\n",
    "            for sheet_name, df in data_dict.items():\n",
    "                # Excel sheet name limit is 31 characters\n",
    "                clean_sheet_name = sheet_name[:31] if len(sheet_name) > 31 else sheet_name\n",
    "                df.to_excel(writer, sheet_name=clean_sheet_name, index=False)\n",
    "        \n",
    "        print(f\"Saved to Excel: {filepath}\")\n",
    "        return filepath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e845f65",
   "metadata": {},
   "source": [
    "## Step 4: Database Initialization and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb9a1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatabaseManager:\n",
    "    \"\"\"Manages SQLite database connection and operations\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = DB_PATH):\n",
    "        self.db_path = db_path\n",
    "        self.conn = None\n",
    "        self._initialize_database()\n",
    "    \n",
    "    def _initialize_database(self):\n",
    "        \"\"\"Initialize database and create necessary tables\"\"\"\n",
    "        \n",
    "        # Create directory if it doesn't exist\n",
    "        db_dir = os.path.dirname(self.db_path)\n",
    "        if db_dir:\n",
    "            os.makedirs(db_dir, exist_ok=True)\n",
    "        \n",
    "        # Connect to database\n",
    "        self.conn = sqlite3.connect(self.db_path)\n",
    "        \n",
    "        print(f\"Database initialized: {self.db_path}\")\n",
    "        \n",
    "        # Create metadata tables if they don't exist\n",
    "        self._create_metadata_tables()\n",
    "        \n",
    "        # Show existing tables\n",
    "        self._show_database_info()\n",
    "    \n",
    "    def _create_metadata_tables(self):\n",
    "        \"\"\"Create metadata tables for tracking processing\"\"\"\n",
    "        \n",
    "        # Checkpoint metadata table\n",
    "        if self.conn is None:\n",
    "            raise ValueError(\"Database connection is not initialized.\")\n",
    "        \n",
    "        self.conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS checkpoint_metadata (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            checkpoint_name TEXT,\n",
    "            table_name TEXT,\n",
    "            timestamp TEXT,\n",
    "            row_count INTEGER,\n",
    "            column_count INTEGER,\n",
    "            columns TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        # Processing log table\n",
    "        self.conn.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS processing_log (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                stage TEXT,\n",
    "                status TEXT,\n",
    "                timestamp TEXT,\n",
    "                duration_seconds REAL,\n",
    "                rows_processed INTEGER,\n",
    "                notes TEXT\n",
    "            )\n",
    "        \"\"\")\n",
    "        \n",
    "        self.conn.commit()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def import_initial_dataset(self, csv_path: str, table_name: str = 'WHP_EarlyNahuatl_Data', \n",
    "                              encoding: str = DEFAULT_ENCODING, replace: bool = False) -> bool:\n",
    "        \"\"\"\n",
    "        Import initial CSV dataset into SQLite as the base reference.\n",
    "        This is a ONE-TIME operation to establish the source data in the database.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"INITIAL DATASET IMPORT TO SQLITE\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Check if table already exists\n",
    "        existing_tables = pd.read_sql(\n",
    "            \"SELECT name FROM sqlite_master WHERE type='table'\",\n",
    "            self.conn\n",
    "        )\n",
    "        \n",
    "        if table_name in existing_tables['name'].values:\n",
    "            if not replace:\n",
    "                print(f\"Table '{table_name}' already exists!\")\n",
    "                response = input(\"Do you want to replace it? (yes/no): \").lower()\n",
    "                if response != 'yes':\n",
    "                    print(\"Import cancelled\")\n",
    "                    return False\n",
    "            else:\n",
    "                print(f\" Replacing existing table '{table_name}'\")\n",
    "        \n",
    "        try:\n",
    "            # Load CSV with encoding protection\n",
    "            print(f\"Reading CSV file: {csv_path}\")\n",
    "            print(f\"   Encoding: {encoding}\")\n",
    "            \n",
    "            # First, detect actual encoding if needed\n",
    "            with open(csv_path, 'rb') as file:\n",
    "                raw_data = file.read(10000)\n",
    "                detected = chardet.detect(raw_data)\n",
    "                print(f\"   Detected encoding: {detected['encoding']} (confidence: {detected['confidence']:.2%})\")\n",
    "            \n",
    "            # Load the CSV\n",
    "            df = pd.read_csv(\n",
    "                csv_path,\n",
    "                encoding=encoding,\n",
    "                na_values=[''],\n",
    "                keep_default_na=True,\n",
    "                dtype=str,  # Import everything as text to preserve formatting\n",
    "                low_memory=False\n",
    "            )\n",
    "            \n",
    "            print(f\"Loaded {len(df):,} rows × {len(df.columns)} columns\")\n",
    "            \n",
    "            # Show sample of data\n",
    "            print(\"\\nData Sample (first 3 rows):\")\n",
    "            print(df.head(3).to_string(max_cols=5))\n",
    "            \n",
    "            # Import to SQLite\n",
    "            print(f\"\\nImporting to SQLite table: {table_name}\")\n",
    "            \n",
    "            df.to_sql(\n",
    "                table_name,\n",
    "                self.conn,\n",
    "                if_exists='replace' if replace else 'fail',\n",
    "                index=False,\n",
    "                dtype='text',  # Store as text to preserve all characters\n",
    "                chunksize=5000  # Process in chunks for large datasets\n",
    "            )\n",
    "            \n",
    "            # Create indexes for common query columns\n",
    "            print(\"Creating indexes...\")\n",
    "            \n",
    "            if self.conn is None:\n",
    "                raise ValueError(\"Database connection is not initialized.\")\n",
    "        \n",
    "            # Add index on Ref column if it exists\n",
    "            if 'Ref' in df.columns:\n",
    "                self.conn.execute(f\"CREATE INDEX IF NOT EXISTS idx_{table_name}_ref ON {table_name}(Ref)\")\n",
    "            \n",
    "            # Add index on Headword column if it exists  \n",
    "            if 'Headword' in df.columns:\n",
    "                self.conn.execute(f\"CREATE INDEX IF NOT EXISTS idx_{table_name}_headword ON {table_name}(Headword)\")\n",
    "            \n",
    "            self.conn.commit()\n",
    "            \n",
    "            # Verify import\n",
    "            verify_count = pd.read_sql(f\"SELECT COUNT(*) as cnt FROM {table_name}\", self.conn).iloc[0, 0]\n",
    "            \n",
    "            if verify_count == len(df):\n",
    "                print(f\"Successfully imported {verify_count:,} rows\")\n",
    "                \n",
    "                # Log the import\n",
    "                import_metadata = pd.DataFrame([{\n",
    "                    'operation': 'initial_import',\n",
    "                    'source_file': csv_path,\n",
    "                    'table_name': table_name,\n",
    "                    'timestamp': datetime.now().isoformat(),\n",
    "                    'row_count': verify_count,\n",
    "                    'column_count': len(df.columns),\n",
    "                    'columns': ','.join(df.columns),\n",
    "                    'encoding_used': encoding\n",
    "                }])\n",
    "                \n",
    "                import_metadata.to_sql(\n",
    "                    'import_history',\n",
    "                    self.conn,\n",
    "                    if_exists='append',\n",
    "                    index=False\n",
    "                )\n",
    "                \n",
    "                print(\"Import logged to metadata\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Row count mismatch! Expected {len(df)}, got {verify_count}\")\n",
    "                return False\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Import failed: {e}\")\n",
    "            return False\n",
    "        \n",
    "        \n",
    "    def _show_database_info(self):\n",
    "        \"\"\"Display information about the database\"\"\"\n",
    "        \n",
    "        tables = pd.read_sql(\n",
    "            \"SELECT name, type FROM sqlite_master WHERE type IN ('table', 'view')\",\n",
    "            self.conn\n",
    "        )\n",
    "        \n",
    "        if not tables.empty:\n",
    "            print(f\"Existing database objects: {len(tables)}\")\n",
    "            for _, row in tables.iterrows():\n",
    "                # Get row count for tables\n",
    "                if row['type'] == 'table':\n",
    "                    try:\n",
    "                        count = pd.read_sql(f\"SELECT COUNT(*) as cnt FROM [{row['name']}]\", self.conn).iloc[0, 0]\n",
    "                        print(f\"   - {row['name']} ({row['type']}): {count:,} rows\")\n",
    "                    except:\n",
    "                        print(f\"   - {row['name']} ({row['type']})\")\n",
    "        else:\n",
    "            print(\"No existing tables in database\")\n",
    "    \n",
    "    def log_processing_stage(self, stage: str, status: str, \n",
    "                        duration: Optional[float] = None, \n",
    "                        rows_processed: Optional[int] = None, \n",
    "                        notes: Optional[str] = None):\n",
    "        \"\"\"Log processing stage to database\"\"\"\n",
    "        \n",
    "        log_entry = pd.DataFrame([{\n",
    "            'stage': stage,\n",
    "            'status': status,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'duration_seconds': duration,\n",
    "            'rows_processed': rows_processed,\n",
    "            'notes': notes\n",
    "        }])\n",
    "        \n",
    "        log_entry.to_sql('processing_log', self.conn, if_exists='append', index=False)\n",
    "    \n",
    "    \n",
    "    def verify_base_dataset(self, table_name: str = 'WHP_EarlyNahuatl_Data') -> bool:\n",
    "        \"\"\"Verify that base dataset exists and is valid\"\"\"\n",
    "        \n",
    "        try:\n",
    "            # Check if table exists\n",
    "            tables = pd.read_sql(\n",
    "                f\"SELECT name FROM sqlite_master WHERE type='table' AND name='{table_name}'\",\n",
    "                self.conn\n",
    "            )\n",
    "            \n",
    "            if tables.empty:\n",
    "                print(f\"Base dataset table '{table_name}' not found\")\n",
    "                return False\n",
    "            \n",
    "            # Get basic info\n",
    "            count = pd.read_sql(f\"SELECT COUNT(*) as cnt FROM {table_name}\", self.conn).iloc[0, 0]\n",
    "            cols = pd.read_sql(f\"PRAGMA table_info({table_name})\", self.conn)\n",
    "            \n",
    "            print(f\"   Base dataset verified:\")\n",
    "            print(f\"   Table: {table_name}\")\n",
    "            print(f\"   Rows: {count:,}\")\n",
    "            print(f\"   Columns: {len(cols)}\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Verification failed: {e}\")\n",
    "            return False\n",
    "        \n",
    "        \n",
    "    def get_checkpoint_list(self) -> pd.DataFrame:\n",
    "        \"\"\"Get list of available checkpoints\"\"\"\n",
    "        \n",
    "        try:\n",
    "            checkpoints = pd.read_sql(\n",
    "                \"SELECT * FROM checkpoint_metadata ORDER BY timestamp DESC\",\n",
    "                self.conn\n",
    "            )\n",
    "            return checkpoints\n",
    "        except:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def load_checkpoint(self, checkpoint_name: Optional[str] = None, \n",
    "                   table_name: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"Load a specific checkpoint\"\"\"\n",
    "        \n",
    "        if table_name:\n",
    "            return pd.read_sql(f\"SELECT * FROM [{table_name}]\", self.conn)\n",
    "        elif checkpoint_name:\n",
    "            # Get most recent checkpoint with this name\n",
    "            query = \"\"\"\n",
    "                SELECT table_name \n",
    "                FROM checkpoint_metadata \n",
    "                WHERE checkpoint_name = ? \n",
    "                ORDER BY timestamp DESC \n",
    "                LIMIT 1\n",
    "            \"\"\"\n",
    "            result = pd.read_sql(query, self.conn, params=[checkpoint_name])\n",
    "            if not result.empty:\n",
    "                table_name = str(result.iloc[0, 0])\n",
    "                return pd.read_sql(f\"SELECT * FROM [{table_name}]\", self.conn)\n",
    "        \n",
    "        raise ValueError(\"No checkpoint found\")\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close database connection\"\"\"\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            print(\"Database connection closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc24e398",
   "metadata": {},
   "source": [
    "## Step 5: HTML Tag Handler (Detection + Repair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c084831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLTagHandler:\n",
    "    \"\"\"Handles HTML tag detection and repair\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Valid HTML tags\n",
    "        self.valid_tags = {\n",
    "            'p', 'br', 'div', 'span', 'a', 'b', 'i', 'u', 'strong', 'em',\n",
    "            'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'li', 'table',\n",
    "            'tr', 'td', 'th', 'img', 'link', 'meta', 'head', 'body', 'html',\n",
    "            'bibl', 'title', 'sup', 'sub', 'del'\n",
    "        }\n",
    "        \n",
    "        # Known malformations and their fixes\n",
    "        self.malformed_fixes = {\n",
    "            '</p</bibl>': '</p></bibl>',       \n",
    "            '<bibl<': '<bibl>',\n",
    "            '</bibbl>': '</bibl>',\n",
    "            '<bibbl>': '<bibl>',\n",
    "            '<bobl>': '<bibl>',\n",
    "            '</bobl>': '</bibl>',\n",
    "            '<b9bl>': '<bibl>',\n",
    "            '<bibi>': '<bibl>',\n",
    "            '<bibl></p>': '</bibl></p>',\n",
    "        }\n",
    "    \n",
    "    def find_all_tags(self, text: str) -> List[str]:\n",
    "        \"\"\"Find all angle bracket patterns\"\"\"\n",
    "        if pd.isna(text) or not text:\n",
    "            return []\n",
    "        return re.findall(r'</?[^<>]+/?>', str(text))\n",
    "    \n",
    "    def classify_tag(self, tag: str) -> Tuple[str, Optional[str]]:\n",
    "        \"\"\"\n",
    "        Classify a tag as: valid_html, malformed_html, or non_html\n",
    "        Returns: (tag_type, suggested_fix)\n",
    "        \"\"\"\n",
    "        # Check if it's a known malformation\n",
    "        if tag in self.malformed_fixes:\n",
    "            return 'malformed_html', self.malformed_fixes[tag]\n",
    "        \n",
    "        # Extract tag name\n",
    "        match = re.match(r'^</?([^>\\s/]+)', tag)\n",
    "        if match:\n",
    "            tag_name = match.group(1).lower()\n",
    "            if tag_name in self.valid_tags:\n",
    "                return 'valid_html', None\n",
    "            \n",
    "            # Check if it looks like HTML but misspelled\n",
    "            if any(valid in tag_name for valid in ['bibl', 'p', 'br', 'div']):\n",
    "                # Suggest a fix based on what it looks like\n",
    "                if 'bibl' in tag_name:\n",
    "                    return 'malformed_html', '</bibl>' if tag.startswith('</') else '<bibl>'\n",
    "                return 'malformed_html', tag  # Can't auto-fix\n",
    "        \n",
    "        # Not HTML at all\n",
    "        return 'non_html', None\n",
    "    \n",
    "    def repair_text(self, text: str) -> Tuple[str, int]:\n",
    "        \"\"\"\n",
    "        Repair all malformed HTML tags in text\n",
    "        Returns: (repaired_text, number_of_repairs)\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or not text:\n",
    "            return text, 0\n",
    "        \n",
    "        text_str = str(text)\n",
    "        repairs_made = 0\n",
    "        \n",
    "        # Apply known fixes\n",
    "        for malformed, fix in self.malformed_fixes.items():\n",
    "            if malformed in text_str:\n",
    "                count = text_str.count(malformed)\n",
    "                text_str = text_str.replace(malformed, fix)\n",
    "                repairs_made += count\n",
    "        \n",
    "        # Apply pattern-based fixes\n",
    "        text_str = re.sub(r'<<(\\w+)>', r'<\\1>', text_str)  # Double opening\n",
    "        text_str = re.sub(r'<(\\w+)>>', r'<\\1>', text_str)  # Double closing\n",
    "        \n",
    "        return text_str, repairs_made\n",
    "    \n",
    "    def encode_non_html(self, text: str) -> str:\n",
    "        \"\"\"Encode non-HTML angle brackets\"\"\"\n",
    "        if pd.isna(text) or not text:\n",
    "            return text\n",
    "        \n",
    "        text_str = str(text)\n",
    "        tags = self.find_all_tags(text_str)\n",
    "        \n",
    "        for tag in tags:\n",
    "            tag_type, _ = self.classify_tag(tag)\n",
    "            if tag_type == 'non_html':\n",
    "                encoded = tag.replace('<', '&lt;').replace('>', '&gt;')\n",
    "                text_str = text_str.replace(tag, encoded)\n",
    "        \n",
    "        return text_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7def01c3",
   "metadata": {},
   "source": [
    "### Step 5.1 HTML Malformed Tag Repair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c383d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MalformedTagRepairer:\n",
    "    \"\"\"Specialized handler for malformed HTML tag repair\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.html_analyzer = HTMLTagHandler() \n",
    "        self.html_tags = {\n",
    "            'p', 'br', 'div', 'span', 'a', 'b', 'i', 'u', 'strong', 'em',\n",
    "            'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'li', 'table',\n",
    "            'tr', 'td', 'th', 'img', 'bibl', 'title', 'sup', 'sub', 'del'\n",
    "        }\n",
    "        self.self_closing_tags = {'br', 'hr', 'img', 'input', 'meta', 'link'}\n",
    "        # Specific patterns for actual HTML malformations\n",
    "        self.html_malformation_patterns = {\n",
    "            '</bibbl>': '</bibl>',\n",
    "            '<bibbl>': '<bibl>',\n",
    "            '</bobl>': '</bibl>',\n",
    "            '<bobl>': '<bibl>',\n",
    "            '<b9bl>': '<bibl>',\n",
    "            '<bibi>': '<bibl>',\n",
    "            '</p</bibl>': '</p></bibl>',\n",
    "            '<p<': '<p>',\n",
    "            '</p>p>': '</p>',\n",
    "        }\n",
    "        self.auto_repairable_tags = {'bibl'}    \n",
    "        self.tag_closing_heuristics = {\n",
    "            'bibl': [\n",
    "                r'</p>',           # End at paragraph close\n",
    "                r'\\n\\s*<p',        # Before new paragraph  \n",
    "                r'\\.\\s*(?=<|$)',   # After period at end of sentence\n",
    "                r'$'               # End of text as fallback\n",
    "            ]\n",
    "        }\n",
    "        self.regex_patterns = {\n",
    "            r'<<(\\w+)>': r'<\\1>',        # Double opening\n",
    "            r'<(\\w+)>>': r'<\\1>',        # Double closing\n",
    "        }\n",
    "        \n",
    "    def detect_mismatches(self, text: str) -> list[dict[str, str]]:\n",
    "        \"\"\"Detect tag count mismatches\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return []\n",
    "        \n",
    "        mismatches = []\n",
    "        text_str = str(text)\n",
    "        \n",
    "        for tag_name in self.html_tags:\n",
    "            if tag_name in self.self_closing_tags:\n",
    "                continue\n",
    "            \n",
    "            # Count opening and closing tags\n",
    "            open_pattern = f\"<{tag_name}(?:\\\\s+[^>]*)?>\"\n",
    "            close_pattern = f'</{tag_name}>'\n",
    "            \n",
    "            open_count = len(re.findall(open_pattern, text_str, re.IGNORECASE))\n",
    "            close_count = len(re.findall(close_pattern, text_str, re.IGNORECASE))\n",
    "            \n",
    "            if open_count != close_count:\n",
    "                mismatches.append({\n",
    "                    'tag': tag_name,\n",
    "                    'open_count': open_count,\n",
    "                    'close_count': close_count,\n",
    "                    'issue': f'Mismatch: {open_count} open, {close_count} closed'\n",
    "                })\n",
    "        \n",
    "        return mismatches\n",
    "    \n",
    "    \n",
    "    def analyze_malformed_tags(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"Analyze both malformed patterns AND mismatches\"\"\"\n",
    "        from collections import Counter\n",
    "        \n",
    "        results = {\n",
    "            'malformed_by_row': [],\n",
    "            'malformed_summary': []\n",
    "        }\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            for col in df.columns:\n",
    "                cell_value = row[col]\n",
    "                if pd.notna(cell_value) and cell_value != '':\n",
    "                    text_str = str(cell_value)\n",
    "                    \n",
    "                    # Check for literal malformed patterns\n",
    "                    for pattern, replacement in self.html_malformation_patterns.items():\n",
    "                        if pattern in text_str:\n",
    "                            results['malformed_by_row'].append({\n",
    "                                'Row': idx,\n",
    "                                'Column': col,\n",
    "                                'Malformed_Tag': pattern,\n",
    "                                'Suggested_Repair': replacement,\n",
    "                                'Context': text_str[:100]\n",
    "                            })\n",
    "                    \n",
    "                    # ADD MISMATCH DETECTION\n",
    "                    mismatches = self.detect_mismatches(text_str)\n",
    "                    for mismatch in mismatches:\n",
    "                        results['malformed_by_row'].append({\n",
    "                            'Row': idx,\n",
    "                            'Column': col,\n",
    "                            'Malformed_Tag': f\"<{mismatch['tag']}>\",\n",
    "                            'Suggested_Repair': mismatch['issue'],\n",
    "                            'Context': text_str[:100]\n",
    "                        })\n",
    "                        \n",
    "        for idx, row in df.iterrows():\n",
    "            for col in df.columns:\n",
    "                cell_value = row[col]\n",
    "                if pd.notna(cell_value) and cell_value != '':\n",
    "                    text_str = str(cell_value)\n",
    "                    \n",
    "                    # Check regex patterns\n",
    "                    for pattern, replacement in self.regex_patterns.items():\n",
    "                        if re.search(pattern, text_str):\n",
    "                            results['malformed_by_row'].append({\n",
    "                                'Row': idx,\n",
    "                                'Column': col,\n",
    "                                'Malformed_Tag': f\"REGEX:{pattern}\",\n",
    "                                'Suggested_Repair': replacement,\n",
    "                                'Context': text_str[:100]\n",
    "                            })\n",
    "        # Create summary\n",
    "        if results['malformed_by_row']:\n",
    "            pattern_counts = Counter(item['Malformed_Tag'] for item in results['malformed_by_row'])\n",
    "            \n",
    "            for pattern, count in pattern_counts.items():\n",
    "                first_occurrence = next(item for item in results['malformed_by_row'] \n",
    "                                       if item['Malformed_Tag'] == pattern)\n",
    "                \n",
    "                results['malformed_summary'].append({\n",
    "                    'Malformed_Tag': pattern,\n",
    "                    'Count': count,\n",
    "                    'Suggested_Repair': first_occurrence['Suggested_Repair'],\n",
    "                    'Sample_Context': first_occurrence['Context']\n",
    "                })\n",
    "        \n",
    "        malformed_by_row_df = pd.DataFrame(results['malformed_by_row'])\n",
    "        malformed_summary_df = pd.DataFrame(results['malformed_summary'])\n",
    "        \n",
    "        if not malformed_summary_df.empty:\n",
    "            malformed_summary_df = malformed_summary_df.sort_values('Count', ascending=False)\n",
    "        \n",
    "        return {\n",
    "            'Malformed_Tags_by_Row': malformed_by_row_df,\n",
    "            'Malformed_Tags_Summary': malformed_summary_df\n",
    "        }\n",
    "    \n",
    "    def repair_tags(self, df: pd.DataFrame, malformed_analysis: Dict) -> pd.DataFrame:\n",
    "        \"\"\"Apply both malformed tag repairs AND mismatch repairs\"\"\"\n",
    "        df_repaired = df.copy()\n",
    "        \n",
    "        if malformed_analysis['Malformed_Tags_Summary'].empty:\n",
    "            return df_repaired\n",
    "        \n",
    "        # 1. Apply literal pattern replacements (existing logic)\n",
    "        literal_repairs = 0\n",
    "        for _, row in malformed_analysis['Malformed_Tags_Summary'].iterrows():\n",
    "            pattern = row['Malformed_Tag']\n",
    "            repair = row['Suggested_Repair']\n",
    "            \n",
    "            # Skip mismatches and regex patterns for now\n",
    "            if \"Mismatch:\" in str(repair) or pattern.startswith(\"REGEX:\"):\n",
    "                continue\n",
    "            \n",
    "            # Apply literal pattern replacements\n",
    "            if pattern in self.html_malformation_patterns:\n",
    "                for col in df_repaired.columns:\n",
    "                    if df_repaired[col].dtype == 'object':\n",
    "                        before_count = df_repaired[col].astype(str).str.contains(\n",
    "                            re.escape(pattern), regex=True\n",
    "                        ).sum()\n",
    "                        \n",
    "                        df_repaired[col] = df_repaired[col].astype(str).str.replace(\n",
    "                            pattern, repair, regex=False\n",
    "                        )\n",
    "                        \n",
    "                        after_count = df_repaired[col].astype(str).str.contains(\n",
    "                            re.escape(pattern), regex=True\n",
    "                        ).sum()\n",
    "                        \n",
    "                        literal_repairs += (before_count - after_count)\n",
    "        \n",
    "        print(f\"  Applied {literal_repairs} literal tag repairs\")\n",
    "        \n",
    "        # 2. Apply mismatch repairs (NEW!)\n",
    "        df_repaired = self.repair_tag_mismatches(df_repaired, malformed_analysis)\n",
    "        \n",
    "        return df_repaired\n",
    "\n",
    "    def generate_mismatch_report(self, malformed_analysis: Dict) -> pd.DataFrame:\n",
    "            \"\"\"Generate a report of mismatches for manual review\"\"\"\n",
    "            mismatch_rows = []\n",
    "            \n",
    "            for _, row in malformed_analysis['Malformed_Tags_Summary'].iterrows():\n",
    "                if \"Mismatch:\" in str(row['Suggested_Repair']):\n",
    "                    mismatch_rows.append({\n",
    "                        'Tag': row['Malformed_Tag'],\n",
    "                        'Issue': row['Suggested_Repair'],\n",
    "                        'Occurrences': row['Count'],\n",
    "                        'Sample_Context': row['Sample_Context'],\n",
    "                        'Action_Required': 'Manual review needed'\n",
    "                    })\n",
    "            \n",
    "            return pd.DataFrame(mismatch_rows)\n",
    "    \n",
    "    def _extract_mismatch_info(self, malformed_analysis: Dict) -> Dict:\n",
    "        \"\"\"Extract mismatch information from analysis\"\"\"\n",
    "        mismatches = {}\n",
    "        \n",
    "        for _, row in malformed_analysis['Malformed_Tags_Summary'].iterrows():\n",
    "            if \"Mismatch:\" in str(row['Suggested_Repair']):\n",
    "                tag = row['Malformed_Tag'].replace('<', '').replace('>', '')\n",
    "                issue = row['Suggested_Repair']\n",
    "                \n",
    "                # Parse \"Mismatch: X open, Y closed\"\n",
    "                import re\n",
    "                match = re.search(r'Mismatch: (\\d+) open, (\\d+) closed', issue)\n",
    "                if match:\n",
    "                    mismatches[tag] = {\n",
    "                        'open_count': int(match.group(1)),\n",
    "                        'close_count': int(match.group(2)),\n",
    "                        'occurrences': row['Count']\n",
    "                    }\n",
    "        \n",
    "        return mismatches\n",
    "    \n",
    "    def _add_missing_closing_tags(self, df: pd.DataFrame, tag_name: str, missing_count: int) -> tuple[pd.DataFrame, int]:\n",
    "        \"\"\"Add missing closing tags at logical boundaries\"\"\"\n",
    "        df_repaired = df.copy()\n",
    "        total_repairs = 0\n",
    "        \n",
    "        for col in df_repaired.columns:\n",
    "            if df_repaired[col].dtype != 'object':\n",
    "                continue\n",
    "                \n",
    "            for idx in df_repaired.index:\n",
    "                text = df_repaired.at[idx, col]\n",
    "                if pd.isna(text) or text == '':\n",
    "                    continue\n",
    "                \n",
    "                text_str = str(text)\n",
    "                # Check if this cell has the tag mismatch\n",
    "                open_count = len(re.findall(f\"<{tag_name}(?:\\\\s+[^>]*)?>\" , text_str, re.IGNORECASE))\n",
    "                close_count = len(re.findall(f'</{tag_name}>', text_str, re.IGNORECASE))\n",
    "                \n",
    "                if open_count > close_count:\n",
    "                    cell_missing = open_count - close_count\n",
    "                    repaired_text = self._repair_text_missing_closes(text_str, tag_name, cell_missing)\n",
    "                    \n",
    "                    if repaired_text != text_str:\n",
    "                        df_repaired.at[idx, col] = repaired_text\n",
    "                        total_repairs += cell_missing\n",
    "        \n",
    "        return df_repaired, total_repairs\n",
    "    \n",
    "    def _repair_text_missing_closes(self, text: str, tag_name: str, missing_count: int) -> str:\n",
    "        \"\"\"Apply smart heuristics to add missing closing tags\"\"\"\n",
    "        if tag_name not in self.tag_closing_heuristics:\n",
    "            return text\n",
    "        \n",
    "        rules = self.tag_closing_heuristics[tag_name]\n",
    "        repaired_text = text\n",
    "        remaining_to_fix = missing_count\n",
    "        \n",
    "        # Try each heuristic rule in order of preference\n",
    "        for rule in rules:\n",
    "            if remaining_to_fix <= 0:\n",
    "                break\n",
    "                \n",
    "            # Find all potential closing positions\n",
    "            matches = list(re.finditer(rule, repaired_text))\n",
    "            \n",
    "            # Insert closing tags (in reverse order to preserve positions)\n",
    "            insert_count = min(len(matches), remaining_to_fix)\n",
    "            for match in reversed(matches[:insert_count]):\n",
    "                pos = match.start()\n",
    "                repaired_text = (repaired_text[:pos] + \n",
    "                               f'</{tag_name}>' + \n",
    "                               repaired_text[pos:])\n",
    "                remaining_to_fix -= 1\n",
    "        \n",
    "        return repaired_text\n",
    "    \n",
    "    def _add_missing_opening_tags(self, text: str, tag_name: str, missing_count: int) -> str:\n",
    "        \"\"\"Add opening tags before orphaned closing tags using smart heuristics\"\"\"\n",
    "        \n",
    "        # Define backward-looking heuristics for different tag types\n",
    "        opening_heuristics = {\n",
    "            'bibl': [\n",
    "                r'([^<>\\n]{1,100})(\\s*</bibl>)', # Longer content for bibliography\n",
    "            ],\n",
    "        }\n",
    "        \n",
    "        if tag_name not in opening_heuristics:\n",
    "            return text\n",
    "        \n",
    "        repaired_text = text\n",
    "        rules = opening_heuristics[tag_name]\n",
    "        remaining_to_fix = missing_count\n",
    "        \n",
    "        # Apply heuristics to add opening tags\n",
    "        for rule in rules:\n",
    "            if remaining_to_fix <= 0:\n",
    "                break\n",
    "                \n",
    "            # Find matches and add opening tags (process in reverse to preserve positions)\n",
    "            matches = list(re.finditer(rule, repaired_text))\n",
    "            for match in reversed(matches[:remaining_to_fix]):\n",
    "                content = match.group(1)\n",
    "                closing_part = match.group(2)\n",
    "                \n",
    "                # Insert opening tag before the content\n",
    "                replacement = f'<{tag_name}>{content}{closing_part}'\n",
    "                start, end = match.span()\n",
    "                repaired_text = repaired_text[:start] + replacement + repaired_text[end:]\n",
    "                remaining_to_fix -= 1\n",
    "        \n",
    "        return repaired_text\n",
    "    \n",
    "    def _fix_orphaned_closing_tags(self, df: pd.DataFrame, tag_name: str, orphaned_count: int) -> tuple[pd.DataFrame, int]:\n",
    "        \"\"\"Add missing opening tags for orphaned closing tags\"\"\"\n",
    "        df_repaired = df.copy()\n",
    "        total_additions = 0\n",
    "        \n",
    "        for col in df_repaired.columns:\n",
    "            if df_repaired[col].dtype != 'object':\n",
    "                continue\n",
    "                \n",
    "            for idx in df_repaired.index:\n",
    "                text = df_repaired.at[idx, col]\n",
    "                if pd.isna(text) or text == '':\n",
    "                    continue\n",
    "                \n",
    "                text_str = str(text)\n",
    "                open_count = len(re.findall(f\"<{tag_name}(?:\\\\s+[^>]*)?>\" , text_str, re.IGNORECASE))\n",
    "                close_count = len(re.findall(f'</{tag_name}>', text_str, re.IGNORECASE))\n",
    "                \n",
    "                if close_count > open_count:\n",
    "                    excess_closes = close_count - open_count\n",
    "                    repaired_text = self._add_missing_opening_tags(text_str, tag_name, excess_closes)\n",
    "                    \n",
    "                    if repaired_text != text_str:\n",
    "                        df_repaired.at[idx, col] = repaired_text\n",
    "                        total_additions += excess_closes\n",
    "        \n",
    "        return df_repaired, total_additions\n",
    "    \n",
    "    def repair_tag_mismatches(self, df: pd.DataFrame, malformed_analysis: Dict) -> pd.DataFrame:\n",
    "        \"\"\"Repair ONLY auto-repairable tag mismatches (currently just bibl)\"\"\"\n",
    "        df_repaired = df.copy()\n",
    "        repairs_made = 0\n",
    "        manual_review_needed = []\n",
    "        \n",
    "        # Extract mismatch information from analysis\n",
    "        mismatch_info = self._extract_mismatch_info(malformed_analysis)\n",
    "        \n",
    "        for tag_name, details in mismatch_info.items():\n",
    "            open_count = details['open_count']\n",
    "            close_count = details['close_count']\n",
    "            \n",
    "            if tag_name in self.auto_repairable_tags:\n",
    "                # AUTO-REPAIR: Only bibl tags\n",
    "                if open_count > close_count:\n",
    "                    missing_closes = open_count - close_count\n",
    "                    df_repaired, fixed_count = self._add_missing_closing_tags(\n",
    "                        df_repaired, tag_name, missing_closes\n",
    "                    )\n",
    "                    repairs_made += fixed_count\n",
    "                    print(f\"  ✓ AUTO-FIXED: Added {fixed_count} missing </{tag_name}> tags\")\n",
    "                    \n",
    "                elif close_count > open_count:\n",
    "                    orphaned_closes = close_count - open_count\n",
    "                    df_repaired, added_count = self._fix_orphaned_closing_tags(\n",
    "                        df_repaired, tag_name, orphaned_closes\n",
    "                    )\n",
    "                    repairs_made += added_count\n",
    "                    print(f\"  ✓ AUTO-FIXED: Added {added_count} missing <{tag_name}> opening tags\")\n",
    "            else:\n",
    "                # MANUAL REVIEW: All other tags\n",
    "                manual_review_needed.append({\n",
    "                    'tag': tag_name,\n",
    "                    'open_count': open_count,\n",
    "                    'close_count': close_count,\n",
    "                    'occurrences': details['occurrences']\n",
    "                })\n",
    "                print(f\"MANUAL REVIEW: <{tag_name}> has {open_count} open, {close_count} closed ({details['occurrences']} occurrences)\")\n",
    "        \n",
    "        if manual_review_needed:\n",
    "            print(f\"\\nSUMMARY: {len(manual_review_needed)} tag types need manual review:\")\n",
    "            for item in manual_review_needed:\n",
    "                print(f\"   - <{item['tag']}>: {item['open_count']} open, {item['close_count']} closed\")\n",
    "        \n",
    "        print(f\"Total automatic repairs: {repairs_made}\")\n",
    "        return df_repaired\n",
    "        \n",
    "    def find_mismatch_cells(self, df: pd.DataFrame, target_tags: list) -> pd.DataFrame:\n",
    "        \"\"\"Find specific cells that contain tag mismatches\"\"\"\n",
    "        problem_cells = []\n",
    "        \n",
    "        for col in df.columns:\n",
    "            if df[col].dtype != 'object':\n",
    "                continue\n",
    "                \n",
    "            for idx in df.index:\n",
    "                text = df.at[idx, col]\n",
    "                if pd.isna(text) or text == '':\n",
    "                    continue\n",
    "                    \n",
    "                text_str = str(text)\n",
    "                \n",
    "                for tag_name in target_tags:\n",
    "                    if tag_name in self.self_closing_tags:\n",
    "                        continue\n",
    "                        \n",
    "                    # Count tags\n",
    "                    open_count = len(re.findall(f\"<{tag_name}(?:\\\\s+[^>]*)?>\" , text_str, re.IGNORECASE))\n",
    "                    close_count = len(re.findall(f'</{tag_name}>', text_str, re.IGNORECASE))\n",
    "                    \n",
    "                    if open_count != close_count:\n",
    "                        problem_cells.append({\n",
    "                            'Row': idx + 1,  # +1 for Excel-style numbering\n",
    "                            'Column': col,\n",
    "                            'Tag': tag_name,\n",
    "                            'Open_Count': open_count,\n",
    "                            'Close_Count': close_count,\n",
    "                            'Content_Preview': text_str[:200] + '...' if len(text_str) > 200 else text_str\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(problem_cells).sort_values(['Tag', 'Row'])  \n",
    "    \n",
    "    def get_repair_statistics(self, malformed_analysis: Dict) -> Dict:\n",
    "            \"\"\"Enhanced statistics showing auto vs manual repair breakdown\"\"\"\n",
    "            stats = {\n",
    "                'total_issues': len(malformed_analysis['Malformed_Tags_by_Row']),\n",
    "                'unique_patterns': len(malformed_analysis['Malformed_Tags_Summary']),\n",
    "                'auto_fixable': 0,\n",
    "                'manual_review_needed': 0,\n",
    "                'literal_malformations': 0,\n",
    "                'tag_mismatches': 0,\n",
    "                'bibl_mismatches': 0,  # Track bibl specifically\n",
    "                'other_mismatches': 0\n",
    "            }\n",
    "            \n",
    "            for _, row in malformed_analysis['Malformed_Tags_Summary'].iterrows():\n",
    "                if \"Mismatch:\" in str(row['Suggested_Repair']):\n",
    "                    stats['tag_mismatches'] += 1\n",
    "                    tag_name = row['Malformed_Tag'].replace('<', '').replace('>', '')\n",
    "                    \n",
    "                    if tag_name in self.auto_repairable_tags:\n",
    "                        stats['auto_fixable'] += row['Count']\n",
    "                        stats['bibl_mismatches'] += row['Count']\n",
    "                    else:\n",
    "                        stats['manual_review_needed'] += row['Count'] \n",
    "                        stats['other_mismatches'] += row['Count']\n",
    "                else:\n",
    "                    stats['auto_fixable'] += row['Count']\n",
    "                    stats['literal_malformations'] += 1\n",
    "            \n",
    "            return stats\n",
    "\n",
    "    def generate_manual_review_report(self, malformed_analysis: Dict) -> pd.DataFrame:\n",
    "        \"\"\"Generate a focused report for manual review (non-bibl tags)\"\"\"\n",
    "        manual_rows = []\n",
    "        \n",
    "        for _, row in malformed_analysis['Malformed_Tags_Summary'].iterrows():\n",
    "            if \"Mismatch:\" in str(row['Suggested_Repair']):\n",
    "                tag_name = row['Malformed_Tag'].replace('<', '').replace('>', '')\n",
    "                \n",
    "                if tag_name not in self.auto_repairable_tags:\n",
    "                    manual_rows.append({\n",
    "                        'Tag': row['Malformed_Tag'],\n",
    "                        'Issue': row['Suggested_Repair'],\n",
    "                        'Occurrences': row['Count'],\n",
    "                        'Sample_Context': row['Sample_Context'],\n",
    "                        'Action_Required': 'Manual review needed',\n",
    "                        'Issue_Type': 'Tag Mismatch'\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(manual_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f577a9",
   "metadata": {},
   "source": [
    "### Step 5.2 NonHTMLTagProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6bf446f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NonHTMLTagProcessor:\n",
    "    \"\"\"Handles encoding of non-HTML angle brackets - OPTIMIZED VERSION\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.html_analyzer = HTMLTagHandler()\n",
    "        \n",
    "        # Known HTML tags that should never be encoded\n",
    "        self.valid_html_tags = {\n",
    "            'p', 'br', 'div', 'span', 'a', 'b', 'i', 'u', 'strong', 'em',\n",
    "            'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'ul', 'ol', 'li', 'table',\n",
    "            'tr', 'td', 'th', 'img', 'bibl', 'title', 'sup', 'sub', 'del'\n",
    "        }\n",
    "    \n",
    "    def find_non_html_tags(self, text: str) -> List[str]:\n",
    "        \"\"\"Find tags that are definitely not HTML\"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return []\n",
    "        \n",
    "        # Find all < ... > patterns\n",
    "        pattern = r'<[^<>]*>'\n",
    "        all_brackets = re.findall(pattern, str(text))\n",
    "        \n",
    "        non_html_tags = []\n",
    "        for tag in all_brackets:\n",
    "            # Check if it's valid HTML\n",
    "            tag_type, _ = self.html_analyzer.classify_tag(tag)\n",
    "            if tag_type == 'non_html':\n",
    "                non_html_tags.append(tag)\n",
    "        \n",
    "        return non_html_tags\n",
    "    \n",
    "    def analyze_non_html_tags(self, df: pd.DataFrame) -> Dict[str, pd.DataFrame]:\n",
    "        \"\"\"OPTIMIZED: Analyze non-HTML tags across DataFrame efficiently\"\"\"\n",
    "        results = {\n",
    "            'non_html_by_row': [],\n",
    "            'non_html_summary': []\n",
    "        }\n",
    "        \n",
    "        # Collect all unique non-HTML tags first\n",
    "        unique_non_html_tags = set()\n",
    "        tag_locations = defaultdict(list)\n",
    "        \n",
    "        # Process in chunks for better memory efficiency\n",
    "        for idx, row in df.iterrows():\n",
    "            # Process multiple columns at once\n",
    "            for col in df.columns:\n",
    "                cell_value = row[col]\n",
    "                if pd.notna(cell_value) and cell_value != '':\n",
    "                    non_html_tags = self.find_non_html_tags(cell_value)\n",
    "                    for tag in non_html_tags:\n",
    "                        unique_non_html_tags.add(tag)\n",
    "                        tag_locations[tag].append({\n",
    "                            'Row': idx,\n",
    "                            'Column': col,\n",
    "                            'Context': str(cell_value)[:100]\n",
    "                        })\n",
    "        \n",
    "        # Build results from collected data\n",
    "        for tag, locations in tag_locations.items():\n",
    "            for loc in locations:\n",
    "                results['non_html_by_row'].append({\n",
    "                    'Row': loc['Row'],\n",
    "                    'Column': loc['Column'],\n",
    "                    'Non_HTML_Tag': tag,\n",
    "                    'Context': loc['Context']\n",
    "                })\n",
    "        \n",
    "        # Create summary\n",
    "        if unique_non_html_tags:\n",
    "            summary_data = []\n",
    "            for tag in unique_non_html_tags:\n",
    "                summary_data.append({\n",
    "                    'Tag': tag,\n",
    "                    'Count': len(tag_locations[tag]),\n",
    "                    'Encoded_Version': tag.replace('<', '&lt;').replace('>', '&gt;')\n",
    "                })\n",
    "            results['non_html_summary'] = summary_data\n",
    "        \n",
    "        non_html_by_row_df = pd.DataFrame(results['non_html_by_row'])\n",
    "        non_html_summary_df = pd.DataFrame(results['non_html_summary'])\n",
    "        \n",
    "        return {\n",
    "            'Non_HTML_Tags_by_Row': non_html_by_row_df,\n",
    "            'Non_HTML_Tags_Summary': non_html_summary_df\n",
    "        }\n",
    "    \n",
    "    def encode_brackets(self, df: pd.DataFrame, non_html_analysis: Dict) -> pd.DataFrame:\n",
    "        \"\"\"OPTIMIZED: Apply non-HTML bracket encoding using vectorized operations\"\"\"\n",
    "        df_encoded = df.copy()\n",
    "        \n",
    "        # Get unique non-HTML tags from analysis\n",
    "        if not non_html_analysis['Non_HTML_Tags_Summary'].empty:\n",
    "            # Process all replacements at once using vectorized operations\n",
    "            for _, row in non_html_analysis['Non_HTML_Tags_Summary'].iterrows():\n",
    "                tag = row['Tag']\n",
    "                encoded_tag = row['Encoded_Version']\n",
    "                \n",
    "                # Apply to all string columns at once\n",
    "                for col in df_encoded.columns:\n",
    "                    if df_encoded[col].dtype == 'object':  # Only process string columns\n",
    "                        # Use vectorized string replacement\n",
    "                        mask = df_encoded[col].astype(str).str.contains(\n",
    "                            re.escape(tag), regex=True, na=False\n",
    "                        )\n",
    "                        if mask.any():\n",
    "                            df_encoded.loc[mask, col] = df_encoded.loc[mask, col].astype(str).str.replace(\n",
    "                                tag, encoded_tag, regex=False\n",
    "                            )\n",
    "        \n",
    "        return df_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ac2e75",
   "metadata": {},
   "source": [
    "### Step 5.3 Link Validator `<a>` tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8e5fcc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkValidator:\n",
    "    def __init__(self):\n",
    "        # regex for raw <http://...> or <https://...>\n",
    "        self.raw_link_pattern = re.compile(r'<(https?://[^>\\s]+)>')\n",
    "        # regex for opening <a> tags\n",
    "        self.anchor_open_pattern = re.compile(r'<a([^>]*)>', flags=re.IGNORECASE)\n",
    "        # regex for closing </a>\n",
    "        self.anchor_close_pattern = re.compile(r'</a>', flags=re.IGNORECASE)\n",
    "    \n",
    "    def has_href_attribute(self, attrs_string):\n",
    "        # Look for href attribute with optional whitespace around =\n",
    "        href_pattern = r'href\\s*=\\s*[\"\\'][^\"\\']*[\"\\']'\n",
    "        return bool(re.search(href_pattern, attrs_string, re.IGNORECASE))\n",
    "   \n",
    "    def validate_links(self, text: str) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Validates links inside a text string.\n",
    "        - Finds raw URLs in <> that should be <a href=\"\">\n",
    "        - Ensures <a> tags have href\n",
    "        - Ensures <a> tags are properly closed\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return []\n",
    "        issues = []\n",
    "        text_str = str(text)\n",
    "        \n",
    "        # 1. Detect raw <http://...>\n",
    "        for url in self.raw_link_pattern.findall(text_str):\n",
    "            issues.append({\n",
    "                \"Issue\": \"Raw URL in angle brackets\",\n",
    "                \"URL\": url,\n",
    "                \"Suggested_Fix\": f'<a href=\"{url}\">{url}</a>'\n",
    "            })\n",
    "        \n",
    "        # 2. Detect <a> tags without href\n",
    "        for attrs in self.anchor_open_pattern.findall(text_str):\n",
    "            if not self.has_href_attribute(attrs):\n",
    "                issues.append({\n",
    "                    \"Issue\": \"Anchor tag missing href\",\n",
    "                    \"Tag\": f\"<a{attrs}>\",\n",
    "                    \"Suggested_Fix\": '<a href=\"URL_HERE\">'\n",
    "                })\n",
    "        \n",
    "        # 3. Check open/close balance\n",
    "        open_count = len(self.anchor_open_pattern.findall(text_str))\n",
    "        close_count = len(self.anchor_close_pattern.findall(text_str))\n",
    "        if open_count != close_count:\n",
    "            issues.append({\n",
    "                \"Issue\": \"Anchor tag mismatch\",\n",
    "                \"Opens\": open_count,\n",
    "                \"Closes\": close_count,\n",
    "                \"Suggested_Fix\": \"Ensure each <a> has matching </a>\"\n",
    "            })\n",
    "        \n",
    "        return issues\n",
    "    \n",
    "    def repair_a_tags(self, df_columns: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Repair <a> tag issues in the dataframe columns.\n",
    "        Currently handles:\n",
    "        - Converting raw URLs in angle brackets to proper <a href=\"\"> tags\n",
    "        \"\"\"\n",
    "        repaired_df = df_columns.copy()\n",
    "        repairs_made = 0\n",
    "        \n",
    "        for col in repaired_df.columns:\n",
    "            for idx in repaired_df.index:\n",
    "                value = repaired_df.at[idx, col]\n",
    "                \n",
    "                if pd.isna(value) or value == '':\n",
    "                    continue\n",
    "                    \n",
    "                text_str = str(value)\n",
    "                original_text = text_str\n",
    "                \n",
    "                # Fix raw URLs in angle brackets\n",
    "                def replace_raw_url(match):\n",
    "                    url = match.group(1)\n",
    "                    return f'<a href=\"{url}\">{url}</a>'\n",
    "                \n",
    "                text_str = self.raw_link_pattern.sub(replace_raw_url, text_str)\n",
    "                \n",
    "                # Update if changes were made\n",
    "                if text_str != original_text:\n",
    "                    repaired_df.at[idx, col] = text_str\n",
    "                    repairs_made += 1\n",
    "        \n",
    "        print(f\"  Repaired {repairs_made} raw URL issues\")\n",
    "        return repaired_df\n",
    "    \n",
    "    def analyze_link_issues(self, df_columns: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze all link issues in the dataframe columns.\n",
    "        Returns structured analysis similar to other analyzers.\n",
    "        \"\"\"\n",
    "        all_issues = []\n",
    "        \n",
    "        for col in df_columns.columns:\n",
    "            for idx in df_columns.index:\n",
    "                value = df_columns.at[idx, col]\n",
    "                issues = self.validate_links(value)\n",
    "                \n",
    "                for issue in issues:\n",
    "                    issue[\"Row\"] = idx + 1  # Fix: Add 1 to match actual row numbers\n",
    "                    issue[\"Column\"] = col\n",
    "                    all_issues.append(issue)\n",
    "        \n",
    "        issues_df = pd.DataFrame(all_issues) if all_issues else pd.DataFrame()\n",
    "        \n",
    "        return {\n",
    "            'Link_Issues_by_Row': issues_df,\n",
    "            'Total_Issues': len(all_issues),\n",
    "            'Repairable_Issues': len([i for i in all_issues if i['Issue'] == 'Raw URL in angle brackets'])\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e678a5",
   "metadata": {},
   "source": [
    "## Step 6 HTML Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69e35f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTMLProcessor:\n",
    "    \"\"\"Main processor that coordinates all operations - OPTIMIZED\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Use YOUR existing comprehensive classes\n",
    "        self.tag_handler = HTMLTagHandler()\n",
    "        self.malformed_repairer = MalformedTagRepairer()\n",
    "        self.non_html_processor = NonHTMLTagProcessor()\n",
    "        self.link_validator = LinkValidator()\n",
    "        self.stats = {}\n",
    "        self.content_columns = [\n",
    "            'Principal English Translation',\n",
    "            'Attestations from sources in English',\n",
    "            'Attestations from sources in Spanish',\n",
    "            'Alonso de Molina',\n",
    "            'Frances Karttunen', \n",
    "            'Horacio Carochi / English',\n",
    "            'Andrés de Olmos',\n",
    "            \"Lockhart’s Nahuatl as Written\",\n",
    "            \n",
    "        ]\n",
    "    \n",
    "    \n",
    "    def  process_dataframe(\n",
    "        self, df: pd.DataFrame, \n",
    "        repair_malformed: bool = True,\n",
    "        validate_links: bool = True,\n",
    "        extract_citations: bool = True,\n",
    "        extract_crossrefs: bool = True,\n",
    "        encode_non_html: bool = True\n",
    "    ) -> Tuple[pd.DataFrame, Dict]:\n",
    "        \n",
    "        \"\"\"\n",
    "        Process entire dataframe efficiently, including:\n",
    "        - Malformed HTML repair\n",
    "        - Non-HTML tag encoding\n",
    "        - <a> link validation\n",
    "        Returns: (processed_df, analysis_report)\n",
    "        \"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"HTML PROCESSING PIPELINE\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        result_df = df.copy()\n",
    "        \n",
    "        # Add extraction columns if needed\n",
    "        if extract_citations:\n",
    "            result_df['Citations'] = ''\n",
    "            result_df['Number_of_Citations'] = 0\n",
    "        if extract_crossrefs:\n",
    "            result_df['Cross_References'] = ''\n",
    "            result_df['Number_of_Cross_References'] = 0\n",
    "        \n",
    "        analysis = {\n",
    "            'repairs_made': 0,\n",
    "            'citations_extracted': 0,\n",
    "            'crossrefs_extracted': 0,\n",
    "            'non_html_encoded': 0,\n",
    "            'tag_analysis': [],\n",
    "            'processing_details': []\n",
    "        }\n",
    "        report = {}\n",
    "        reports = []  # collect all issues for unified mismatch report\n",
    "        total_rows = len(result_df)\n",
    "        print(f\"\\nProcessing {total_rows:,} rows...\")\n",
    "\n",
    "        # -------------------------------\n",
    "        # STEP 1: Malformed HTML tags\n",
    "        # -------------------------------\n",
    "        if repair_malformed:\n",
    "            print(\"  Analyzing malformed tags...\")\n",
    "            malformed_analysis = self.malformed_repairer.analyze_malformed_tags(result_df[self.content_columns])\n",
    "\n",
    "            if not malformed_analysis['Malformed_Tags_Summary'].empty:\n",
    "                stats = self.malformed_repairer.get_repair_statistics(malformed_analysis)\n",
    "                print(f\"  Found {stats['total_issues']} malformed tag instances\")\n",
    "                print(f\"  Found {stats['unique_patterns']} unique malformed patterns\")\n",
    "                print(f\"    - Auto-fixable: {stats['auto_fixable']}\")\n",
    "                print(f\"    - Manual review needed: {stats['manual_review_needed']}\")\n",
    "\n",
    "                mismatch_report = self.malformed_repairer.generate_mismatch_report(malformed_analysis)\n",
    "                if not mismatch_report.empty:\n",
    "                    mismatch_report = mismatch_report.copy()\n",
    "                    mismatch_report[\"Issue_Type\"] = \"Malformed Tag\"\n",
    "                    reports.append(mismatch_report)\n",
    "\n",
    "                print(\"  Repairing malformed tags...\")\n",
    "                result_df[self.content_columns] = self.malformed_repairer.repair_tags(\n",
    "                    result_df[self.content_columns],\n",
    "                    malformed_analysis\n",
    "                )\n",
    "\n",
    "                analysis['repairs_made'] = stats['auto_fixable']\n",
    "                analysis['bibl_repairs'] = stats.get('bibl_mismatches', 0)\n",
    "                analysis['repair_stats'] = stats\n",
    "                # optional raw malformed data\n",
    "                analysis['malformed_repairs'] = malformed_analysis\n",
    "            else:\n",
    "                print(\"  No malformed tags found\")\n",
    "\n",
    "        # -------------------------------\n",
    "        # STEP 2: Non-HTML tags\n",
    "        # -------------------------------\n",
    "        if encode_non_html:\n",
    "            print(\"  Analyzing non-HTML tags...\")\n",
    "            non_html_analysis = self.non_html_processor.analyze_non_html_tags(result_df[self.content_columns])\n",
    "\n",
    "            if not non_html_analysis['Non_HTML_Tags_Summary'].empty:\n",
    "                print(f\"  Found {len(non_html_analysis['Non_HTML_Tags_by_Row'])} non-HTML tag instances\")\n",
    "                print(f\"  Found {len(non_html_analysis['Non_HTML_Tags_Summary'])} unique non-HTML tags\")\n",
    "\n",
    "                # Encode brackets in the dataframe\n",
    "                result_df[self.content_columns] = self.non_html_processor.encode_brackets(\n",
    "                    result_df[self.content_columns],\n",
    "                    non_html_analysis\n",
    "                )\n",
    "\n",
    "                analysis['non_html_encoded'] = len(non_html_analysis['Non_HTML_Tags_by_Row'])\n",
    "                analysis['non_html_encoding'] = non_html_analysis\n",
    "\n",
    "                # Prepare report entries\n",
    "                non_html_rows = []\n",
    "                non_html_df = non_html_analysis['Non_HTML_Tags_by_Row']\n",
    "                for _, row in non_html_df.iterrows():\n",
    "                    non_html_rows.append({\n",
    "                        \"Row\": row['Row'],\n",
    "                        \"Column\": row['Column'],\n",
    "                        \"Issue\": row['Non_HTML_Tag'],\n",
    "                        \"Issue_Type\": \"Non-HTML Tag\"\n",
    "                    })\n",
    "                if non_html_rows:\n",
    "                    reports.append(pd.DataFrame(non_html_rows))\n",
    "            else:\n",
    "                print(\"  No non-HTML tags found\")\n",
    "\n",
    "        # -------------------------------\n",
    "        # STEP 3: Link validation\n",
    "        # -------------------------------\n",
    "        if validate_links:\n",
    "            print(\"  Analyzing link issues...\")\n",
    "            link_analysis = self.link_validator.analyze_link_issues(result_df[self.content_columns])\n",
    "            \n",
    "            if link_analysis['Total_Issues'] > 0:\n",
    "                print(f\"  Found {link_analysis['Total_Issues']} link issues\")\n",
    "                print(f\"  Repairable issues: {link_analysis['Repairable_Issues']}\")\n",
    "                \n",
    "                # Repair what we can\n",
    "                if link_analysis['Repairable_Issues'] > 0:\n",
    "                    print(\"  Repairing link issues...\")\n",
    "                    result_df[self.content_columns] = self.link_validator.repair_a_tags(\n",
    "                        result_df[self.content_columns]\n",
    "                    )\n",
    "                \n",
    "                # Add remaining issues to report\n",
    "                if not link_analysis['Link_Issues_by_Row'].empty:\n",
    "                    link_report = link_analysis['Link_Issues_by_Row'].copy()\n",
    "                    link_report[\"Issue_Type\"] = \"Link Issue\"\n",
    "                    reports.append(link_report)\n",
    "                    \n",
    "                analysis['link_repairs'] = link_analysis['Repairable_Issues']\n",
    "                analysis['link_analysis'] = link_analysis\n",
    "            else:\n",
    "                print(\"  No link issues found\")\n",
    "\n",
    "        # -------------------------------\n",
    "        # STEP 4: Unified mismatch report\n",
    "        # -------------------------------\n",
    "        if reports:\n",
    "            unified_report = pd.concat(reports, ignore_index=True)\n",
    "        else:\n",
    "            unified_report = pd.DataFrame(columns=[\"Row\", \"Column\", \"Issue\", \"Issue_Type\"])\n",
    "\n",
    "        report['analysis'] = analysis\n",
    "        report[\"mismatch_report\"] = unified_report\n",
    "\n",
    "        self._print_summary(analysis)\n",
    "        return result_df, report\n",
    "\n",
    "    def _print_summary(self, analysis: Dict):\n",
    "        \"\"\"Print processing summary\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"PROCESSING COMPLETE\")\n",
    "        print(\"=\" * 70)\n",
    "        print(f\"Repairs made:        {analysis['repairs_made']:,}\")\n",
    "        print(f\"Citations extracted: {analysis['citations_extracted']:,}\")\n",
    "        print(f\"Cross-refs extracted: {analysis['crossrefs_extracted']:,}\")\n",
    "        print(f\"Non-HTML encoded:    {analysis['non_html_encoded']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dacf8c",
   "metadata": {},
   "source": [
    "## Main Processor Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aabd3624",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NahuatlProcessor:\n",
    "    \"\"\"Main processor class that coordinates all operations\"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = DB_PATH, working_dir: str = WORKING_DIR):\n",
    "        \"\"\"Initialize the processor with all necessary components\"\"\"\n",
    "        \n",
    "        print(\"=\" * 70)\n",
    "        print(\"NAHUATL DATA PROCESSOR - HYBRID APPROACH\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Initialize components\n",
    "        self.html_processor = HTMLProcessor()\n",
    "        self.loader = DataLoader()\n",
    "        self.saver = DataSaver(working_dir)\n",
    "        self.db = DatabaseManager(db_path)\n",
    "        \n",
    "        # Data containers\n",
    "        self.original_df = None\n",
    "        self.working_df = None\n",
    "        \n",
    "        # Processing state\n",
    "        self.current_stage = 'initialized'\n",
    "        self.processing_history = []\n",
    "        \n",
    "        # Verify base dataset exists\n",
    "        if not self.db.verify_base_dataset():\n",
    "            print(\"\\n No base dataset found in SQLite\")\n",
    "        print(\"   Run processor.initial_import() to import your CSV data first\")\n",
    "        print(\"Processor initialized and ready\")\n",
    "        print(\"=\" * 70)\n",
    "    \n",
    "    def initial_import(self, csv_path: str, table_name: str = 'WHP_EarlyNahuatl_Data', \n",
    "                      encoding: str = DEFAULT_ENCODING, replace: bool = False):\n",
    "        \"\"\"\n",
    "        Perform initial import of CSV data into SQLite.\n",
    "        This establishes the base dataset in the database.\n",
    "        \"\"\"\n",
    "        \n",
    "        success = self.db.import_initial_dataset(\n",
    "            csv_path=csv_path,\n",
    "            table_name=table_name,\n",
    "            encoding=encoding,\n",
    "            replace=replace\n",
    "        )\n",
    "        \n",
    "        if success:\n",
    "            print(\"\\nInitial import complete!\")\n",
    "            print(\"   You can now proceed with data processing\")\n",
    "        else:\n",
    "            print(\"\\nInitial import failed\")\n",
    "            \n",
    "        return success\n",
    "    \n",
    "    def load_data(self, source: str, source_type: str = 'auto', **kwargs) -> pd.DataFrame:\n",
    "        \"\"\"Load data from any source\"\"\"\n",
    "        \n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        # Auto-detect source type if needed\n",
    "        if source_type == 'auto':\n",
    "            source_type = 'csv' if source.endswith('.csv') else 'sqlite'\n",
    "        \n",
    "        # Load based on source type\n",
    "        if source_type == 'csv':\n",
    "            df = self.loader.load_from_csv(source, **kwargs)\n",
    "        elif source_type == 'sqlite':\n",
    "            table_name = kwargs.get('table_name', 'WHP_EarlyNahuatl_Data')\n",
    "            df = self.loader.load_from_sqlite(source, table_name)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown source type: {source_type}\")\n",
    "        \n",
    "        # Create working copies\n",
    "        self.original_df, self.working_df = self.loader.create_working_copy(df)\n",
    "        \n",
    "        # Save initial checkpoint\n",
    "        self.save_checkpoint('initial')\n",
    "        \n",
    "        # Log the operation\n",
    "        duration = (datetime.now() - start_time).total_seconds()\n",
    "        self.db.log_processing_stage(\n",
    "            stage='data_loading',\n",
    "            status='completed',\n",
    "            duration=duration,\n",
    "            rows_processed=len(df),\n",
    "            notes=f\"Loaded from {source_type}: {source}\"\n",
    "        )\n",
    "        \n",
    "        self.current_stage = 'data_loaded'\n",
    "        \n",
    "        return self.working_df\n",
    "    \n",
    "    def save_checkpoint(\n",
    "        self,\n",
    "        checkpoint_name: str,\n",
    "        additional_data: Optional[Dict[str, pd.DataFrame]] = None\n",
    "    ):\n",
    "        \"\"\"Enhanced checkpoint saving with additional reports\"\"\"\n",
    "        if self.working_df is not None and self.db.conn is not None:\n",
    "            # Save main checkpoint\n",
    "            self.saver.save_checkpoint_to_sqlite(\n",
    "                self.working_df,\n",
    "                checkpoint_name,\n",
    "                self.db.conn\n",
    "            )\n",
    "            \n",
    "            # Save additional data if provided (like mismatch reports)\n",
    "            if additional_data:\n",
    "                for data_name, data_df in additional_data.items():\n",
    "                    if isinstance(data_df, pd.DataFrame) and not data_df.empty:\n",
    "                        table_name = f'{data_name}_{checkpoint_name}_{datetime.now().strftime(\"%Y%m%d\")}'\n",
    "                        data_df.to_sql(\n",
    "                            table_name,\n",
    "                            self.db.conn,\n",
    "                            if_exists='replace',\n",
    "                            index=False\n",
    "                        )\n",
    "                        print(f\"  Saved {data_name} to {table_name}\")\n",
    "            \n",
    "            self.processing_history.append({\n",
    "                'checkpoint': checkpoint_name,\n",
    "                'timestamp': datetime.now(),\n",
    "                'rows': len(self.working_df),\n",
    "                'columns': len(self.working_df.columns)\n",
    "            })\n",
    "        else:\n",
    "            print(f\"Warning: Cannot save checkpoint '{checkpoint_name}' - no data or connection available\")\n",
    "    \n",
    "    def export_final_results(self, filename_base: str = 'final_nahuatl_data'):\n",
    "        \"\"\"Export final results to CSV with encoding protection\"\"\"\n",
    "        \n",
    "        if self.working_df is None:\n",
    "            print(\"Error: No working data to export\")\n",
    "            return\n",
    "        \n",
    "        if self.original_df is None:\n",
    "            print(\"Warning: No original data for comparison\")\n",
    "            original_rows = 0\n",
    "            original_cols = 0\n",
    "        else:\n",
    "            original_rows = len(self.original_df)\n",
    "            original_cols = len(self.original_df.columns)\n",
    "        \n",
    "        # Save to CSV\n",
    "        csv_path = self.saver.save_to_csv(self.working_df, f\"{filename_base}.csv\")\n",
    "        \n",
    "        # Save final checkpoint\n",
    "        self.save_checkpoint('final')\n",
    "        \n",
    "        # Create summary report\n",
    "        summary = {\n",
    "            'processing_summary': pd.DataFrame(self.processing_history),\n",
    "            'data_info': pd.DataFrame([{\n",
    "                'original_rows': original_rows,\n",
    "                'final_rows': len(self.working_df),\n",
    "                'original_columns': original_cols,\n",
    "                'final_columns': len(self.working_df.columns),\n",
    "                'stages_completed': len(self.processing_history)\n",
    "            }])\n",
    "        }\n",
    "        \n",
    "        # Save summary to Excel\n",
    "        excel_path = self.saver.save_to_excel(\n",
    "            summary,\n",
    "            f\"{filename_base}_summary_{datetime.now().strftime('%Y%m%d')}.xlsx\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"EXPORT COMPLETE\")\n",
    "        print(f\"CSV: {csv_path}\")\n",
    "        print(f\"Summary: {excel_path}\")\n",
    "        print(\"=\" * 70)\n",
    "    \n",
    "    def load_data_from_base(self, table_name: str = 'WHP_EarlyNahuatl_Data') -> pd.DataFrame:\n",
    "        \"\"\"Convenience method to load from the base SQLite dataset\"\"\"\n",
    "        \n",
    "        if not self.db.verify_base_dataset(table_name):\n",
    "            raise ValueError(f\"Base dataset '{table_name}' not found. Run initial_import() first.\")\n",
    "        \n",
    "        return self.load_data(\n",
    "            source=self.db.db_path,\n",
    "            source_type='sqlite',\n",
    "            table_name=table_name\n",
    "        )\n",
    "        \n",
    "    def process_html_tags(self, save_checkpoint: bool = True):\n",
    "        \"\"\"Step 4-5: HTML tag processing\"\"\"\n",
    "        if self.working_df is None:\n",
    "            raise ValueError(\"No data loaded. Run load_data() first.\")\n",
    "        \n",
    "        # Delegate to HTML processor\n",
    "        self.working_df, report = self.html_processor.process_dataframe(\n",
    "            self.working_df,\n",
    "            repair_malformed=True,\n",
    "            encode_non_html=True,\n",
    "            validate_links=True,\n",
    "        )\n",
    "        \n",
    "        if save_checkpoint:\n",
    "            additional_data = {}\n",
    "            \n",
    "            # The HTMLProcessor already creates a unified mismatch_report\n",
    "            # that includes ALL issues (malformed tags, non-HTML tags, link issues)\n",
    "            # Each row already has an 'Issue_Type' column\n",
    "            if 'mismatch_report' in report and not report['mismatch_report'].empty:\n",
    "                additional_data['mismatch_report'] = report['mismatch_report']\n",
    "                \n",
    "                # Print summary by issue type\n",
    "                print(\"\\nIssue Summary:\")\n",
    "                issue_counts = report['mismatch_report']['Issue_Type'].value_counts()\n",
    "                for issue_type, count in issue_counts.items():\n",
    "                    print(f\"  - {issue_type}: {count} issues\")\n",
    "                    \n",
    "                print(f\"\\nTotal issues found: {len(report['mismatch_report'])}\")\n",
    "            else:\n",
    "                print(\"\\nNo issues found - data is clean!\")\n",
    "            \n",
    "            # Save checkpoint with the unified report\n",
    "            self.save_checkpoint('after_html_processing', additional_data)\n",
    "            \n",
    "        return report    \n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        self.db.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3519d6",
   "metadata": {},
   "source": [
    "At this point we've done enough thorough analysis and for the sake of correctness, manual correction will be done on the checkpoint table in the above log. After the corrections are made we will begin extracting citations and cross references.\n",
    "Note: only after this stage since we did some manual cleaning up use this db:\n",
    "checkpoint_final_20250914.db\n",
    "\n",
    "now that we've extracted citations and cross references will be uploading the working version to be the checkpoint_after_citation_and_crossref_extraction_20250922 table.\n",
    "\n",
    "this table will include the manual changes from the checkpoint_final_20250914 table.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b08bf1a",
   "metadata": {},
   "source": [
    "## Step 7: Citation Extractor (Extract and Remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0b048aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CitationExtractor:\n",
    "    \"\"\"Extract and remove <bibl> citations from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Pattern for bibl tags with any attributes\n",
    "        self.bibl_pattern = re.compile(\n",
    "            r'<bibl[^>]*>(.*?)</bibl>',\n",
    "            re.DOTALL | re.IGNORECASE\n",
    "        )\n",
    "        \n",
    "    def extract_and_remove_citations(self, text: str) -> Tuple[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Extract citations from text and return cleaned text WITHOUT citations.\n",
    "        Returns: (cleaned_text, list_of_citations)\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return text, []\n",
    "        \n",
    "        text_str = str(text)\n",
    "        citations = []\n",
    "        \n",
    "        # Find all bibl tags and their content\n",
    "        matches = list(self.bibl_pattern.finditer(text_str))\n",
    "        \n",
    "        # Process in reverse order to preserve positions\n",
    "        for match in reversed(matches):\n",
    "            # Extract the ENTIRE bibl block (including the tags)\n",
    "            full_citation = match.group(0)  # Changed from group(1) to group(0)\n",
    "            \n",
    "            # Clean up the citation content\n",
    "            cleaned_citation = self._clean_citation_content(full_citation)\n",
    "            \n",
    "            if cleaned_citation:\n",
    "                citations.insert(0, cleaned_citation)  # Insert at beginning to maintain order\n",
    "                \n",
    "            # Remove the entire bibl tag from original text\n",
    "            start, end = match.span()\n",
    "            text_str = text_str[:start] + text_str[end:]\n",
    "        \n",
    "        # Clean up any extra whitespace left behind\n",
    "        text_str = re.sub(r'\\s+', ' ', text_str).strip()\n",
    "        \n",
    "        return text_str, citations\n",
    "    \n",
    "    def _clean_citation_content(self, content: str) -> str:\n",
    "        \"\"\"Clean up extracted citation content\"\"\"\n",
    "        if not content:\n",
    "            return \"\"\n",
    "        # normalize whitespace\n",
    "        cleaned = content.strip()    \n",
    "        # preserves content within tags while normalizing spaces between\n",
    "        cleaned = re.sub(r'>\\s+<', '><', cleaned)\n",
    "        cleaned = re.sub(r'\\s+',' ',cleaned)\n",
    "        cleaned = cleaned.strip()\n",
    "        if len(cleaned) > 0:\n",
    "            return cleaned\n",
    "        return \"\"\n",
    "    \n",
    "    def process_dataframe(self, df: pd.DataFrame, content_columns: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process dataframe to extract and remove citations.\n",
    "        Creates new citation columns and cleans original columns.\n",
    "        \"\"\"\n",
    "        print(\"  Extracting and removing citations...\")\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Initialize new columns\n",
    "        if 'Citations' not in df_processed.columns:\n",
    "            df_processed['Citations'] = ''\n",
    "        if 'Number_of_Citations' not in df_processed.columns:\n",
    "            df_processed['Number_of_Citations'] = 0\n",
    "            \n",
    "        # Ensure numeric column is proper integer type\n",
    "        df_processed['Number_of_Citations'] = df_processed['Number_of_Citations'].astype(int)\n",
    "        \n",
    "        total_citations_found = 0\n",
    "        rows_with_citations = 0\n",
    "        \n",
    "        for idx, row in df_processed.iterrows():\n",
    "            if idx % 5000 == 0:\n",
    "                print(f\"    Processing row {idx}...\")\n",
    "            \n",
    "            all_citations = []\n",
    "            \n",
    "            # Process each content column\n",
    "            for col in content_columns:\n",
    "                if col not in df_processed.columns:\n",
    "                    continue\n",
    "                    \n",
    "                cell_value = row[col]\n",
    "                if pd.notna(cell_value) and str(cell_value).strip():\n",
    "                    # Extract and remove citations\n",
    "                    cleaned_text, citations = self.extract_and_remove_citations(cell_value)\n",
    "                    \n",
    "                    # Update the cell with cleaned text\n",
    "                    df_processed.at[idx, col] = cleaned_text\n",
    "                    \n",
    "                    # Collect citations\n",
    "                    all_citations.extend(citations)\n",
    "            \n",
    "            # Store citations in new columns\n",
    "            if all_citations:\n",
    "                df_processed.at[idx, 'Citations'] = ' | '.join(all_citations)\n",
    "                df_processed.at[idx, 'Number_of_Citations'] = len(all_citations)\n",
    "                total_citations_found += len(all_citations)\n",
    "                rows_with_citations += 1\n",
    "        \n",
    "        print(f\"    ✓ Extracted {total_citations_found} citations from {rows_with_citations} rows\")\n",
    "        print(f\"    ✓ Removed <bibl> tags from original columns\")\n",
    "        print(f\"    ✓ Preserved HTML structure within citations\")\n",
    "        \n",
    "        return df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92c0d29",
   "metadata": {},
   "source": [
    "## Step 8: Cross-Reference Extractor (Extract and Remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23ab7a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossReferenceExtractor:\n",
    "    \"\"\"Extract and remove cross-references from text while preserving HTML structure\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Define cross-reference patterns to extract and remove\n",
    "        # ORDER MATTERS: More specific patterns first to avoid duplicates\n",
    "        self.crossref_patterns = [\n",
    "            # Scholar references FIRST (most specific)\n",
    "            (r'\\(see\\s+Molina[^)]*\\)', 'molina_ref'),\n",
    "            (r'\\(see\\s+Karttunen[^)]*\\)', 'karttunen_ref'),\n",
    "            (r'\\(see\\s+Carochi[^)]*\\)', 'carochi_ref'),\n",
    "            (r'\\(see\\s+Lockhart[^)]*\\)', 'lockhart_ref'),\n",
    "            \n",
    "            # Then other parenthetical references\n",
    "            (r'\\(see\\s+([^)]+)\\)', 'see'),\n",
    "            (r'\\(cf\\.\\s+([^)]+)\\)', 'cf'),\n",
    "            (r'\\(compare\\s+([^)]+)\\)', 'compare'),\n",
    "            (r'\\(also\\s+see\\s+([^)]+)\\)', 'also_see'),\n",
    "            (r'\\(see\\s+also\\s+([^)]+)\\)', 'see_also'),\n",
    "            (r'\\(variant\\s+of\\s+([^)]+)\\)', 'variant'),\n",
    "            (r'\\(also\\s+([^)]+)\\)', 'also'),\n",
    "            (r'\\(See\\s+([^)]+)\\)', 'see'),  # Capital See\n",
    "            \n",
    "            # Bracketed references\n",
    "            (r'\\[see\\s+([^\\]]+)\\]', 'see'),\n",
    "            (r'\\[cf\\.\\s+([^\\]]+)\\]', 'cf'),\n",
    "        ]\n",
    "        \n",
    "        # Compile patterns for efficiency\n",
    "        self.compiled_patterns = [\n",
    "            (re.compile(pattern, re.IGNORECASE), ref_type) \n",
    "            for pattern, ref_type in self.crossref_patterns\n",
    "        ]\n",
    "    \n",
    "    def extract_and_remove_crossrefs(self, text: str) -> Tuple[str, List[Dict[str, str]]]:\n",
    "        \"\"\"\n",
    "        Extract cross-references and return cleaned text WITHOUT them.\n",
    "        PRESERVES HTML tags in the remaining text.\n",
    "        Returns: (cleaned_text, list_of_crossref_dicts)\n",
    "        \"\"\"\n",
    "        if pd.isna(text) or text == '':\n",
    "            return text, []\n",
    "        \n",
    "        text_str = str(text)\n",
    "        crossrefs = []\n",
    "        \n",
    "        # Track positions we've already processed to avoid duplicates\n",
    "        processed_ranges = []\n",
    "        \n",
    "        # Track all matches with their positions\n",
    "        all_matches = []\n",
    "        \n",
    "        for pattern, ref_type in self.compiled_patterns:\n",
    "            for match in pattern.finditer(text_str):\n",
    "                start, end = match.span()\n",
    "                \n",
    "                # Check if this position overlaps with already processed text\n",
    "                overlap = False\n",
    "                for proc_start, proc_end in processed_ranges:\n",
    "                    if (start >= proc_start and start < proc_end) or \\\n",
    "                       (end > proc_start and end <= proc_end):\n",
    "                        overlap = True\n",
    "                        break\n",
    "                \n",
    "                if overlap:\n",
    "                    continue  # Skip this match, it's already been handled\n",
    "                \n",
    "                # Handle both patterns with and without capture groups\n",
    "                if match.groups():  # Has capture groups\n",
    "                    content = match.group(1)\n",
    "                else:  # No capture groups - use the whole match\n",
    "                    content = match.group(0)\n",
    "                \n",
    "                all_matches.append({\n",
    "                    'match': match,\n",
    "                    'type': ref_type,\n",
    "                    'full_text': match.group(0),\n",
    "                    'content': content\n",
    "                })\n",
    "                \n",
    "                # Mark this range as processed\n",
    "                processed_ranges.append((start, end))\n",
    "        \n",
    "        # Sort by position (reverse order for safe removal)\n",
    "        all_matches.sort(key=lambda x: x['match'].start(), reverse=True)\n",
    "        \n",
    "        # Extract and remove each match\n",
    "        for match_info in all_matches:\n",
    "            match = match_info['match']\n",
    "            \n",
    "            # Clean the extracted content\n",
    "            if match_info['type'] in ['molina_ref', 'karttunen_ref', 'carochi_ref', 'lockhart_ref']:\n",
    "                # For scholar refs, clean the full match\n",
    "                cleaned_content = match_info['full_text'].strip('()[]')\n",
    "            else:\n",
    "                # For other refs, clean the captured group\n",
    "                cleaned_content = self._clean_crossref_content(match_info['content'])\n",
    "            \n",
    "            if cleaned_content:\n",
    "                crossrefs.insert(0, {\n",
    "                    'text': cleaned_content,\n",
    "                    'type': match_info['type']\n",
    "                })\n",
    "                \n",
    "            # Remove from original text\n",
    "            start, end = match.span()\n",
    "            \n",
    "            # Also remove a preceding space if there is one\n",
    "            if start > 0 and text_str[start-1] == ' ':\n",
    "                text_str = text_str[:start-1] + text_str[end:]\n",
    "            else:\n",
    "                text_str = text_str[:start] + text_str[end:]\n",
    "        \n",
    "        # Minimal cleanup - just normalize excessive whitespace\n",
    "        text_str = re.sub(r'  +', ' ', text_str)\n",
    "        \n",
    "        return text_str, crossrefs\n",
    "    \n",
    "    def _clean_crossref_content(self, content: str) -> str:\n",
    "        \"\"\"Clean up extracted cross-reference content\"\"\"\n",
    "        if not content:\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove any HTML tags from the cross-reference content\n",
    "        cleaned = re.sub(r'<[^>]+>', '', content)\n",
    "        \n",
    "        # Normalize whitespace\n",
    "        cleaned = ' '.join(cleaned.strip().split())\n",
    "        \n",
    "        # Remove trailing punctuation\n",
    "        cleaned = cleaned.strip('.,;:')\n",
    "        \n",
    "        # Must be substantial\n",
    "        if len(cleaned) > 1 and any(c.isalnum() for c in cleaned):\n",
    "            return cleaned.strip()\n",
    "        \n",
    "        return \"\"\n",
    "    \n",
    "    def process_dataframe(self, df: pd.DataFrame, content_columns: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process dataframe to extract and remove cross-references.\n",
    "        Creates new cross-reference columns and cleans original columns.\n",
    "        \"\"\"\n",
    "        print(\"  Extracting and removing cross-references...\")\n",
    "        print(\"  NOTE: Preserving HTML tag structure\")\n",
    "        df_processed = df.copy()\n",
    "        \n",
    "        # Initialize new columns if they don't exist\n",
    "        if 'Cross_References' not in df_processed.columns:\n",
    "            df_processed['Cross_References'] = ''\n",
    "        if 'Number_of_Cross_References' not in df_processed.columns:\n",
    "            df_processed['Number_of_Cross_References'] = 0\n",
    "        if 'CrossRef_Types' not in df_processed.columns:\n",
    "            df_processed['CrossRef_Types'] = ''\n",
    "        \n",
    "        # Ensure numeric column is proper type\n",
    "        df_processed['Number_of_Cross_References'] = pd.to_numeric(\n",
    "            df_processed['Number_of_Cross_References'], \n",
    "            errors='coerce'\n",
    "        ).fillna(0).astype(int)\n",
    "        \n",
    "        total_crossrefs_found = 0\n",
    "        rows_with_crossrefs = 0\n",
    "        \n",
    "        for idx, row in df_processed.iterrows():\n",
    "            if idx % 5000 == 0:\n",
    "                print(f\"    Processing row {idx}...\")\n",
    "            \n",
    "            all_crossrefs = []\n",
    "            all_types = set()\n",
    "            \n",
    "            # Process each content column\n",
    "            for col in content_columns:\n",
    "                if col not in df_processed.columns:\n",
    "                    continue\n",
    "                # Skip our newly added columns\n",
    "                if col in ['Citations', 'Number_of_Citations', 'Cross_References', \n",
    "                          'Number_of_Cross_References', 'CrossRef_Types']:\n",
    "                    continue\n",
    "                    \n",
    "                cell_value = row[col]\n",
    "                if pd.notna(cell_value) and str(cell_value).strip():\n",
    "                    try:\n",
    "                        # Extract and remove cross-references\n",
    "                        cleaned_text, crossrefs = self.extract_and_remove_crossrefs(cell_value)\n",
    "                        \n",
    "                        # Update the cell with cleaned text\n",
    "                        df_processed.at[idx, col] = cleaned_text\n",
    "                        \n",
    "                        # Collect cross-references\n",
    "                        for crossref in crossrefs:\n",
    "                            all_crossrefs.append(crossref['text'])\n",
    "                            all_types.add(crossref['type'])\n",
    "                    except Exception as e:\n",
    "                        print(f\"    Warning: Error processing row {idx}, column {col}: {e}\")\n",
    "                        continue\n",
    "            \n",
    "            # Store cross-references in new columns\n",
    "            if all_crossrefs:\n",
    "                df_processed.at[idx, 'Cross_References'] = ' | '.join(all_crossrefs)\n",
    "                df_processed.at[idx, 'Number_of_Cross_References'] = len(all_crossrefs)\n",
    "                df_processed.at[idx, 'CrossRef_Types'] = ', '.join(sorted(all_types))\n",
    "                total_crossrefs_found += len(all_crossrefs)\n",
    "                rows_with_crossrefs += 1\n",
    "        \n",
    "        print(f\"    ✓ Extracted {total_crossrefs_found} cross-references from {rows_with_crossrefs} rows\")\n",
    "        print(f\"    ✓ Removed cross-reference patterns from original columns\")\n",
    "        print(f\"    ✓ Preserved HTML structure in remaining content\")\n",
    "        \n",
    "        return df_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "524dd9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_citations_and_crossrefs(processor: NahuatlProcessor, \n",
    "                                    save_checkpoint: bool = True,\n",
    "                                    content_columns: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract citations and cross-references from the processor's working dataframe.\n",
    "    \n",
    "    Args:\n",
    "        processor: NahuatlProcessor instance with loaded data\n",
    "        save_checkpoint: Whether to save checkpoint after both extractions\n",
    "        content_columns: List of columns to process (uses default if None)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with extraction statistics\n",
    "    \"\"\"\n",
    "    if processor.working_df is None:\n",
    "        raise ValueError(\"No data loaded in processor. Run processor.load_data() first.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CITATION AND CROSS-REFERENCE EXTRACTION\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Use default content columns if not provided\n",
    "    if content_columns is None:\n",
    "        content_columns = [\n",
    "            'Principal English Translation',\n",
    "            'Attestations from sources in English',\n",
    "            'Attestations from sources in Spanish',\n",
    "            'Alonso de Molina',\n",
    "            'Frances Karttunen',\n",
    "            'Horacio Carochi / English',\n",
    "            'Andrés de Olmos',\n",
    "            \"Lockhart’s Nahuatl as Written\",\n",
    "\n",
    "        ]\n",
    "    \n",
    "    # Step 1: Extract and remove citations\n",
    "    print(\"\\nStep 1: Citation Extraction\")\n",
    "    print(\"-\" * 40)\n",
    "    citation_extractor = CitationExtractor()\n",
    "    processor.working_df = citation_extractor.process_dataframe(\n",
    "        processor.working_df,\n",
    "        content_columns\n",
    "    )\n",
    "    \n",
    "    # Step 2: Extract and remove cross-references\n",
    "    print(\"\\nStep 2: Cross-Reference Extraction\")\n",
    "    print(\"-\" * 40)\n",
    "    crossref_extractor = CrossReferenceExtractor()\n",
    "    processor.working_df = crossref_extractor.process_dataframe(\n",
    "        processor.working_df,\n",
    "        content_columns\n",
    "    )\n",
    "    \n",
    "    processor.working_df['Number_of_Citations'] = pd.to_numeric(\n",
    "        processor.working_df['Number_of_Citations'], errors='coerce'\n",
    "    ).fillna(0).astype(int)\n",
    "    \n",
    "    processor.working_df['Number_of_Cross_References'] = pd.to_numeric(\n",
    "        processor.working_df['Number_of_Cross_References'], errors='coerce'\n",
    "    ).fillna(0).astype(int)\n",
    "    \n",
    "    # Save single combined checkpoint after both extractions are complete\n",
    "    if save_checkpoint:\n",
    "        processor.save_checkpoint('after_citation_and_crossref_extraction')\n",
    "    \n",
    "    # Generate summary statistics\n",
    "    stats = {\n",
    "        'total_rows': len(processor.working_df),\n",
    "        'rows_with_citations': len(processor.working_df[processor.working_df['Number_of_Citations'] > 0]),\n",
    "        'total_citations': processor.working_df['Number_of_Citations'].sum(),\n",
    "        'max_citations_per_row': processor.working_df['Number_of_Citations'].max(),\n",
    "        'avg_citations_per_row': processor.working_df['Number_of_Citations'].mean(),\n",
    "        'rows_with_crossrefs': len(processor.working_df[processor.working_df['Number_of_Cross_References'] > 0]),\n",
    "        'total_crossrefs': processor.working_df['Number_of_Cross_References'].sum(),\n",
    "        'max_crossrefs_per_row': processor.working_df['Number_of_Cross_References'].max(),\n",
    "        'avg_crossrefs_per_row': processor.working_df['Number_of_Cross_References'].mean(),\n",
    "    }\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"EXTRACTION SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Total rows processed: {stats['total_rows']:,}\")\n",
    "    print(\"\\nCitations:\")\n",
    "    print(f\"  - Rows with citations: {stats['rows_with_citations']:,} ({stats['rows_with_citations']/stats['total_rows']*100:.1f}%)\")\n",
    "    print(f\"  - Total citations extracted: {stats['total_citations']:,}\")\n",
    "    print(f\"  - Average per row: {stats['avg_citations_per_row']:.2f}\")\n",
    "    print(f\"  - Maximum per row: {stats['max_citations_per_row']}\")\n",
    "    print(\"\\nCross-References:\")\n",
    "    print(f\"  - Rows with cross-references: {stats['rows_with_crossrefs']:,} ({stats['rows_with_crossrefs']/stats['total_rows']*100:.1f}%)\")\n",
    "    print(f\"  - Total cross-references extracted: {stats['total_crossrefs']:,}\")\n",
    "    print(f\"  - Average per row: {stats['avg_crossrefs_per_row']:.2f}\")\n",
    "    print(f\"  - Maximum per row: {stats['max_crossrefs_per_row']}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5200438f",
   "metadata": {},
   "source": [
    "extracted citations and cross references into this checkpoint: checkpoint_after_citation_and_crossref_extraction_2025092\n",
    "there was an issue regarding there being empty \"p\" tags in the Principal English Translation column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a84497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NAHUATL DATA PROCESSOR - HYBRID APPROACH\n",
      "======================================================================\n",
      "Database initialized: ../../data/sqLiteDb/nahuatl_processing.db\n",
      "Existing database objects: 10\n",
      "   - checkpoint_metadata (table): 77 rows\n",
      "   - sqlite_sequence (table): 2 rows\n",
      "   - processing_log (table): 39 rows\n",
      "   - WHP_EarlyNahuatl_Data (table): 31,806 rows\n",
      "   - import_history (table): 1 rows\n",
      "   - checkpoint_after_citation_and_crossref_extraction_20250922 (table): 31,806 rows\n",
      "   - checkpoint_initial_20250922 (table): 31,806 rows\n",
      "   - checkpoint_initial_20250929 (table): 31,806 rows\n",
      "   - checkpoint_removed_empty_p_tags_20250929 (table): 31,806 rows\n",
      "   - checkpoint_initial_20251002 (table): 31,806 rows\n",
      "   Base dataset verified:\n",
      "   Table: WHP_EarlyNahuatl_Data\n",
      "   Rows: 31,806\n",
      "   Columns: 13\n",
      "   Run processor.initial_import() to import your CSV data first\n",
      "Processor initialized and ready\n",
      "======================================================================\n",
      "Loading data from SQLite: ../../data/sqLiteDb/nahuatl_processing.db\n",
      "Table: checkpoint_after_citation_and_crossref_extraction_20250922\n",
      "Successfully loaded 31,806 rows × 18 columns\n",
      "Created working copy of data\n",
      "Checkpoint saved to SQLite: checkpoint_initial_20251002\n"
     ]
    }
   ],
   "source": [
    "def remove_empty_p_tags(text):\n",
    "    \"\"\"\n",
    "    Remove empty <p></p> tags from text, including those with only whitespace\n",
    "    \"\"\"\n",
    "    # Pattern to match <p></p> tags with optional whitespace inside and around\n",
    "    pattern = r'\\s*<p>\\s*</p>\\s*'\n",
    "    \n",
    "    # Replace all matches with empty string\n",
    "    cleaned_text = re.sub(pattern, '', text)\n",
    "    \n",
    "    # Clean up any multiple spaces that might be left\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# If you have a pandas DataFrame column: = \n",
    "processor = NahuatlProcessor()\n",
    "processor.load_data(\"../../data/sqLiteDb/nahuatl_processing.db\", source_type='sqlite', table_name='checkpoint_after_citation_and_crossref_extraction_20250922')\n",
    "\n",
    "# processor.working_df['Principal English Translation'] = processor.working_df['Principal English Translation'].apply(remove_empty_p_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d8adace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint saved to SQLite: checkpoint_removed_empty_p_tags_20251002\n"
     ]
    }
   ],
   "source": [
    "processor.saver.save_checkpoint_to_sqlite(processor.working_df, \"removed_empty_p_tags\", processor.db.conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dffcc2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "LOCKHART COLUMN CITATION EXTRACTION (補充)\n",
      "======================================================================\n",
      "✓ Found column: 'Lockhart’s Nahuatl as Written'\n",
      "  Processing row 0...\n",
      "  Processing row 5000...\n",
      "  Processing row 10000...\n",
      "  Processing row 15000...\n",
      "  Processing row 20000...\n",
      "  Processing row 25000...\n",
      "  Processing row 30000...\n",
      "\n",
      "======================================================================\n",
      "LOCKHART EXTRACTION SUMMARY\n",
      "======================================================================\n",
      "Column processed: 'Lockhart’s Nahuatl as Written'\n",
      "Rows updated: 907\n",
      "New citations extracted: 924\n",
      "Average citations per updated row: 1.02\n",
      "======================================================================\n",
      "Checkpoint saved to SQLite: checkpoint_after_lockhart_citation_extraction_20251002\n"
     ]
    }
   ],
   "source": [
    "def extract_lockhart_citations(processor: NahuatlProcessor) -> Dict:\n",
    "    \"\"\"\n",
    "    Extract citations from Lockhart's column and append to existing Citations.\n",
    "    Handles the apostrophe character issue and updates citation counts.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"LOCKHART COLUMN CITATION EXTRACTION (補充)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Find the actual column name with the special apostrophe\n",
    "    lockhart_col = None\n",
    "    for col in processor.working_df.columns:\n",
    "        if 'Lockhart' in col and 'Nahuatl' in col:\n",
    "            lockhart_col = col\n",
    "            break\n",
    "    \n",
    "    if lockhart_col is None:\n",
    "        print(\" Lockhart column not found!\")\n",
    "        return {'error': 'Column not found'}\n",
    "    \n",
    "    print(f\"✓ Found column: '{lockhart_col}'\")\n",
    "    \n",
    "    # Initialize the citation extractor\n",
    "    citation_extractor = CitationExtractor()\n",
    "    \n",
    "    # Track statistics\n",
    "    rows_updated = 0\n",
    "    new_citations_found = 0\n",
    "    \n",
    "    # Process each row\n",
    "    for idx, row in processor.working_df.iterrows():\n",
    "        if idx % 5000 == 0:\n",
    "            print(f\"  Processing row {idx}...\")\n",
    "        \n",
    "        lockhart_content = row[lockhart_col]\n",
    "        \n",
    "        # Skip empty cells\n",
    "        if pd.isna(lockhart_content) or not str(lockhart_content).strip():\n",
    "            continue\n",
    "        \n",
    "        # Extract citations from Lockhart column\n",
    "        cleaned_text, new_citations = citation_extractor.extract_and_remove_citations(lockhart_content)\n",
    "        \n",
    "        if new_citations:\n",
    "            # Update the Lockhart column with cleaned text\n",
    "            processor.working_df.at[idx, lockhart_col] = cleaned_text\n",
    "            \n",
    "            # Get existing citations\n",
    "            existing_citations = row.get('Citations', '')\n",
    "            existing_count = row.get('Number_of_Citations', 0)\n",
    "            \n",
    "            # Append new citations\n",
    "            if existing_citations and str(existing_citations).strip():\n",
    "                # Merge with existing citations using | delimiter\n",
    "                combined_citations = f\"{existing_citations} | {' | '.join(new_citations)}\"\n",
    "            else:\n",
    "                # No existing citations, just use new ones\n",
    "                combined_citations = ' | '.join(new_citations)\n",
    "            \n",
    "            # Update dataframe\n",
    "            processor.working_df.at[idx, 'Citations'] = combined_citations\n",
    "            processor.working_df.at[idx, 'Number_of_Citations'] = str(existing_count) + str(len(new_citations))\n",
    "            \n",
    "            rows_updated += 1\n",
    "            new_citations_found += len(new_citations)\n",
    "    \n",
    "    # Ensure citation count is integer type\n",
    "    processor.working_df['Number_of_Citations'] = pd.to_numeric(\n",
    "        processor.working_df['Number_of_Citations'], \n",
    "        errors='coerce'\n",
    "    ).fillna(0).astype(int)\n",
    "    \n",
    "    # Print summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"LOCKHART EXTRACTION SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Column processed: '{lockhart_col}'\")\n",
    "    print(f\"Rows updated: {rows_updated:,}\")\n",
    "    print(f\"New citations extracted: {new_citations_found:,}\")\n",
    "    print(f\"Average citations per updated row: {new_citations_found/rows_updated:.2f}\" if rows_updated > 0 else \"N/A\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return {\n",
    "        'column_name': lockhart_col,\n",
    "        'rows_updated': rows_updated,\n",
    "        'new_citations_found': new_citations_found,\n",
    "        'success': True\n",
    "    }\n",
    "\n",
    "\n",
    "# Usage:\n",
    "results = extract_lockhart_citations(processor)\n",
    "\n",
    "# Optional: Save checkpoint after extraction\n",
    "if results.get('success'):\n",
    "    processor.save_checkpoint('after_lockhart_citation_extraction')\n",
    "    \n",
    "    # Optionally save to CSV to verify\n",
    "    # processor.saver.save_to_csv(\n",
    "    #     processor.working_df, \n",
    "    #     \"lockhart_citations_updated\"\n",
    "    # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1cb4ab52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NAHUATL DATA PROCESSOR - HYBRID APPROACH\n",
      "======================================================================\n",
      "Database initialized: ../../data/sqLiteDb/nahuatl_processing.db\n",
      "Existing database objects: 12\n",
      "   - checkpoint_metadata (table): 80 rows\n",
      "   - sqlite_sequence (table): 2 rows\n",
      "   - processing_log (table): 40 rows\n",
      "   - WHP_EarlyNahuatl_Data (table): 31,806 rows\n",
      "   - import_history (table): 1 rows\n",
      "   - checkpoint_after_citation_and_crossref_extraction_20250922 (table): 31,806 rows\n",
      "   - checkpoint_initial_20250922 (table): 31,806 rows\n",
      "   - checkpoint_initial_20250929 (table): 31,806 rows\n",
      "   - checkpoint_removed_empty_p_tags_20250929 (table): 31,806 rows\n",
      "   - checkpoint_initial_20251002 (table): 31,806 rows\n",
      "   - checkpoint_removed_empty_p_tags_20251002 (table): 31,806 rows\n",
      "   - checkpoint_after_lockhart_citation_extraction_20251002 (table): 31,806 rows\n",
      "   Base dataset verified:\n",
      "   Table: WHP_EarlyNahuatl_Data\n",
      "   Rows: 31,806\n",
      "   Columns: 13\n",
      "   Run processor.initial_import() to import your CSV data first\n",
      "Processor initialized and ready\n",
      "======================================================================\n",
      "Loading data from SQLite: ../../data/sqLiteDb/nahuatl_processing.db\n",
      "Table: checkpoint_after_lockhart_citation_extraction_20251002\n",
      "Successfully loaded 31,806 rows × 18 columns\n",
      "Created working copy of data\n",
      "Checkpoint saved to SQLite: checkpoint_initial_20251002\n",
      "\n",
      "======================================================================\n",
      "REMOVING EMPTY <p></p> TAGS FROM CONTENT COLUMNS\n",
      "======================================================================\n",
      "Processing: 'Principal English Translation'\n",
      "  ✓ Modified 0 cells\n",
      "Processing: 'Attestations from sources in English'\n",
      "  ✓ Modified 32 cells\n",
      "Processing: 'Attestations from sources in Spanish'\n",
      "  ✓ Modified 3 cells\n",
      "Processing: 'Alonso de Molina'\n",
      "  ✓ Modified 1 cells\n",
      "Processing: 'Frances Karttunen'\n",
      "  ✓ Modified 4 cells\n",
      "Processing: 'Horacio Carochi / English'\n",
      "  ✓ Modified 3 cells\n",
      "Processing: 'Andrés de Olmos'\n",
      "  ✓ Modified 0 cells\n",
      "Processing: 'Lockhart’s Nahuatl as Written'\n",
      "  ✓ Modified 363 cells\n",
      "\n",
      "======================================================================\n",
      "CLEANING SUMMARY\n",
      "======================================================================\n",
      "Columns processed: 8/8\n",
      "Total cells modified: 406\n",
      "======================================================================\n",
      "Checkpoint saved to SQLite: checkpoint_after_empty_p_tag_removal_20251002\n",
      "Saved to CSV: working_files\\empty_p_tags_removed_20251002_142358.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'working_files\\\\empty_p_tags_removed_20251002_142358.csv'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_empty_p_tags(text):\n",
    "    \"\"\"\n",
    "    Remove empty <p></p> tags from text, including those with only whitespace\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return text\n",
    "    \n",
    "    # Pattern to match <p></p> tags with optional whitespace inside and around\n",
    "    pattern = r'\\s*<p>\\s*</p>\\s*'\n",
    "    \n",
    "    # Replace all matches with empty string\n",
    "    cleaned_text = re.sub(pattern, '', str(text))\n",
    "    \n",
    "    # Clean up any multiple spaces that might be left\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "\n",
    "def clean_empty_p_tags_from_columns(processor: NahuatlProcessor, \n",
    "                                     content_columns: Optional[List[str]] = None) -> Dict:\n",
    "    \"\"\"\n",
    "    Remove empty <p></p> tags from all content columns.\n",
    "    Handles missing columns and special characters gracefully.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"REMOVING EMPTY <p></p> TAGS FROM CONTENT COLUMNS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Default content columns if not provided\n",
    "    if content_columns is None:\n",
    "        content_columns = [\n",
    "            'Principal English Translation',\n",
    "            'Attestations from sources in English',\n",
    "            'Attestations from sources in Spanish',\n",
    "            'Alonso de Molina',\n",
    "            'Frances Karttunen',\n",
    "            'Horacio Carochi / English',\n",
    "            'Andrés de Olmos',\n",
    "            \"Lockhart’s Nahuatl as Written\",\n",
    "        ]\n",
    "    \n",
    "    # Track what we process\n",
    "    columns_processed = []\n",
    "    columns_not_found = []\n",
    "    total_changes = 0\n",
    "    \n",
    "    # Process each column\n",
    "    for target_col in content_columns:\n",
    "        # Find the actual column (handles apostrophe variations)\n",
    "        matched_col = None\n",
    "        \n",
    "        # Try exact match first\n",
    "        if target_col in processor.working_df.columns:\n",
    "            matched_col = target_col\n",
    "        else:\n",
    "            # Try fuzzy match (for apostrophe issues)\n",
    "            for col in processor.working_df.columns:\n",
    "                # Remove special characters for comparison\n",
    "                normalized_target = target_col.replace(\"'\", \"\").replace(\"'\", \"\").lower()\n",
    "                normalized_col = col.replace(\"'\", \"\").replace(\"'\", \"\").lower()\n",
    "                if normalized_target == normalized_col:\n",
    "                    matched_col = col\n",
    "                    break\n",
    "        \n",
    "        if matched_col:\n",
    "            print(f\"Processing: '{matched_col}'\")\n",
    "            \n",
    "            # Count changes\n",
    "            before = processor.working_df[matched_col].astype(str)\n",
    "            processor.working_df[matched_col] = processor.working_df[matched_col].apply(remove_empty_p_tags)\n",
    "            after = processor.working_df[matched_col].astype(str)\n",
    "            \n",
    "            changes = (before != after).sum()\n",
    "            total_changes += changes\n",
    "            \n",
    "            columns_processed.append(matched_col)\n",
    "            print(f\"  ✓ Modified {changes:,} cells\")\n",
    "        else:\n",
    "            columns_not_found.append(target_col)\n",
    "            print(f\"  ⚠ Column not found: '{target_col}'\")\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CLEANING SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Columns processed: {len(columns_processed)}/{len(content_columns)}\")\n",
    "    print(f\"Total cells modified: {total_changes:,}\")\n",
    "    \n",
    "    if columns_not_found:\n",
    "        print(f\"\\nColumns not found ({len(columns_not_found)}):\")\n",
    "        for col in columns_not_found:\n",
    "            print(f\"  - {col}\")\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return {\n",
    "        'columns_processed': columns_processed,\n",
    "        'columns_not_found': columns_not_found,\n",
    "        'total_changes': total_changes\n",
    "    }\n",
    "\n",
    "\n",
    "# Usage:\n",
    "processor = NahuatlProcessor()\n",
    "processor.load_data(\n",
    "    \"../../data/sqLiteDb/nahuatl_processing.db\", \n",
    "    source_type='sqlite', \n",
    "    table_name='checkpoint_after_lockhart_citation_extraction_20251002'\n",
    ")\n",
    "\n",
    "# Clean all content columns\n",
    "results = clean_empty_p_tags_from_columns(processor)\n",
    "\n",
    "# Save checkpoint\n",
    "processor.save_checkpoint('after_empty_p_tag_removal')\n",
    "\n",
    "# Optional: Save to CSV\n",
    "processor.saver.save_to_csv(processor.working_df, \"empty_p_tags_removed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95317fc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "NAHUATL DATA PROCESSOR - HYBRID APPROACH\n",
      "======================================================================\n",
      "Database initialized: ../../data/sqLiteDb/nahuatl_processing.db\n",
      "Existing database objects: 14\n",
      "   - checkpoint_metadata (table): 83 rows\n",
      "   - sqlite_sequence (table): 2 rows\n",
      "   - processing_log (table): 42 rows\n",
      "   - WHP_EarlyNahuatl_Data (table): 31,806 rows\n",
      "   - import_history (table): 1 rows\n",
      "   - checkpoint_after_citation_and_crossref_extraction_20250922 (table): 31,806 rows\n",
      "   - checkpoint_initial_20250922 (table): 31,806 rows\n",
      "   - checkpoint_initial_20250929 (table): 31,806 rows\n",
      "   - checkpoint_removed_empty_p_tags_20250929 (table): 31,806 rows\n",
      "   - checkpoint_removed_empty_p_tags_20251002 (table): 31,806 rows\n",
      "   - checkpoint_after_lockhart_citation_extraction_20251002 (table): 31,806 rows\n",
      "   - checkpoint_after_empty_p_tag_removal_20251002 (table): 31,806 rows\n",
      "   - checkpoint_initial_20251002 (table): 31,806 rows\n",
      "   - IDIEZ_modern_nahuatl-all-2024-03-27T09-45-31 (table): 6,846 rows\n",
      "   Base dataset verified:\n",
      "   Table: WHP_EarlyNahuatl_Data\n",
      "   Rows: 31,806\n",
      "   Columns: 13\n",
      "   Run processor.initial_import() to import your CSV data first\n",
      "Processor initialized and ready\n",
      "======================================================================\n",
      "Loading data from CSV: ../scripts/export_to_csv_prototype/data/nahuatl_batch_import.csv\n",
      "Encoding: utf-8-sig\n",
      "Successfully loaded 38,652 rows × 7 columns\n",
      "Columns: ['Headwords', 'Gloss', 'Language', 'Etyma', 'HomographNumber']...\n",
      "Created working copy of data\n",
      "Checkpoint saved to SQLite: checkpoint_initial_20251009\n",
      "\n",
      "======================================================================\n",
      "CREATING BATCHED EXPORTS\n",
      "======================================================================\n",
      "Total rows: 38,652\n",
      "Batch size: 1000\n",
      "Number of batches: 39\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "CREATING PLAN B FILES (Test on small batch first)\n",
      "----------------------------------------------------------------------\n",
      "✓ Created TEST file: nahuatl_batch_PLAN_B_TEST_10rows_20251009_114348.csv\n",
      "  Rows: 10\n",
      "✓ Created REMAINING file: nahuatl_batch_PLAN_B_REMAINING_990rows_20251009_114348.csv\n",
      "  Rows: 990\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "CREATING REGULAR BATCHES\n",
      "----------------------------------------------------------------------\n",
      "✓ Batch  1/39: nahuatl_batch_01_of_39_rows_1-1000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 1-1000)\n",
      "✓ Batch  2/39: nahuatl_batch_02_of_39_rows_1001-2000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 1001-2000)\n",
      "✓ Batch  3/39: nahuatl_batch_03_of_39_rows_2001-3000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 2001-3000)\n",
      "✓ Batch  4/39: nahuatl_batch_04_of_39_rows_3001-4000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 3001-4000)\n",
      "✓ Batch  5/39: nahuatl_batch_05_of_39_rows_4001-5000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 4001-5000)\n",
      "✓ Batch  6/39: nahuatl_batch_06_of_39_rows_5001-6000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 5001-6000)\n",
      "✓ Batch  7/39: nahuatl_batch_07_of_39_rows_6001-7000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 6001-7000)\n",
      "✓ Batch  8/39: nahuatl_batch_08_of_39_rows_7001-8000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 7001-8000)\n",
      "✓ Batch  9/39: nahuatl_batch_09_of_39_rows_8001-9000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 8001-9000)\n",
      "✓ Batch 10/39: nahuatl_batch_10_of_39_rows_9001-10000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 9001-10000)\n",
      "✓ Batch 11/39: nahuatl_batch_11_of_39_rows_10001-11000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 10001-11000)\n",
      "✓ Batch 12/39: nahuatl_batch_12_of_39_rows_11001-12000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 11001-12000)\n",
      "✓ Batch 13/39: nahuatl_batch_13_of_39_rows_12001-13000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 12001-13000)\n",
      "✓ Batch 14/39: nahuatl_batch_14_of_39_rows_13001-14000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 13001-14000)\n",
      "✓ Batch 15/39: nahuatl_batch_15_of_39_rows_14001-15000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 14001-15000)\n",
      "✓ Batch 16/39: nahuatl_batch_16_of_39_rows_15001-16000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 15001-16000)\n",
      "✓ Batch 17/39: nahuatl_batch_17_of_39_rows_16001-17000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 16001-17000)\n",
      "✓ Batch 18/39: nahuatl_batch_18_of_39_rows_17001-18000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 17001-18000)\n",
      "✓ Batch 19/39: nahuatl_batch_19_of_39_rows_18001-19000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 18001-19000)\n",
      "✓ Batch 20/39: nahuatl_batch_20_of_39_rows_19001-20000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 19001-20000)\n",
      "✓ Batch 21/39: nahuatl_batch_21_of_39_rows_20001-21000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 20001-21000)\n",
      "✓ Batch 22/39: nahuatl_batch_22_of_39_rows_21001-22000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 21001-22000)\n",
      "✓ Batch 23/39: nahuatl_batch_23_of_39_rows_22001-23000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 22001-23000)\n",
      "✓ Batch 24/39: nahuatl_batch_24_of_39_rows_23001-24000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 23001-24000)\n",
      "✓ Batch 25/39: nahuatl_batch_25_of_39_rows_24001-25000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 24001-25000)\n",
      "✓ Batch 26/39: nahuatl_batch_26_of_39_rows_25001-26000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 25001-26000)\n",
      "✓ Batch 27/39: nahuatl_batch_27_of_39_rows_26001-27000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 26001-27000)\n",
      "✓ Batch 28/39: nahuatl_batch_28_of_39_rows_27001-28000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 27001-28000)\n",
      "✓ Batch 29/39: nahuatl_batch_29_of_39_rows_28001-29000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 28001-29000)\n",
      "✓ Batch 30/39: nahuatl_batch_30_of_39_rows_29001-30000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 29001-30000)\n",
      "✓ Batch 31/39: nahuatl_batch_31_of_39_rows_30001-31000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 30001-31000)\n",
      "✓ Batch 32/39: nahuatl_batch_32_of_39_rows_31001-32000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 31001-32000)\n",
      "✓ Batch 33/39: nahuatl_batch_33_of_39_rows_32001-33000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 32001-33000)\n",
      "✓ Batch 34/39: nahuatl_batch_34_of_39_rows_33001-34000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 33001-34000)\n",
      "✓ Batch 35/39: nahuatl_batch_35_of_39_rows_34001-35000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 34001-35000)\n",
      "✓ Batch 36/39: nahuatl_batch_36_of_39_rows_35001-36000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 35001-36000)\n",
      "✓ Batch 37/39: nahuatl_batch_37_of_39_rows_36001-37000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 36001-37000)\n",
      "✓ Batch 38/39: nahuatl_batch_38_of_39_rows_37001-38000_20251009_114348.csv\n",
      "  Rows: 1000 (rows 37001-38000)\n",
      "✓ Batch 39/39: nahuatl_batch_39_of_39_rows_38001-38652_20251009_114348.csv\n",
      "  Rows: 652 (rows 38001-38652)\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "CREATING SUMMARY FILE\n",
      "----------------------------------------------------------------------\n",
      "✓ Created summary: nahuatl_batch_SUMMARY_20251009_114348.xlsx\n",
      "\n",
      "======================================================================\n",
      "BATCH EXPORT COMPLETE\n",
      "======================================================================\n",
      "Output directory: working_files\\batched_output\n",
      "\n",
      "Files created:\n",
      "  Plan B files: 2\n",
      "  Regular batches: 39\n",
      "  Summary file: 1\n",
      "\n",
      "Total data rows exported: 38,652\n",
      "\n",
      "RECOMMENDED PROCESSING ORDER:\n",
      "  1. Start with PLAN_B_TEST file (10 rows)\n",
      "  2. If successful, add PLAN_B_REMAINING file (990 rows)\n",
      "  3. If successful, process batches 02 through 39\n",
      "======================================================================\n",
      "\n",
      " FILE LOCATIONS:\n",
      "\n",
      "Plan B Files:\n",
      "  nahuatl_batch_PLAN_B_TEST_10rows_20251009_114348.csv\n",
      "  nahuatl_batch_PLAN_B_REMAINING_990rows_20251009_114348.csv\n",
      "\n",
      "Regular Batches:\n",
      "  nahuatl_batch_01_of_39_rows_1-1000_20251009_114348.csv\n",
      "  nahuatl_batch_02_of_39_rows_1001-2000_20251009_114348.csv\n",
      "  nahuatl_batch_03_of_39_rows_2001-3000_20251009_114348.csv\n",
      "  ... and 36 more batches\n",
      "\n",
      "Summary: nahuatl_batch_SUMMARY_20251009_114348.xlsx\n"
     ]
    }
   ],
   "source": [
    "def create_batched_exports(\n",
    "    processor: NahuatlProcessor,\n",
    "    batch_size: int = 1000,\n",
    "    create_plan_b: bool = True,\n",
    "    plan_b_test_size: int = 10,\n",
    "    output_prefix: str = \"nahuatl_batch\"\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Split dataset into batches and create Plan B test files.\n",
    "    \n",
    "    Args:\n",
    "        processor: NahuatlProcessor with loaded data\n",
    "        batch_size: Rows per batch (default 1000)\n",
    "        create_plan_b: Whether to create Plan B split of first batch\n",
    "        plan_b_test_size: Size of Plan B test file (default 10)\n",
    "        output_prefix: Prefix for output filenames\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with file paths and statistics\n",
    "    \"\"\"\n",
    "    if processor.working_df is None:\n",
    "        raise ValueError(\"No data loaded in processor\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CREATING BATCHED EXPORTS\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    df = processor.working_df\n",
    "    total_rows = len(df)\n",
    "    num_batches = (total_rows + batch_size - 1) // batch_size  # Ceiling division\n",
    "    \n",
    "    print(f\"Total rows: {total_rows:,}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Number of batches: {num_batches}\")\n",
    "    \n",
    "    # Prepare output directory\n",
    "    output_dir = Path(processor.saver.working_dir)\n",
    "    batch_dir = output_dir / \"batched_output\"\n",
    "    batch_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    created_files = {\n",
    "        'regular_batches': [],\n",
    "        'plan_b_files': [],\n",
    "        'summary_file': None\n",
    "    }\n",
    "    \n",
    "    # === PLAN B: First batch split ===\n",
    "    if create_plan_b:\n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(\"CREATING PLAN B FILES (Test on small batch first)\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        first_batch = df.iloc[:batch_size].copy()\n",
    "        \n",
    "        # Plan B Test file (first 10 rows)\n",
    "        test_df = first_batch.iloc[:plan_b_test_size]\n",
    "        test_filename = f\"{output_prefix}_PLAN_B_TEST_{plan_b_test_size}rows_{timestamp}.csv\"\n",
    "        test_path = batch_dir / test_filename\n",
    "        \n",
    "        test_df.to_csv(\n",
    "            test_path,\n",
    "            index=False,\n",
    "            encoding=processor.saver.encoding,\n",
    "            na_rep='',\n",
    "            quoting=csv.QUOTE_MINIMAL\n",
    "        )\n",
    "        created_files['plan_b_files'].append({\n",
    "            'path': str(test_path),\n",
    "            'rows': len(test_df),\n",
    "            'description': f'Plan B TEST - First {plan_b_test_size} rows'\n",
    "        })\n",
    "        print(f\"✓ Created TEST file: {test_filename}\")\n",
    "        print(f\"  Rows: {len(test_df)}\")\n",
    "        \n",
    "        # Plan B Remaining file (remaining 990 rows)\n",
    "        remaining_df = first_batch.iloc[plan_b_test_size:]\n",
    "        remaining_filename = f\"{output_prefix}_PLAN_B_REMAINING_{len(remaining_df)}rows_{timestamp}.csv\"\n",
    "        remaining_path = batch_dir / remaining_filename\n",
    "        \n",
    "        remaining_df.to_csv(\n",
    "            remaining_path,\n",
    "            index=False,\n",
    "            encoding=processor.saver.encoding,\n",
    "            na_rep='',\n",
    "            quoting=csv.QUOTE_MINIMAL\n",
    "        )\n",
    "        created_files['plan_b_files'].append({\n",
    "            'path': str(remaining_path),\n",
    "            'rows': len(remaining_df),\n",
    "            'description': f'Plan B REMAINING - Rows {plan_b_test_size + 1} to {batch_size}'\n",
    "        })\n",
    "        print(f\"✓ Created REMAINING file: {remaining_filename}\")\n",
    "        print(f\"  Rows: {len(remaining_df)}\")\n",
    "    \n",
    "    # === REGULAR BATCHES ===\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"CREATING REGULAR BATCHES\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    for batch_num in range(num_batches):\n",
    "        start_idx = batch_num * batch_size\n",
    "        end_idx = min(start_idx + batch_size, total_rows)\n",
    "        \n",
    "        batch_df = df.iloc[start_idx:end_idx].copy()\n",
    "        \n",
    "        # Create filename with batch info\n",
    "        filename = f\"{output_prefix}_{batch_num + 1:02d}_of_{num_batches:02d}_rows_{start_idx + 1}-{end_idx}_{timestamp}.csv\"\n",
    "        filepath = batch_dir / filename\n",
    "        \n",
    "        # Save batch\n",
    "        batch_df.to_csv(\n",
    "            filepath,\n",
    "            index=False,\n",
    "            encoding=processor.saver.encoding,\n",
    "            na_rep='',\n",
    "            quoting=csv.QUOTE_MINIMAL\n",
    "        )\n",
    "        \n",
    "        created_files['regular_batches'].append({\n",
    "            'batch_number': batch_num + 1,\n",
    "            'path': str(filepath),\n",
    "            'rows': len(batch_df),\n",
    "            'row_range': f\"{start_idx + 1}-{end_idx}\"\n",
    "        })\n",
    "        \n",
    "        print(f\"✓ Batch {batch_num + 1:2d}/{num_batches}: {filename}\")\n",
    "        print(f\"  Rows: {len(batch_df)} (rows {start_idx + 1}-{end_idx})\")\n",
    "    \n",
    "    # === CREATE SUMMARY FILE ===\n",
    "    print(\"\\n\" + \"-\" * 70)\n",
    "    print(\"CREATING SUMMARY FILE\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    summary_data = {\n",
    "        'Batch Overview': [],\n",
    "        'Plan B Files': [],\n",
    "        'Processing Instructions': []\n",
    "    }\n",
    "    \n",
    "    # Batch overview\n",
    "    for batch_info in created_files['regular_batches']:\n",
    "        summary_data['Batch Overview'].append({\n",
    "            'Batch': batch_info['batch_number'],\n",
    "            'Filename': Path(batch_info['path']).name,\n",
    "            'Rows': batch_info['rows'],\n",
    "            'Row Range': batch_info['row_range']\n",
    "        })\n",
    "    \n",
    "    # Plan B files\n",
    "    if create_plan_b:\n",
    "        for plan_b_info in created_files['plan_b_files']:\n",
    "            summary_data['Plan B Files'].append({\n",
    "                'Filename': Path(plan_b_info['path']).name,\n",
    "                'Rows': plan_b_info['rows'],\n",
    "                'Description': plan_b_info['description']\n",
    "            })\n",
    "    \n",
    "    # Instructions\n",
    "    summary_data['Processing Instructions'].append({\n",
    "        'Step': 1,\n",
    "        'Action': f'Test with Plan B TEST file ({plan_b_test_size} rows)',\n",
    "        'File': Path(created_files['plan_b_files'][0]['path']).name if create_plan_b else 'N/A'\n",
    "    })\n",
    "    summary_data['Processing Instructions'].append({\n",
    "        'Step': 2,\n",
    "        'Action': 'If successful, add Plan B REMAINING file (990 rows)',\n",
    "        'File': Path(created_files['plan_b_files'][1]['path']).name if create_plan_b else 'N/A'\n",
    "    })\n",
    "    summary_data['Processing Instructions'].append({\n",
    "        'Step': 3,\n",
    "        'Action': 'If successful, process remaining batches in order',\n",
    "        'File': 'Batches 02 through ' + f\"{num_batches:02d}\"\n",
    "    })\n",
    "    \n",
    "    # Save summary to Excel\n",
    "    summary_filename = f\"{output_prefix}_SUMMARY_{timestamp}.xlsx\"\n",
    "    summary_path = batch_dir / summary_filename\n",
    "    \n",
    "    with pd.ExcelWriter(summary_path, engine='openpyxl') as writer:\n",
    "        for sheet_name, data in summary_data.items():\n",
    "            pd.DataFrame(data).to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    \n",
    "    created_files['summary_file'] = str(summary_path)\n",
    "    print(f\"✓ Created summary: {summary_filename}\")\n",
    "    \n",
    "    # === FINAL SUMMARY ===\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"BATCH EXPORT COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Output directory: {batch_dir}\")\n",
    "    print(f\"\\nFiles created:\")\n",
    "    if create_plan_b:\n",
    "        print(f\"  Plan B files: {len(created_files['plan_b_files'])}\")\n",
    "    print(f\"  Regular batches: {len(created_files['regular_batches'])}\")\n",
    "    print(f\"  Summary file: 1\")\n",
    "    print(f\"\\nTotal data rows exported: {total_rows:,}\")\n",
    "    print(\"\\nRECOMMENDED PROCESSING ORDER:\")\n",
    "    print(\"  1. Start with PLAN_B_TEST file (10 rows)\")\n",
    "    print(\"  2. If successful, add PLAN_B_REMAINING file (990 rows)\")\n",
    "    print(\"  3. If successful, process batches 02 through \" + f\"{num_batches:02d}\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    return created_files\n",
    "\n",
    "\n",
    "# Usage:\n",
    "processor = NahuatlProcessor()\n",
    "\n",
    "# Load the combined CSV we just created\n",
    "processor.load_data(\n",
    "    \"../scripts/export_to_csv_prototype/data/nahuatl_batch_import.csv\",  # The file we just created\n",
    "    source_type='csv'\n",
    ")\n",
    "\n",
    "# Create batched files with Plan B\n",
    "files = create_batched_exports(\n",
    "    processor,\n",
    "    batch_size=1000,\n",
    "    create_plan_b=True,\n",
    "    plan_b_test_size=10,\n",
    "    output_prefix=\"nahuatl_batch\"\n",
    ")\n",
    "\n",
    "# Print file locations for easy access\n",
    "print(\"\\n FILE LOCATIONS:\")\n",
    "print(\"\\nPlan B Files:\")\n",
    "for file_info in files['plan_b_files']:\n",
    "    print(f\"  {Path(file_info['path']).name}\")\n",
    "\n",
    "print(\"\\nRegular Batches:\")\n",
    "for file_info in files['regular_batches'][:3]:  # Show first 3\n",
    "    print(f\"  {Path(file_info['path']).name}\")\n",
    "if len(files['regular_batches']) > 3:\n",
    "    print(f\"  ... and {len(files['regular_batches']) - 3} more batches\")\n",
    "\n",
    "print(f\"\\nSummary: {Path(files['summary_file']).name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8518af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nahuaLEX_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
