import sqlite3
import pandas as pd
import json
import re
from typing import List, Dict, Optional

class NahuatlCSVExporter:
    """Transform normalized database to batch import CSV format"""
    
    def __init__(self, db_path: str):
        self.conn = sqlite3.connect(db_path)
        
        # Reference table abbreviations
        self.authority_mapping = {
            'Molina': 'VLM',
            'Karttunen': 'AND',
            'Carochi': 'GML',
            'Olmos': 'ALM',
            'Lockhart': 'NWN'
        }
    
    def extract_page_number(self, citation: str) -> str:
        """Extract page number from citation using regex"""
        if not citation:
            return ""
        
        patterns = [
            r'f\.\s*(\d+[rv]?)',           # f. 5r
            r'p\.\s*(\d+)',                # p. 123
            r',\s*(\d+[rv]?)\.',           # , 6.
            r',\s*(\d+–\d+)',              # , 214–215
            r'(\d+–\d+)\.',                # 242–243.
            r'vol\.\s*\d+,\s*(\d+–\d+)',   # vol. 2, 214–215
        ]
        
        for pattern in patterns:
            match = re.search(pattern, citation)
            if match:
                return match.group(1)
        
        return ""
    
    def match_citation_to_authority(self, citation: str) -> Optional[str]:
        """Match citation to authority abbreviation by author name"""
        citation_lower = citation.lower()
        
        for author_name, abbrev in self.authority_mapping.items():
            if author_name.lower() in citation_lower:
                return abbrev
        
        return None
    
    def parse_citations(self, citations_text: str) -> List[str]:
        """Parse pipe-separated citation tags"""
        if not citations_text or pd.isna(citations_text):
            return []
        
        # Split by pipe and clean
        citations = [c.strip() for c in citations_text.split('|')]
        return citations
    
    def build_whp_sources(self, row: pd.Series) -> List[Dict]:
        """Build Sources JSON for WHP/Classical Nahuatl entry"""
        sources = []
        
        # Parse citations
        citations = self.parse_citations(row['Citations'])
        
        # Authority columns to check
        authority_columns = {
            'Alonso de Molina': 'VLM',
            'Frances Karttunen': 'AND',
            'Horacio Carochi / English': 'GML',
            'Andrés de Olmos': 'ALM',
            "Lockhart’s Nahuatl as Written": 'NWN'
        }
        
        # Process each authority column
        for col_name, abbrev in authority_columns.items():
            authority_content = row.get(col_name)
            
            # Skip if None or empty
            if not authority_content or pd.isna(authority_content) or authority_content == 'None':
                continue
            
            # Find matching citations
            matching_citations = [
                cit for cit in citations 
                if self.match_citation_to_authority(cit) == abbrev
            ]
            
            # Create source entry for each matching citation
            for citation in matching_citations:
                page_number = self.extract_page_number(citation)
                
                sources.append({
                    "source": abbrev,
                    "page_number": page_number,
                    "original_entry": authority_content,
                    "bibliography": citation
                })
            
            # If no matching citation found but column has content, add without bibliography
            if not matching_citations:
                sources.append({
                    "source": abbrev,
                    "page_number": "",
                    "original_entry": authority_content,
                    "bibliography": ""
                })
        
        return sources
    
    def build_idiez_sources(self, row: pd.Series) -> List[Dict]:
        """Build Sources JSON for IDIEZ/Huasteca Nahuatl entry"""
        # Compile all IDIEZ fields
        fields = []
        
        if row.get('tlahtolli') and not pd.isna(row.get('tlahtolli')):
            fields.append(f"IDIEZ morfema: {row['tlahtolli']}")
        if row.get('IDIEZ traduc. inglés') and not pd.isna(row.get('IDIEZ traduc. inglés')):
            fields.append(f"IDIEZ traduc. inglés: {row['IDIEZ traduc. inglés']}")
        if row.get('IDIEZ def. náhuatl') and not pd.isna(row.get('IDIEZ def. náhuatl')):
            fields.append(f"IDIEZ def. náhuatl: {row['IDIEZ def. náhuatl']}")
        if row.get('IDIEZ def. español') and not pd.isna(row.get('IDIEZ def. español')):
            fields.append(f"IDIEZ def. español: {row['IDIEZ def. español']}")
        if row.get('IDIEZ morfología') and not pd.isna(row.get('IDIEZ morfología')):
            fields.append(f"IDIEZ morfología: {row['IDIEZ morfología']}")
        if row.get('IDIEZ gramática') and not pd.isna(row.get('IDIEZ gramática')):
            fields.append(f"IDIEZ gramática: {row['IDIEZ gramática']}")
        
        original_entry = ". ".join(fields) + "." if fields else ""
        
        return [{
            "source": "IDIEZ",
            "page_number": "",
            "original_entry": original_entry
        }]
    
    def detect_homographs(self, df: pd.DataFrame, headword_col: str) -> pd.Series:
        """Assign homograph numbers to duplicate headwords within same dataset"""
        # Group by headword and count
        headword_counts = df.groupby(headword_col).cumcount() + 1
        
        # Only assign numbers if there are duplicates
        duplicates = df[headword_col].duplicated(keep=False)
        homograph_numbers = headword_counts.where(duplicates, "")
        
        return homograph_numbers
    
    def process_whp_data(self) -> pd.DataFrame:
        """Process WHP data to CSV format"""
        
        print("Loading WHP data...")
        query = """
        SELECT 
            Ref,
            Headword,
            "Orthographic Variants",
            "Principal English Translation",
            "Attestations from sources in English",
            "Attestations from sources in Spanish",
            "Alonso de Molina",
            "Frances Karttunen",
            "Horacio Carochi / English",
            "Andrés de Olmos",
            "Lockhart’s Nahuatl as Written",
            themes,
            "Spanish Loanword",
            Citations,
            Number_of_Citations
        FROM "checkpoint_after_empty_p_tag_removal_20251002"
        """
        
        df = pd.read_sql(query, self.conn)
        
        print(f"Processing {len(df)} WHP entries...")
        
        export_rows = []
        
        for idx, row in df.iterrows():
            if idx % 1000 == 0:
                print(f"  Processing WHP entry {idx}...")
            
            # Build Headwords (keep period)
            headwords = row['Headword']
          
            # Build Gloss (keep HTML)
            gloss = row['Principal English Translation'] or ""
            
            # Build Sources JSON
            sources = self.build_whp_sources(row)
            sources_json = json.dumps(sources, ensure_ascii=False)
            
            # Build Extra Data
            extra_data = ""
            if row.get('themes') and not pd.isna(row['themes']):
                extra_data = json.dumps({"themes": row['themes']}, ensure_ascii=False)
            
            export_rows.append({
                'Headwords': headwords,
                'Gloss': gloss,
                'Language': 'Classical Nahuatl',
                'Etyma': '',
                'HomographNumber': '',
                'Sources': sources_json,
                'Extra Data': extra_data,
                'Ref': row['Ref']  # Keep for etyma linking
            })
        
        return pd.DataFrame(export_rows)
    
    def process_idiez_data(self, whp_headword_map: Dict[str, str]) -> pd.DataFrame:
        """Process IDIEZ data to CSV format"""
        
        print("Loading IDIEZ data...")
        query = """
        SELECT 
            Ref,
            OND_Node_Title,
            tlahtolli,
            "IDIEZ gramática",
            "IDIEZ def. náhuatl",
            "IDIEZ def. español",
            SShort_IDIEZ,
            "IDIEZ traduc. inglés",
            "IDIEZ morfología",
            Credit
        FROM "IDIEZ_modern_nahuatl-all-2024-03-27T09-45-31"
        """
        
        df = pd.read_sql(query, self.conn)
        
        print(f"Processing {len(df)} IDIEZ entries...")
        
        export_rows = []
        
        for idx, row in df.iterrows():
            if idx % 1000 == 0:
                print(f"  Processing IDIEZ entry {idx}...")
            
            # Build Headwords (keep period)
            headwords = row['OND_Node_Title']
            
            # Build Gloss
            gloss = row['IDIEZ traduc. inglés'] or ""
            
            # Build Etyma (link to WHP if hybrid)
            etyma = ""
            ref = row['Ref']
            if ref in whp_headword_map:
                etyma = whp_headword_map[ref]
            
            # Build Sources JSON
            sources = self.build_idiez_sources(row)
            sources_json = json.dumps(sources, ensure_ascii=False)
            
            export_rows.append({
                'Headwords': headwords,
                'Gloss': gloss,
                'Language': 'Huasteca Nahuatl',
                'Etyma': etyma,
                'HomographNumber': '',
                'Sources': sources_json,
                'Extra Data': '',
                'Ref': row['Ref']
            })
        
        return pd.DataFrame(export_rows)
    
    def export_combined_csv(self, output_path: str = "data/nahuatl_batch_import.csv"):
        """Export both WHP and IDIEZ to single combined CSV"""
        
        print("="*70)
        print("EXPORTING COMBINED NAHUATL LEXICON")
        print("="*70)
        print()
        
        # Process WHP
        whp_df = self.process_whp_data()
        
        # Create ref to headword mapping for etyma linking
        whp_headword_map = dict(zip(whp_df['Ref'], whp_df['Headwords']))
        
        print()
        
        # Process IDIEZ (with etyma links to WHP)
        idiez_df = self.process_idiez_data(whp_headword_map)
        
        # Combine
        print("\nCombining datasets...")
        combined_df = pd.concat([whp_df, idiez_df], ignore_index=True)
        
        # Detect homographs within each language
        print("Detecting homographs within Classical Nahuatl...")
        classical_mask = combined_df['Language'] == 'Classical Nahuatl'
        combined_df.loc[classical_mask, 'HomographNumber'] = self.detect_homographs(
            combined_df[classical_mask], 'Headwords'
        )
        
        print("Detecting homographs within Huasteca Nahuatl...")
        huasteca_mask = combined_df['Language'] == 'Huasteca Nahuatl'
        combined_df.loc[huasteca_mask, 'HomographNumber'] = self.detect_homographs(
            combined_df[huasteca_mask], 'Headwords'
        )
        
        # Drop Ref column (was only for internal linking)
        combined_df = combined_df.drop(columns=['Ref'])
        
        # Reorder columns to match expected format
        column_order = ['Headwords', 'Gloss', 'Language', 'Etyma', 'HomographNumber', 'Sources', 'Extra Data']
        combined_df = combined_df[column_order]
        
        # Export combined CSV
        combined_df.to_csv(output_path, index=False, encoding='utf-8-sig')
        
        print(f"\n{'='*70}")
        print(f"✓ EXPORT COMPLETE")
        print(f"{'='*70}")
        print(f"Total entries: {len(combined_df)}")
        print(f"  - Classical Nahuatl: {len(whp_df)}")
        print(f"  - Huasteca Nahuatl: {len(idiez_df)}")
        print(f"\nOutput file: {output_path}")
        
        return combined_df


# Usage
if __name__ == "__main__":
    # Update with your actual database path
    exporter = NahuatlCSVExporter("../../../data/sqLiteDb/nahuatl_processing.db")
    
    # Export to single combined CSV
    combined_df = exporter.export_combined_csv("data/nahuatl_batch_import.csv")